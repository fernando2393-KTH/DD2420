{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Clustering Using LDA\n",
    "\n",
    "The purpose of this tutorial is to give you a basic understanding of Latent Dirichlet Allocation (LDA) from http://ai.stanford.edu/~ang/papers/jair03-lda.pdf and use it to implement a simple downscaled topic clustering on the newsgroup dataset from scikit-learn.\n",
    "\n",
    "Section 1 will give some background to the mechanics and theory behind LDA. Section 2 will then tackle the task of implementing LDA to infer topics in documents based on their content. This section will provide you with skeleton code already written in Python 3 using the numpy, scipy, and scikit-learn libraries. \n",
    "\n",
    "If you do not have jupyter notebook installed then you probably aren't reading this, but see http://jupyter.readthedocs.io/en/latest/install.html\n",
    "\n",
    "If you do not have a python 3 kernel installed for jupyter notebook see https://ipython.readthedocs.io/en/latest/install/kernel_install.html or https://stackoverflow.com/questions/28831854/how-do-i-add-python3-kernel-to-jupyter-ipython\n",
    "\n",
    "If you do not have some of the libraries installed for your python 3 kernel, use the \"Kernel -> Conda packages\" dropdown menu in Jupyter if you used anaconda for your python 3 kernel, if not use the normal pip install commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Theory\n",
    "\n",
    "### Background and terminology\n",
    "\n",
    "Since we will be working in the setting of text corpora, we should clarify some of the terminology used in this setting:\n",
    "<ul>\n",
    "<li>A word is the basic unit of discrete data, defined to be an item from a vocabulary indexed by {1, . . . ,V}. \n",
    "<li> A document is a sequence of <i>N</i> words denoted by <b>w</b>=(<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>, . . . ,<i>w</i><sub>N</sub>), where <i>w</i><sub>n</sub> is the <i>n</i>th word in the sequence.</li>\n",
    "<li>A corpus is a collection of <i>M</i> documents denoted by $D$ ={<b>w</b><sub>1</sub>,<b>w</b><sub>2</sub>, . . . ,<b>w</b><sub>M</sub>}</li>\n",
    "</ul>\n",
    "\n",
    "It is important to note that LDA works in other domains besides text collections, but this is the setting in which we will use it.\n",
    "\n",
    "LDA is a generative probabilistic model that is used for modeling collections of discrete data. In our application we will be using it to model text corpora, or more specifically news group e-mails. The purpose of the model is to give us compact representations of the data in these collections, allowing us to process large collections while still retaining sufficient information to be able to perform for example classification and relevance measures. \n",
    "\n",
    "There have been several solutions for this type of information retrieval problem, such as the tf-idf (term frequency - inverse document frequency) scheme by Salton and McGill, 1983. This approach produces a term-by-document matrix X whose columns contain the tf-idf values for each of the documents in the corpus. This representation however did not provide significantly shortened representation of the corpora, or represent the inter- or intra- document statistics in a intuitive way. A step forward from this was given by LSI (latent semantic indexing) where singular value decomposition was used on the matrix X to offer a more compact representation. The authors of the method also argued that since the LSI features are linnear combinations of the basic tf-idf features, they incorporate some linguistical notions such as synonomy and polysemy.\n",
    "The first step to providing a generative model was the <i>probabilistic</i> LSI (pLSI), which uses mixture models to model each word in a document. The mixture components are the \"topics\" and represented as multinomial random variables, allowing different words in the document to be genereated by different topics. The compact representation for each document is then the list of numbers representing the mixing proportions for the fixed set of topics. The method however gives no generative probabilistic model for getting these numbers, causing the number of parameters in the model to grow linearly with the corpus size. Also, since there is no probabilistic model for the mixture components that represent a document, there is no clear way of assigning a probability to a document that is outside the training set.\n",
    "\n",
    "Both LSI and pLSI use the \"bag-of-words\" approach which assumes exchangeability within the words of the document as well as the documents themselves, meaning their order is of no importance. A theorem due to de Finetti (1990) states that any collection of exchangeable random variables has a representation as a mixture distribution—in general an infinite mixture. This means we must consider mixture models that capture the exchangeability of both documents and words if we wish to achieve exchangeable representations for them. It is this line of thinking that leads to LDA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Theory Behind LDA\n",
    "\n",
    "As mentioned earlier, LDA is a generative probabilistic model for a corpus. It can be seen as a hierarchical Bayesian model with three levels: each document in a corpus is modeled as a finite random mixture over a latent set of topics, and each of these topics are characterized by a distribution of words. A graphical model for LDA using plate notation can be seen below:\n",
    "![title](imgs/LDAPlateGM.png)\n",
    "From here we can see the three levels of the model. $\\alpha$ and $\\beta$ and corpus level parameters, $\\theta$ is a document level parameter for the M documents in the corpus, and $z$ and $w$ are word level parameters for the N words in a document.\n",
    "\n",
    "The generative process according to LDA for each document <b>w</b> is then:\n",
    "<ol>\n",
    "<li>Choose N ∼Poisson(ξ)</li>\n",
    "<li>Choose $\\theta$∼Dir($\\alpha$)</li>\n",
    "<li>For each of the N words w<sub>n</sub>:\n",
    "<ol type=\"a\">\n",
    "    <li>Choose a topic z<sub>n</sub> ∼Multinomial($\\theta$).</li>\n",
    "    <li>Choose a word w<sub>n</sub> from p(w<sub>n</sub> |z<sub>n</sub>,$\\beta$), a multinomial probability conditioned on the topic z<sub>n</sub>.</li>\n",
    "    </ol></li>\n",
    "</ol>\n",
    "\n",
    "There are however some simplifications to these steps that we will utilize. First, we assume that the dimensionality of the Dirichlet distribution, and therefore the dimensionality for the topic variable $z$ is known and fixed, meaning we assume a fixed known number of topics, $k$. Furthermore, the probabilities for words ($w$) are parameterized by a $k \\times V$ matrix $\\beta$ which defines $p(w^j = 1| z^i = 1) = \\beta_{i,j}$, that we will estimate later and keep fixed. We also note that $N$ is independant of the other data generating variables $\\theta$ and <b>z</b> so we will ignore the Poisson assumption and set it to a known fixed value (the length of the document).  \n",
    "\n",
    "#### Dirichlet Distribution in LDA\n",
    "\n",
    "The probability distribution for a $k$-dimensional Dirichlet random variable $\\theta$ is defined as follows: \n",
    "\n",
    "<b>Eq. 1:</b>\n",
    "![Eq 1](imgs/LDAEq1.png \"Eq 1\")\n",
    "\n",
    "\n",
    "where $\\alpha$ is $k$-dimensional with all elements larger than 0 and $\\Gamma(x)$ is the Gamma function. The Dirichlet distribution has some advantegous advantageous qualities; it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution. These properties help us in running variational inference for the parameters later.\n",
    "\n",
    "We can now express the joint distribution of a topic mixture $\\theta$, a set of $N$ topics <b>z</b>, and\n",
    "a set of $N$ words <b>w</b> given the corpus level parameters $\\alpha,\\beta$ as:\n",
    "\n",
    "<b>Eq. 2:</b>\n",
    "![Eq. 2](imgs/LDAEq2.png)\n",
    "where the probability $p(z_n |\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z^i_n=1$. We can then obtain the marginal distribution over a document by integrating over $\\theta$ and summing over $z$:\n",
    "\n",
    "<b>Eq. 3:</b>\n",
    "![Eq. 3](imgs/LDAEq3.png)\n",
    "\n",
    "\n",
    "#### Comparison to other Latent Variable Models\n",
    "In order to get feeling for how LDA works and what highlights its strengths, it can be helpful to relate it to other related models:\n",
    "\n",
    "  a) Unigram Model\n",
    "\n",
    "  b) Mixture of Unigrams Model\n",
    "\n",
    "  c) pLSI Model\n",
    "  \n",
    "\n",
    "\n",
    "We will begin by examing the absolute simplest model, the unigram model: \n",
    "\n",
    "![Eq. 3](imgs/UniGramMdl.png)\n",
    "\n",
    "This method has no latent variables and instead states that each word in a document is independantly drawn from a single multinomial distribution as seen here:\n",
    "\n",
    "![Eq. 3](imgs/UniGramEq.png)\n",
    "\n",
    "\n",
    "A slighly more complex model is the mixture of unigrams:\n",
    "\n",
    "![Eq. 3](imgs/MixUniGramMdl.png)\n",
    "\n",
    "This model incorporates a discrete latent topic variable, $z$. Here, each document <b>w</b> is generated by first sampling the topic variable $z$, and then generating all words from a conditional probability on that choice:\n",
    "\n",
    "![Eq. 3](imgs/MixUniGramEq.png)\n",
    "\n",
    "This effectively limits the modeling of words in a document to only being representative of one topic. The LDA model on the other hand allows for documents to exhibit multiple topics with different mixtures.\n",
    "\n",
    "Finally we have the pLSI model which we mentioned earlier. It was a relatively popular model around the time that LDA was proposed, and is the model with highest generative capabilities of these three mentioned. \n",
    "\n",
    "![Eq. 3](imgs/PISLMdl.png)\n",
    "\n",
    "pLSI proposes that each word is conditionally independant a \"document label\", $d$, given an unobserved topic $z$:\n",
    "\n",
    "![Eq. 3](imgs/PISLEq.png)\n",
    "\n",
    "This proposal aims to soften the constraint of having each document modeled as being generated from only one topic, as it is in the mixture of unigrams approach. It does so by incorporating the probability, $p(z | d)$ for a certain document $d$ as the mixture of topics that document. A true generative model cannot be created for this mixture however; as d is only a dummy index to the documents pISL was trained with, meaning it is a multinomial random variable with the same amount of possible values as training documents. This leads to the method only learning the topic mixtures, $p(z | d)$, for documents it has already seen, so there is no natural way to assign probability to an unseen document with it.\n",
    "Another problem is that to model $k$ topics with pLSI you need K multinomial distributions with vocabulary size $V$ and $M$ mixtures over the hidden topics $k$ for each training document, resulting in $kV + kM$ parameters. Not only does this not scale well but it is also prone to overfitting.\n",
    "\n",
    "LDA however treats the topic mixture weights as a $k$-parameter hidden variable, meaning the amount of parameters does not scale linnearly with the number training documents, and the generative model can still be used even with unseen documents.\n",
    "\n",
    "We can see these differences geometrically as well if we examine the distribution over words as a $V$-1 dimensional on a vocabulary of size $V$ with another $k$-1 dimensional simplex spanning $k$ topics. We can set $V$ and $k$ to 3 for simplicity (3 words gives a two-dimensional triangle):\n",
    "\n",
    "![title](imgs/UnigramSampling.png)\n",
    "\n",
    "How this distribution is spread out and how it uses the topics distribution differs among the methods. The mixture of unigrams method pics a random point on the word simplex that corresponds to one of the topic simplex vertices k, and draws all the words for a document from the distribution corresponding to that point. pLSI assumes that all words in training documents belong to a single randomly chosen topic. The topics are drawn from a document-specific distribution, meaning each document has a topic distribution that sits on the topic simplex. The training documents then give an empirical distribution (with the marked 'x's) over the topic simplex. LDA instead models that <b>each word</b> in a document is drawn from a randomly chosen topic that is sampled from a distribution governed by a random parameter. Since this parameter is sampled once per document, it gives a smooth probability distribution over the topic simplex (the circular topology markers). \n",
    "\n",
    "Now that we know how LDA compares with other methods, lets take a look at how to do inference in LDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and Parameter Estimation with LDA\n",
    "\n",
    "The main inference problem we will be interested in solving is the posterior distribution of the latent variables given a document, which would allow us to infer the topics associated with the document. This is given by the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "p( \\theta, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta) = \\frac{p( \\theta, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)}{p(\\mathbf{w} \\mid \\alpha, \\beta)} \n",
    "\\end{equation}\n",
    "\n",
    "However, to compute the normalizing denominator we would rewrite equation 3 using equation 1 and $p(z_n \\mid\\theta)=\\theta_i$ and then integrate, resulting in:\n",
    "\n",
    "\\begin{equation}\n",
    "p( \\mathbf{w} \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\sum_i\\alpha_i)}{\\prod_i\\Gamma(\\alpha_i)}\\int\\Bigg(\\prod_{i=1}^k\\theta_i^{\\alpha_i-1}\\Bigg)\\Bigg(\\prod_{n=1}^N\\sum_{i=1}^k\\prod_{j=1}^V(\\theta_i\\beta_{ij})^{w_n^j}\\Bigg)d\\theta\n",
    "\\end{equation}\n",
    "\n",
    "Unfortunately, this expression is intractable due to the coupling of $\\theta$ and $\\beta$ in the summation over topics. We can however solve this problem approximately using variational inference methods. \n",
    "\n",
    "#### Variational Inference for LDA\n",
    "\n",
    "It is possible to use several VI methods for LDA, including La Place approximation, variational approximation, and MCMC methods. In our case, we will be using the convexity-based variational approximation that was mentioned in  Olga's tutorial. From there we learned that in this VI we attempt to reformulate/simplify the original graphical model by removing some dependencies and introducing free variational parameters. This leads to a family of distributions dependant on these variational parameters which form a lower bound on the log likelyhood. We then aim to find the parameter values that give the tightest lower bound. \n",
    "\n",
    "In our case, the problematic dependancy is between $\\theta$ and $\\beta$ which is introduced by the edges between $\\theta, \\mathbf{z}$ and $\\mathbf{w}$ (remember w is a 'collision' node and is observed). If we simplify our model by removing these edges along with the <b>w</b> node, and introduce two variational parameters $\\gamma$ and $\\phi$ which give a family of distributions over the remaining latent variables, we are left with the graphical model shown on the right in the figure below:\n",
    "\n",
    "![LDA VI](imgs/LDAVIGM.png \"GM for the VI used for our LDA\")\n",
    "\n",
    "This results in the following distribution over the latent variables:\n",
    "\n",
    "\\begin{equation}\n",
    "p( \\theta, \\mathbf{z} \\mid \\gamma, \\phi) = q(\\theta\\mid\\gamma)\\prod_{n=1}^Nq(z_n\\mid\\phi_n)\n",
    "\\end{equation}\n",
    "\n",
    "where the new Dirichlet parameters $\\gamma$ and the multinomial parameters $\\phi$ are the free variational parameters. Now having simplified our graphical model, we need to find the optimal values for the variational parameters ($\\gamma^*, \\phi^*$). From Olga's tutorial we know that this is equivalent to finding the values which minimize the KL divergence between the simplified distribution and the true posterior distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "( \\gamma^*, \\phi^*) = \\arg\\!\\min_{(\\gamma,\\phi)}D\\big(q( \\theta, \\mathbf{z} \\mid \\gamma, \\phi) \\mid\\mid p( \\theta, \\mathbf{z} \\mid \\mathbf{w},\\alpha, \\beta\\big)\n",
    "\\end{equation}\n",
    "\n",
    "We do this by setting the derivatives of the KL divergence to zero w.r.t $\\gamma$ and $\\phi$ we get the following update equations for the parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_{ni} \\propto \\beta_{iw_n}\\mathrm{exp}\\big\\lbrace\\mathrm{E}_q\\big[log(\\theta_i)\\mid\\gamma\\big]\\big\\rbrace = \\beta_{iw_n}\\mathrm{exp}\\big\\lbrace\\Psi(\\gamma_i)\\big\\rbrace\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\gamma_i = \\alpha_i + \\sum_{n=1}^N\\phi_{ni}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Psi$ is the digamma function (first derivative of log $\\Gamma$). It is important to note that these update equations derived from the KL divergence are dependant on a certain choice of <b>w</b>. This means that the shown approximation for the variational parameters is only valid for one set of words, and must therefore be calculated for each document when we use them later on. \n",
    "\n",
    "We must also find a way of estimating the $\\beta$ matrix, as it is used in the approximations for the variational parameters. The log likelihood of the data given $\\beta$ and $\\alpha$ is intractable as we saw at the end of the previous section. However, it is possible to implement a variational EM procedure that gives us an approximation of the best value for $\\beta$ by first maximizing a lower bound for the optimal variational parameters $\\gamma^*,\\phi^*$, then maximizing the lower bound w.r.t $\\beta$ with the previously acquired variational parameters. Essentially we will iterate the following steps until a sufficient level of convergence:\n",
    "<ol>\n",
    "<li>(E-step) Find the optimizing values of the variational parameters {$\\gamma^∗\n",
    "_d,\\phi^∗_d : d\\in D$},for each document as described earlier.</li>\n",
    "<li>(M-step) Maximize the resulting lower bound on the log likelihood w.r.t $\\beta$ using:\n",
    "\\begin{equation}\n",
    "\\beta_{ij}\\propto\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\phi^*_{dni}w^j_{dn}\n",
    "\\end{equation}\n",
    "as well as maximize the resulting lower bound on the log likelihood w.r.t $\\alpha$ (this will be given to you).\n",
    "</ol>\n",
    "\n",
    "In laymans terms, what we are essentially doing in the E-step is finding out \"How prevalent are topics in the document across its words?\". In the M-step we then ask \"How prevalent are specific words across topics?\". By using the answer from one question as a starting point for the other, we iteratively gain the answer to both. \n",
    "\n",
    "<span style=\"color:red\">For proof for the update equations, see appendix of http://ai.stanford.edu/~ang/papers/jair03-lda.pdf</span>. This appendix also includes the derivation of the Newton-Raphson based method for updating $\\alpha$.  \n",
    "\n",
    "We have now seen the basic intuition behind LDA, and gone through methods for running inference based on the LDA model. In the next section we will put this knowledge in to practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Implementation\n",
    "\n",
    "The goal of this task is to use LDA to create topics newsgroup documents, and infer the topic that new documents would belong to. In this setting, our document corpus is the Newsgroup dataset from scikit-learn, and a document is a certain document/e-mail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "First we will load the dataset we will use for training and testing. We will simplify the example from the original paper to only do clustering for 4 topics, and only use 250 documents with a vocabulary of 750 words. While this does have an effect on the accuracy and performance of the algorithm, it's more important for you to be able to run the code in a managable amount of time. The documents I have chosen come from 4 different categories; \"Christian Religion\", \"Hockey\", \"Space\" and \"Cars\". This means that we have slightly unrealistic prior knowledge by assuming the exact correct number of topics, but don't worry there are bonus assignments in the end where you can play around with the number of topics. I have already compiled and done some preprocessing on the documents, as well as built the vocabulary dataset as pickle files. Run the code in the cell below and double check that you get the output \"found 200 training and 50 test documents, with a vocabulary of 750 words\". Do not worry if you get a warning regarding the version of CountVectorizer which is used for handling the vocabulary of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  200 training and  50  test documents, with a vocabulary of  750  words.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle as pickle\n",
    "\n",
    "#vectorizer = pickle.load(open(\"newsVectorizer2.p\", \"rb\"))\n",
    "#trainDocuments = pickle.load(open(\"newsTrainDocs.p\", \"rb\"))\n",
    "#testDocuments = pickle.load(open(\"newsTestDocs.p\", \"rb\"))\n",
    "\n",
    "vectorizer = pickle.load(open(\"vectorizerNews.p\", \"rb\"))\n",
    "trainDocs = pickle.load(open(\"trainDocsNews.p\", \"rb\"))\n",
    "testDocs = pickle.load(open(\"testDocsNews.p\", \"rb\"))\n",
    "\n",
    "origTrainDocs = pickle.load(open(\"trainDocsNewsOrig.p\", \"rb\"))\n",
    "origTestDocs = pickle.load(open(\"testDocsNewsOrig.p\", \"rb\"))\n",
    "\n",
    "print(\"Found \", len(trainDocs), \"training and \", len(testDocs), \" test documents, with a vocabulary of \", len(vectorizer.get_feature_names()), \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation\n",
    "\n",
    "We must now find the optimal values for the variational parameters, as well as the values for $\\alpha$ and the $\\beta$ matrix that were introduced in the variational inference section. In order to follow the instructions given in the VI section we will need to do some setup first, so run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import scipy.special as special\n",
    "import scipy.optimize \n",
    "import time\n",
    "\n",
    "#diGamma func from scipy, use this in your code!\n",
    "diGamma = special.digamma\n",
    "\n",
    "#Function definitions for maximizing the VI parameters. This will later be completed by you.\n",
    "def maxVIParam(phi, gamma, B, alpha, M, k, Wd, eta):\n",
    "    \n",
    "    for d in range(M):\n",
    "        N = len(Wd[d])\n",
    "        #Initialization of vars, as shown in E-step. \n",
    "        phi[d] = np.ones((N,k))*1.0/k\n",
    "        gamma[d] = np.ones(k)*(N/k) + alpha\n",
    "        converged = False\n",
    "        j = 0 #you can use this to print the update error to check your code in the beginning with something like:\n",
    "        '''if(j%10==0 and d==0):\n",
    "                    print(\"u e: \", updateError)'''\n",
    "        \n",
    "        #-----Complete the method here by implementing the pseudo code for the E-Step----\n",
    "        while(not converged):\n",
    "            for n in range(N):\n",
    "                for i in range(k):\n",
    "                    #print(\"old,\", phi[n,i])\n",
    "                    phi[d][n,i] = B[i,Wd[d][n]]*np.exp(diGamma(gamma[d][i]))\n",
    "                    #print(\"new:\", phi[n,i])\n",
    "                if(np.sum(phi[d][n])==0):\n",
    "                    print(\"crap, phi sum is: \", np.sum(phi[d][n]), \"for doc \",d)\n",
    "                    phi[d][n] = np.zeros(k)\n",
    "                else:\n",
    "                    phi[d][n] = phi[d][n]/np.sum(phi[d][n])\n",
    "\n",
    "            updateError = 0\n",
    "            for i in range(k):\n",
    "                gammaOld = gamma[d][i]\n",
    "                #print(\"old:,\", gammaOld)\n",
    "                gamma[d][i] = alpha[i] + np.sum(phi[d][:,i])\n",
    "                #print(\"new: \", gamma[d][i], \"der: \",np.exp(diGamma(gammaOld)))\n",
    "                updateError += abs(gammaOld - gamma[d][i]) \n",
    "            if(updateError < eta):\n",
    "                converged = True\n",
    "            \n",
    "    \n",
    "    return gamma, phi\n",
    "\n",
    "#Function definitions for maximizing the B parameter. This will later be completed by you.\n",
    "def MaxB(B, phi, k, V, M, Wd):\n",
    "    \n",
    "    #YOUR CODE FOR THE M-STEP HERE\n",
    "    for i in range(k):\n",
    "        for j in range(V):\n",
    "            B[i,j] = 0\n",
    "            for d in range(M):    \n",
    "\n",
    "                if(j in Wd[d]):\n",
    "                    Wj = 1\n",
    "                else:\n",
    "                    Wj = 0\n",
    "\n",
    "                for n in range(len(phi[d])):\n",
    "                    B[i,j] += phi[d][n,i]*Wj\n",
    "    return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Here are the functions needed for updating the alpha parameter, as shown in the start of appendix A.4.2.\n",
    "These are all provided for you as it is just plugging in the definition for the gradient and hessian into the \n",
    "Newton-Raphson based method to find a stationary point using SciPy. Feel free to take a look at the appendix to\n",
    "see where these values come from.'''\n",
    "\n",
    "#value of Likelihood(gamma,phi,alpha,beta) function w.r.t. alpha terms (see start of appendix A.4.2) \n",
    "def L_alpha_val(a):\n",
    "    val = 0\n",
    "    M = len(gamma)\n",
    "    k = len(a)\n",
    "    for d in range(M):\n",
    "        val += (np.log(scipy.special.gamma(np.sum(a))) - np.sum([np.log(scipy.special.gamma(a[i])) for i in range(k)]) + np.sum([((a[i] -1)*(diGamma(gamma[d][i]) - diGamma(np.sum(gamma[d])))) for i in range(k)]))\n",
    "\n",
    "    return -val\n",
    "\n",
    "#value of the derivative of above func w.r.t. alpha_i (2nd eq of appendix A.4.2) \n",
    "def L_alpha_der(a):\n",
    "    M = len(gamma)\n",
    "    k = len(a)\n",
    "    der = np.array(\n",
    "    [(M*(diGamma(np.sum(a)) - diGamma(a[i])) + np.sum([diGamma(gamma[d][i]) - diGamma(np.sum(gamma[d])) for d in range(M)])) for i in range(k)]\n",
    "    )\n",
    "    return -der\n",
    "\n",
    "def L_alpha_hess(a):\n",
    "    hess = np.zeros((len(a),len(a)))\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(a)):\n",
    "            k_delta = 1 if i == j else 0\n",
    "            hess[i,j] = k_delta*M*scipy.special.polygamma(1,a[i]) - scipy.special.polygamma(1,np.sum(a))\n",
    "    return -hess\n",
    "\n",
    "def MaxA(a):\n",
    "    res = scipy.optimize.minimize(L_alpha_val, a, method='Newton-CG',\n",
    "        jac=L_alpha_der, hess=L_alpha_hess,\n",
    "        options={'xtol': 1e-8, 'disp': False})\n",
    "    print(res.x)\n",
    "    \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, we can now initialize the required parameters and define the skeleton of our loop for the parameter estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on iter:  0\n",
      "B max diff:  3063.96729225\n",
      "[ 1.63959055  2.15508332  1.85917669  1.37267296]\n",
      "iter took:  91.53012180328369\n",
      "new alpha:  [ 1.63959055  2.15508332  1.85917669  1.37267296]\n",
      "on iter:  1\n",
      "B max diff:  4381.47512277\n",
      "[ 1.33817758  1.92420598  1.1402266   1.06614479]\n",
      "iter took:  102.99530720710754\n",
      "new alpha:  [ 1.33817758  1.92420598  1.1402266   1.06614479]\n",
      "on iter:  2\n",
      "B max diff:  4730.54074284\n",
      "[ 0.68782902  0.98267654  0.52623431  0.58272761]\n",
      "iter took:  61.22580361366272\n",
      "new alpha:  [ 0.68782902  0.98267654  0.52623431  0.58272761]\n",
      "on iter:  3\n",
      "B max diff:  4782.60032849\n",
      "[ 0.41393411  0.57621589  0.3022423   0.37951737]\n",
      "iter took:  64.54079818725586\n",
      "new alpha:  [ 0.41393411  0.57621589  0.3022423   0.37951737]\n",
      "on iter:  4\n",
      "B max diff:  4767.71837023\n",
      "[ 0.26221524  0.38130595  0.18435596  0.2719242 ]\n",
      "iter took:  62.026681423187256\n",
      "new alpha:  [ 0.26221524  0.38130595  0.18435596  0.2719242 ]\n",
      "on iter:  5\n",
      "B max diff:  4668.59704455\n",
      "[ 0.17298086  0.25695241  0.12308987  0.21966763]\n",
      "iter took:  56.201083183288574\n",
      "new alpha:  [ 0.17298086  0.25695241  0.12308987  0.21966763]\n",
      "on iter:  6\n",
      "B max diff:  4660.95660178\n",
      "[ 0.12487503  0.17763936  0.09114672  0.19319908]\n",
      "iter took:  52.09963655471802\n",
      "new alpha:  [ 0.12487503  0.17763936  0.09114672  0.19319908]\n",
      "on iter:  7\n",
      "B max diff:  4660.23810357\n",
      "[ 0.09647294  0.13186667  0.07132192  0.17626303]\n",
      "iter took:  47.27097225189209\n",
      "new alpha:  [ 0.09647294  0.13186667  0.07132192  0.17626303]\n",
      "on iter:  8\n",
      "B max diff:  4660.69454987\n",
      "[ 0.08016355  0.10544539  0.05778047  0.15619371]\n",
      "iter took:  47.516581296920776\n",
      "new alpha:  [ 0.08016355  0.10544539  0.05778047  0.15619371]\n",
      "on iter:  9\n",
      "B max diff:  4657.33102686\n",
      "[ 0.06924661  0.0891697   0.04905089  0.14325719]\n",
      "iter took:  45.680757999420166\n",
      "new alpha:  [ 0.06924661  0.0891697   0.04905089  0.14325719]\n",
      "on iter:  10\n",
      "B max diff:  4653.39228857\n",
      "[ 0.0616498   0.07923303  0.04285426  0.13343987]\n",
      "iter took:  47.35211706161499\n",
      "new alpha:  [ 0.0616498   0.07923303  0.04285426  0.13343987]\n",
      "on iter:  11\n",
      "B max diff:  4652.35532945\n",
      "[ 0.05606652  0.07297288  0.03823592  0.12554626]\n",
      "iter took:  45.91507315635681\n",
      "new alpha:  [ 0.05606652  0.07297288  0.03823592  0.12554626]\n",
      "on iter:  12\n",
      "B max diff:  4652.0567791\n",
      "[ 0.05232903  0.0684735   0.034834    0.11933655]\n",
      "iter took:  47.544594049453735\n",
      "new alpha:  [ 0.05232903  0.0684735   0.034834    0.11933655]\n",
      "on iter:  13\n",
      "B max diff:  4651.94908618\n",
      "[ 0.04951924  0.06546552  0.03209943  0.11447547]\n",
      "iter took:  46.094252824783325\n",
      "new alpha:  [ 0.04951924  0.06546552  0.03209943  0.11447547]\n",
      "on iter:  14\n",
      "B max diff:  4652.04285125\n",
      "[ 0.04732267  0.06303732  0.03013407  0.10980344]\n",
      "iter took:  46.721999168395996\n",
      "new alpha:  [ 0.04732267  0.06303732  0.03013407  0.10980344]\n",
      "on iter:  15\n",
      "B max diff:  4652.10402183\n",
      "[ 0.04558224  0.06074609  0.02886373  0.10630662]\n",
      "iter took:  48.52951669692993\n",
      "new alpha:  [ 0.04558224  0.06074609  0.02886373  0.10630662]\n",
      "on iter:  16\n",
      "B max diff:  4652.06717033\n",
      "[ 0.04409935  0.05849438  0.02764786  0.1019662 ]\n",
      "iter took:  46.448657751083374\n",
      "new alpha:  [ 0.04409935  0.05849438  0.02764786  0.1019662 ]\n",
      "on iter:  17\n",
      "B max diff:  4652.1642999\n",
      "[ 0.04285006  0.05642164  0.02676516  0.09893042]\n",
      "iter took:  45.30983567237854\n",
      "new alpha:  [ 0.04285006  0.05642164  0.02676516  0.09893042]\n",
      "on iter:  18\n",
      "B max diff:  4652.35483639\n",
      "[ 0.04181358  0.0548499   0.02615385  0.09668407]\n",
      "iter took:  46.20882868766785\n",
      "new alpha:  [ 0.04181358  0.0548499   0.02615385  0.09668407]\n",
      "on iter:  19\n",
      "B max diff:  4652.63545787\n",
      "[ 0.04096779  0.05337875  0.02563154  0.095585  ]\n",
      "iter took:  48.82106947898865\n",
      "new alpha:  [ 0.04096779  0.05337875  0.02563154  0.095585  ]\n",
      "on iter:  20\n",
      "B max diff:  4652.70579963\n",
      "[ 0.04003603  0.05225943  0.02517729  0.09437689]\n",
      "iter took:  45.417680740356445\n",
      "new alpha:  [ 0.04003603  0.05225943  0.02517729  0.09437689]\n",
      "on iter:  21\n",
      "B max diff:  4652.69541963\n",
      "[ 0.03892931  0.05144498  0.02479718  0.09466857]\n",
      "iter took:  46.14843463897705\n",
      "new alpha:  [ 0.03892931  0.05144498  0.02479718  0.09466857]\n",
      "on iter:  22\n",
      "B max diff:  4652.36510296\n",
      "[ 0.03831464  0.05056185  0.02473391  0.09443102]\n",
      "iter took:  46.86821246147156\n",
      "new alpha:  [ 0.03831464  0.05056185  0.02473391  0.09443102]\n",
      "on iter:  23\n",
      "B max diff:  4648.36132436\n",
      "[ 0.03744154  0.04991651  0.02454119  0.09462023]\n",
      "iter took:  46.190295696258545\n",
      "new alpha:  [ 0.03744154  0.04991651  0.02454119  0.09462023]\n",
      "on iter:  24\n",
      "B max diff:  4644.50093645\n",
      "[ 0.0367713   0.04912566  0.02436436  0.09426246]\n",
      "iter took:  44.649062633514404\n",
      "new alpha:  [ 0.0367713   0.04912566  0.02436436  0.09426246]\n",
      "on iter:  25\n",
      "B max diff:  4642.31305871\n",
      "[ 0.03622662  0.04849098  0.02444789  0.09305935]\n",
      "iter took:  45.9592866897583\n",
      "new alpha:  [ 0.03622662  0.04849098  0.02444789  0.09305935]\n",
      "on iter:  26\n",
      "B max diff:  4641.21041122\n",
      "[ 0.03595806  0.0479518   0.02448314  0.09156986]\n",
      "iter took:  48.13630247116089\n",
      "new alpha:  [ 0.03595806  0.0479518   0.02448314  0.09156986]\n",
      "on iter:  27\n",
      "B max diff:  4640.42557928\n",
      "[ 0.03570473  0.04723962  0.0246112   0.09064734]\n",
      "iter took:  46.35582947731018\n",
      "new alpha:  [ 0.03570473  0.04723962  0.0246112   0.09064734]\n",
      "on iter:  28\n",
      "B max diff:  4639.36127595\n",
      "[ 0.0352195   0.04633553  0.0246599   0.08855038]\n",
      "iter took:  46.77786326408386\n",
      "new alpha:  [ 0.0352195   0.04633553  0.0246599   0.08855038]\n",
      "on iter:  29\n",
      "B max diff:  4636.0095825\n",
      "[ 0.03484739  0.04572304  0.02495334  0.08858412]\n",
      "iter took:  45.852160692214966\n",
      "new alpha:  [ 0.03484739  0.04572304  0.02495334  0.08858412]\n",
      "on iter:  30\n",
      "B max diff:  4634.35599516\n",
      "[ 0.03455819  0.04504231  0.02531445  0.08844058]\n",
      "iter took:  45.26426863670349\n",
      "new alpha:  [ 0.03455819  0.04504231  0.02531445  0.08844058]\n",
      "on iter:  31\n",
      "B max diff:  4627.11679649\n",
      "[ 0.03416895  0.04459545  0.02560432  0.08889632]\n",
      "iter took:  48.64435911178589\n",
      "new alpha:  [ 0.03416895  0.04459545  0.02560432  0.08889632]\n",
      "on iter:  32\n",
      "B max diff:  4621.4795148\n",
      "[ 0.03369277  0.04403059  0.02596389  0.08893749]\n",
      "iter took:  47.101970911026\n",
      "new alpha:  [ 0.03369277  0.04403059  0.02596389  0.08893749]\n",
      "on iter:  33\n",
      "B max diff:  4617.78266183\n",
      "[ 0.03332613  0.04362161  0.02609881  0.08878276]\n",
      "iter took:  46.955018758773804\n",
      "new alpha:  [ 0.03332613  0.04362161  0.02609881  0.08878276]\n",
      "on iter:  34\n",
      "B max diff:  4614.94506531\n",
      "[ 0.03265879  0.04278731  0.02617255  0.08833349]\n",
      "iter took:  45.67436337471008\n",
      "new alpha:  [ 0.03265879  0.04278731  0.02617255  0.08833349]\n",
      "on iter:  35\n",
      "B max diff:  4612.39427936\n",
      "[ 0.03212958  0.04216049  0.02620054  0.08778446]\n",
      "iter took:  47.252232789993286\n",
      "new alpha:  [ 0.03212958  0.04216049  0.02620054  0.08778446]\n",
      "on iter:  36\n",
      "B max diff:  4610.97399805\n",
      "[ 0.03151144  0.04139952  0.02631231  0.08647581]\n",
      "iter took:  47.33571267127991\n",
      "new alpha:  [ 0.03151144  0.04139952  0.02631231  0.08647581]\n",
      "on iter:  37\n",
      "B max diff:  4609.26724697\n",
      "[ 0.03101803  0.04102681  0.02636804  0.08559241]\n",
      "iter took:  45.5364933013916\n",
      "new alpha:  [ 0.03101803  0.04102681  0.02636804  0.08559241]\n",
      "on iter:  38\n",
      "B max diff:  4608.38686314\n",
      "[ 0.03062297  0.04071438  0.0263856   0.08494809]\n",
      "iter took:  47.060352087020874\n",
      "new alpha:  [ 0.03062297  0.04071438  0.0263856   0.08494809]\n",
      "on iter:  39\n",
      "B max diff:  4607.82314266\n",
      "[ 0.03030498  0.04045335  0.02637786  0.08444776]\n",
      "iter took:  47.01722002029419\n",
      "new alpha:  [ 0.03030498  0.04045335  0.02637786  0.08444776]\n",
      "on iter:  40\n",
      "B max diff:  4607.42426421\n",
      "[ 0.03004745  0.04023521  0.02635393  0.08404303]\n",
      "iter took:  46.95394039154053\n",
      "new alpha:  [ 0.03004745  0.04023521  0.02635393  0.08404303]\n",
      "on iter:  41\n",
      "B max diff:  4607.13137684\n",
      "[ 0.02983754  0.04005239  0.02632025  0.08370695]\n",
      "iter took:  46.0405375957489\n",
      "new alpha:  [ 0.02983754  0.04005239  0.02632025  0.08370695]\n",
      "on iter:  42\n",
      "B max diff:  4606.91242523\n",
      "[ 0.02965832  0.03988922  0.0265496   0.0829239 ]\n",
      "iter took:  47.513407945632935\n",
      "new alpha:  [ 0.02965832  0.03988922  0.0265496   0.0829239 ]\n",
      "on iter:  43\n",
      "B max diff:  4606.7826994\n",
      "[ 0.0295114   0.03975356  0.02671636  0.08249791]\n",
      "iter took:  48.71144127845764\n",
      "new alpha:  [ 0.0295114   0.03975356  0.02671636  0.08249791]\n",
      "on iter:  44\n",
      "B max diff:  4606.68440523\n",
      "[ 0.02939924  0.03965597  0.02698383  0.0822867 ]\n",
      "iter took:  50.19338607788086\n",
      "new alpha:  [ 0.02939924  0.03965597  0.02698383  0.0822867 ]\n",
      "on iter:  45\n",
      "B max diff:  4606.54981103\n",
      "[ 0.02931527  0.03958807  0.02718989  0.08217185]\n",
      "iter took:  49.26229786872864\n",
      "new alpha:  [ 0.02931527  0.03958807  0.02718989  0.08217185]\n",
      "on iter:  46\n",
      "B max diff:  4606.41950748\n",
      "[ 0.02922998  0.03993872  0.02732809  0.08092495]\n",
      "iter took:  46.79002594947815\n",
      "new alpha:  [ 0.02922998  0.03993872  0.02732809  0.08092495]\n",
      "on iter:  47\n",
      "B max diff:  4607.59400259\n",
      "[ 0.02915793  0.04015834  0.02742726  0.08035311]\n",
      "iter took:  47.04379200935364\n",
      "new alpha:  [ 0.02915793  0.04015834  0.02742726  0.08035311]\n",
      "on iter:  48\n",
      "B max diff:  4608.02260996\n",
      "[ 0.02910191  0.04029918  0.02750146  0.08009139]\n",
      "iter took:  46.74533438682556\n",
      "new alpha:  [ 0.02910191  0.04029918  0.02750146  0.08009139]\n",
      "on iter:  49\n",
      "B max diff:  4608.13589772\n",
      "[ 0.02906028  0.0403918   0.02755851  0.07997309]\n",
      "iter took:  47.27752161026001\n",
      "new alpha:  [ 0.02906028  0.0403918   0.02755851  0.07997309]\n",
      "on iter:  50\n",
      "B max diff:  4608.13755401\n",
      "[ 0.0290302   0.04045417  0.02760311  0.0799207 ]\n",
      "iter took:  46.748591899871826\n",
      "new alpha:  [ 0.0290302   0.04045417  0.02760311  0.0799207 ]\n",
      "on iter:  51\n",
      "B max diff:  4608.10743681\n",
      "[ 0.0290089   0.04049706  0.02763832  0.07989809]\n",
      "iter took:  45.899672985076904\n",
      "new alpha:  [ 0.0290089   0.04049706  0.02763832  0.07989809]\n",
      "on iter:  52\n",
      "B max diff:  4608.07408371\n",
      "[ 0.02899406  0.04052707  0.02766626  0.07988862]\n",
      "iter took:  45.741209268569946\n",
      "new alpha:  [ 0.02899406  0.04052707  0.02766626  0.07988862]\n",
      "on iter:  53\n",
      "B max diff:  4608.04613014\n",
      "[ 0.02898387  0.0405484   0.0276885   0.07988468]\n",
      "iter took:  47.61119055747986\n",
      "new alpha:  [ 0.02898387  0.0405484   0.0276885   0.07988468]\n",
      "on iter:  54\n",
      "B max diff:  4608.0252592\n",
      "[ 0.02897698  0.04056375  0.02770624  0.07988285]\n",
      "iter took:  48.74301266670227\n",
      "new alpha:  [ 0.02897698  0.04056375  0.02770624  0.07988285]\n",
      "on iter:  55\n",
      "B max diff:  4608.01074969\n",
      "[ 0.0289724   0.0405749   0.02772037  0.07988165]\n",
      "iter took:  47.686275243759155\n",
      "new alpha:  [ 0.0289724   0.0405749   0.02772037  0.07988165]\n",
      "on iter:  56\n",
      "B max diff:  4608.00117864\n",
      "[ 0.02896941  0.04058309  0.02773165  0.0798806 ]\n",
      "iter took:  46.73439383506775\n",
      "new alpha:  [ 0.02896941  0.04058309  0.02773165  0.0798806 ]\n",
      "on iter:  57\n",
      "B max diff:  4607.99538421\n",
      "[ 0.02896751  0.04058913  0.02774063  0.07987947]\n",
      "iter took:  45.23814129829407\n",
      "new alpha:  [ 0.02896751  0.04058913  0.02774063  0.07987947]\n",
      "on iter:  58\n",
      "B max diff:  4607.99229533\n",
      "[ 0.02896634  0.04059362  0.0277478   0.07987816]\n",
      "iter took:  45.788057804107666\n",
      "new alpha:  [ 0.02896634  0.04059362  0.0277478   0.07987816]\n",
      "on iter:  59\n",
      "B max diff:  4607.99112312\n",
      "[ 0.02896565  0.04059695  0.02775351  0.0798767 ]\n",
      "iter took:  45.978171825408936\n",
      "new alpha:  [ 0.02896565  0.04059695  0.02775351  0.0798767 ]\n",
      "on iter:  60\n",
      "B max diff:  4607.9913363\n",
      "[ 0.02897392  0.04083838  0.02776605  0.07993171]\n",
      "iter took:  48.628868103027344\n",
      "new alpha:  [ 0.02897392  0.04083838  0.02776605  0.07993171]\n",
      "on iter:  61\n",
      "B max diff:  4608.1072447\n",
      "[ 0.02898727  0.04101754  0.02778262  0.0799792 ]\n",
      "iter took:  44.980873107910156\n",
      "new alpha:  [ 0.02898727  0.04101754  0.02778262  0.0799792 ]\n",
      "on iter:  62\n",
      "B max diff:  4608.48155009\n",
      "[ 0.02898332  0.04111205  0.02778347  0.07938168]\n",
      "iter took:  45.26863956451416\n",
      "new alpha:  [ 0.02898332  0.04111205  0.02778347  0.07938168]\n",
      "on iter:  63\n",
      "B max diff:  4611.419551\n",
      "[ 0.02895996  0.04113467  0.02791373  0.07850865]\n",
      "iter took:  45.52705764770508\n",
      "new alpha:  [ 0.02895996  0.04113467  0.02791373  0.07850865]\n",
      "on iter:  64\n",
      "B max diff:  4611.10943783\n",
      "[ 0.02891272  0.0410891   0.027988    0.0774618 ]\n",
      "iter took:  46.402374505996704\n",
      "new alpha:  [ 0.02891272  0.0410891   0.027988    0.0774618 ]\n",
      "on iter:  65\n",
      "B max diff:  4610.71955519\n",
      "[ 0.02885913  0.04102198  0.02802925  0.07688328]\n",
      "iter took:  45.250481367111206\n",
      "new alpha:  [ 0.02885913  0.04102198  0.02802925  0.07688328]\n",
      "on iter:  66\n",
      "B max diff:  4610.4524518\n",
      "[ 0.02880687  0.04095311  0.0280501   0.07653925]\n",
      "iter took:  42.90622138977051\n",
      "new alpha:  [ 0.02880687  0.04095311  0.0280501   0.07653925]\n",
      "on iter:  67\n",
      "B max diff:  4610.26212135\n",
      "[ 0.02874192  0.0408554   0.0280422   0.07578401]\n",
      "iter took:  42.64117741584778\n",
      "new alpha:  [ 0.02874192  0.0408554   0.0280422   0.07578401]\n",
      "on iter:  68\n",
      "B max diff:  4610.13146617\n",
      "[ 0.02867614  0.04075618  0.02802054  0.07534616]\n",
      "iter took:  43.32445526123047\n",
      "new alpha:  [ 0.02867614  0.04075618  0.02802054  0.07534616]\n",
      "on iter:  69\n",
      "B max diff:  4610.03202518\n",
      "[ 0.02862336  0.04089978  0.02800108  0.07512612]\n",
      "iter took:  43.38652586936951\n",
      "new alpha:  [ 0.02862336  0.04089978  0.02800108  0.07512612]\n",
      "on iter:  70\n",
      "B max diff:  4610.08660655\n",
      "[ 0.028565    0.04095715  0.02796898  0.07448137]\n",
      "iter took:  44.26531910896301\n",
      "new alpha:  [ 0.028565    0.04095715  0.02796898  0.07448137]\n",
      "on iter:  71\n",
      "B max diff:  4610.09933723\n",
      "[ 0.02849413  0.04093838  0.0279189   0.07364318]\n",
      "iter took:  43.47987651824951\n",
      "new alpha:  [ 0.02849413  0.04093838  0.0279189   0.07364318]\n",
      "on iter:  72\n",
      "B max diff:  4610.12224514\n",
      "[ 0.02842326  0.04088837  0.02786428  0.07317367]\n",
      "iter took:  43.06662201881409\n",
      "new alpha:  [ 0.02842326  0.04088837  0.02786428  0.07317367]\n",
      "on iter:  73\n",
      "B max diff:  4610.11017785\n",
      "[ 0.02835768  0.04082879  0.02781119  0.07289039]\n",
      "iter took:  42.646376609802246\n",
      "new alpha:  [ 0.02835768  0.04082879  0.02781119  0.07289039]\n",
      "on iter:  74\n",
      "B max diff:  4610.09239001\n",
      "[ 0.02829916  0.04076946  0.02776215  0.07270515]\n",
      "iter took:  42.81148290634155\n",
      "new alpha:  [ 0.02829916  0.04076946  0.02776215  0.07270515]\n",
      "on iter:  75\n",
      "B max diff:  4610.07614251\n",
      "[ 0.02824788  0.04071448  0.02771803  0.0725743 ]\n",
      "iter took:  42.364585638046265\n",
      "new alpha:  [ 0.02824788  0.04071448  0.02771803  0.0725743 ]\n",
      "on iter:  76\n",
      "B max diff:  4610.06333334\n",
      "[ 0.02820336  0.04066525  0.02767889  0.07247568]\n",
      "iter took:  42.9377875328064\n",
      "new alpha:  [ 0.02820336  0.04066525  0.02767889  0.07247568]\n",
      "on iter:  77\n",
      "B max diff:  4610.05398327\n",
      "[ 0.02816489  0.04062194  0.02764446  0.07239767]\n",
      "iter took:  43.25054144859314\n",
      "new alpha:  [ 0.02816489  0.04062194  0.02764446  0.07239767]\n",
      "on iter:  78\n",
      "B max diff:  4610.0475414\n",
      "[ 0.02813174  0.04058418  0.02761432  0.07233386]\n",
      "iter took:  41.80628800392151\n",
      "new alpha:  [ 0.02813174  0.04058418  0.02761432  0.07233386]\n",
      "on iter:  79\n",
      "B max diff:  4610.04322804\n",
      "[ 0.0281032   0.04055144  0.02758802  0.07228057]\n",
      "iter took:  42.58069896697998\n",
      "new alpha:  [ 0.0281032   0.04055144  0.02758802  0.07228057]\n",
      "on iter:  80\n",
      "B max diff:  4610.04045597\n",
      "[ 0.02807863  0.04052311  0.02756511  0.07223545]\n",
      "iter took:  43.47493815422058\n",
      "new alpha:  [ 0.02807863  0.04052311  0.02756511  0.07223545]\n",
      "on iter:  81\n",
      "B max diff:  4610.03875254\n",
      "[ 0.02805748  0.04049863  0.0275452   0.07219695]\n",
      "iter took:  44.189308166503906\n",
      "new alpha:  [ 0.02805748  0.04049863  0.0275452   0.07219695]\n",
      "on iter:  82\n",
      "B max diff:  4610.03776894\n",
      "[ 0.02803927  0.04047749  0.0275279   0.07216398]\n",
      "iter took:  43.83607745170593\n",
      "new alpha:  [ 0.02803927  0.04047749  0.0275279   0.07216398]\n",
      "on iter:  83\n",
      "B max diff:  4610.03725513\n",
      "[ 0.02802359  0.04045924  0.02751289  0.07213561]\n",
      "iter took:  43.252710580825806\n",
      "new alpha:  [ 0.02802359  0.04045924  0.02751289  0.07213561]\n",
      "on iter:  84\n",
      "B max diff:  4610.03704165\n",
      "[ 0.02801008  0.04044348  0.02749987  0.07211116]\n",
      "iter took:  44.17596173286438\n",
      "new alpha:  [ 0.02801008  0.04044348  0.02749987  0.07211116]\n",
      "on iter:  85\n",
      "B max diff:  4610.03701398\n",
      "[ 0.02799843  0.04042987  0.02748859  0.07209007]\n",
      "iter took:  43.20587706565857\n",
      "new alpha:  [ 0.02799843  0.04042987  0.02748859  0.07209007]\n",
      "on iter:  86\n",
      "B max diff:  4610.03709617\n",
      "[ 0.02798838  0.04041812  0.02747881  0.0720719 ]\n",
      "iter took:  43.50362467765808\n",
      "new alpha:  [ 0.02798838  0.04041812  0.02747881  0.0720719 ]\n",
      "on iter:  87\n",
      "B max diff:  4610.03723909\n",
      "[ 0.02797972  0.04040798  0.02747034  0.07205619]\n",
      "iter took:  42.71505427360535\n",
      "new alpha:  [ 0.02797972  0.04040798  0.02747034  0.07205619]\n",
      "on iter:  88\n",
      "B max diff:  4610.03741068\n",
      "[ 0.02797224  0.04039921  0.027463    0.07204261]\n",
      "iter took:  42.94700837135315\n",
      "new alpha:  [ 0.02797224  0.04039921  0.027463    0.07204261]\n",
      "on iter:  89\n",
      "B max diff:  4610.03759129\n",
      "[ 0.02796579  0.04039163  0.02745665  0.07203088]\n",
      "iter took:  43.04197025299072\n",
      "new alpha:  [ 0.02796579  0.04039163  0.02745665  0.07203088]\n",
      "on iter:  90\n",
      "B max diff:  4610.03776834\n",
      "[ 0.02796022  0.04038509  0.02745115  0.07202074]\n",
      "iter took:  43.32127523422241\n",
      "new alpha:  [ 0.02796022  0.04038509  0.02745115  0.07202074]\n",
      "on iter:  91\n",
      "B max diff:  4610.03793615\n",
      "[ 0.02795541  0.04037943  0.0274464   0.072012  ]\n",
      "iter took:  42.96015191078186\n",
      "new alpha:  [ 0.02795541  0.04037943  0.0274464   0.072012  ]\n",
      "on iter:  92\n",
      "B max diff:  4610.03809067\n",
      "[ 0.02795126  0.04037454  0.02744228  0.07200443]\n",
      "iter took:  43.152892112731934\n",
      "new alpha:  [ 0.02795126  0.04037454  0.02744228  0.07200443]\n",
      "on iter:  93\n",
      "B max diff:  4610.03823102\n",
      "[ 0.02794767  0.04037032  0.02743872  0.07199789]\n",
      "iter took:  43.40873908996582\n",
      "new alpha:  [ 0.02794767  0.04037032  0.02743872  0.07199789]\n",
      "on iter:  94\n",
      "B max diff:  4610.03835695\n",
      "[ 0.02794458  0.04036667  0.02743563  0.07199225]\n",
      "iter took:  45.43678283691406\n",
      "new alpha:  [ 0.02794458  0.04036667  0.02743563  0.07199225]\n",
      "on iter:  95\n",
      "B max diff:  4610.03846834\n",
      "[ 0.0279419   0.04036351  0.02743297  0.07198737]\n",
      "iter took:  46.949280738830566\n",
      "new alpha:  [ 0.0279419   0.04036351  0.02743297  0.07198737]\n",
      "on iter:  96\n",
      "B max diff:  4610.03856668\n",
      "[ 0.02793959  0.04036078  0.02743066  0.07198313]\n",
      "iter took:  47.073431968688965\n",
      "new alpha:  [ 0.02793959  0.04036078  0.02743066  0.07198313]\n",
      "on iter:  97\n",
      "B max diff:  4610.03865331\n",
      "[ 0.02793759  0.04035843  0.02742867  0.07197947]\n",
      "iter took:  46.702712535858154\n",
      "new alpha:  [ 0.02793759  0.04035843  0.02742867  0.07197947]\n",
      "on iter:  98\n",
      "B max diff:  4610.03872926\n",
      "[ 0.02793586  0.04035639  0.02742694  0.07197633]\n",
      "iter took:  45.47147250175476\n",
      "new alpha:  [ 0.02793586  0.04035639  0.02742694  0.07197633]\n",
      "on iter:  99\n",
      "B max diff:  4610.03879511\n",
      "[ 0.02793437  0.04035462  0.02742545  0.0719736 ]\n",
      "iter took:  46.005878925323486\n",
      "new alpha:  [ 0.02793437  0.04035462  0.02742545  0.0719736 ]\n",
      "on iter:  100\n",
      "B max diff:  4610.03885259\n",
      "[ 0.02793308  0.0403531   0.02742416  0.07197123]\n",
      "iter took:  43.279048681259155\n",
      "new alpha:  [ 0.02793308  0.0403531   0.02742416  0.07197123]\n",
      "on iter:  101\n",
      "B max diff:  4610.03890274\n",
      "[ 0.02793196  0.04035178  0.02742304  0.07196921]\n",
      "iter took:  43.48173975944519\n",
      "new alpha:  [ 0.02793196  0.04035178  0.02742304  0.07196921]\n",
      "on iter:  102\n",
      "B max diff:  4610.03894587\n",
      "[ 0.027931    0.04035065  0.02742208  0.07196744]\n",
      "iter took:  46.70628571510315\n",
      "new alpha:  [ 0.027931    0.04035065  0.02742208  0.07196744]\n",
      "on iter:  103\n",
      "B max diff:  4610.03898346\n",
      "[ 0.02793017  0.04034966  0.02742124  0.07196591]\n",
      "iter took:  43.62609314918518\n",
      "new alpha:  [ 0.02793017  0.04034966  0.02742124  0.07196591]\n",
      "on iter:  104\n",
      "B max diff:  4610.03901623\n",
      "[ 0.02792945  0.04034881  0.02742052  0.07196458]\n",
      "iter took:  43.513102293014526\n",
      "new alpha:  [ 0.02792945  0.04034881  0.02742052  0.07196458]\n",
      "on iter:  105\n",
      "B max diff:  4610.03904477\n",
      "[ 0.02792882  0.04034807  0.0274199   0.07196345]\n",
      "iter took:  43.35747575759888\n",
      "new alpha:  [ 0.02792882  0.04034807  0.0274199   0.07196345]\n",
      "on iter:  106\n",
      "B max diff:  4610.03906891\n",
      "[ 0.02792829  0.04034744  0.02741936  0.07196247]\n",
      "iter took:  43.31005668640137\n",
      "new alpha:  [ 0.02792829  0.04034744  0.02741936  0.07196247]\n",
      "on iter:  107\n",
      "B max diff:  4610.03908996\n",
      "[ 0.02792782  0.04034689  0.02741889  0.07196161]\n",
      "iter took:  43.733675479888916\n",
      "new alpha:  [ 0.02792782  0.04034689  0.02741889  0.07196161]\n",
      "on iter:  108\n",
      "B max diff:  4610.03910835\n",
      "[ 0.02792742  0.04034641  0.02741848  0.07196088]\n",
      "iter took:  44.989736557006836\n",
      "new alpha:  [ 0.02792742  0.04034641  0.02741848  0.07196088]\n",
      "on iter:  109\n",
      "B max diff:  4610.0391239\n",
      "[ 0.02792707  0.040346    0.02741813  0.07196025]\n",
      "iter took:  44.41337537765503\n",
      "new alpha:  [ 0.02792707  0.040346    0.02741813  0.07196025]\n",
      "on iter:  110\n",
      "B max diff:  4610.0391375\n",
      "[ 0.02792677  0.04034564  0.02741783  0.07195969]\n",
      "iter took:  44.656954288482666\n",
      "new alpha:  [ 0.02792677  0.04034564  0.02741783  0.07195969]\n",
      "on iter:  111\n",
      "B max diff:  4610.03914946\n",
      "[ 0.02792651  0.04034534  0.02741758  0.07195921]\n",
      "iter took:  43.440200328826904\n",
      "new alpha:  [ 0.02792651  0.04034534  0.02741758  0.07195921]\n",
      "on iter:  112\n",
      "B max diff:  4610.03915975\n",
      "[ 0.02792628  0.04034507  0.02741735  0.07195881]\n",
      "iter took:  43.20366024971008\n",
      "new alpha:  [ 0.02792628  0.04034507  0.02741735  0.07195881]\n",
      "on iter:  113\n",
      "B max diff:  4610.03916845\n",
      "[ 0.02792609  0.04034484  0.02741716  0.07195845]\n",
      "iter took:  42.56967735290527\n",
      "new alpha:  [ 0.02792609  0.04034484  0.02741716  0.07195845]\n",
      "on iter:  114\n",
      "B max diff:  4610.03917611\n",
      "[ 0.02792592  0.04034464  0.02741699  0.07195814]\n",
      "iter took:  43.6342236995697\n",
      "new alpha:  [ 0.02792592  0.04034464  0.02741699  0.07195814]\n",
      "on iter:  115\n",
      "B max diff:  4610.0391829\n",
      "[ 0.02792577  0.04034447  0.02741685  0.07195787]\n",
      "iter took:  43.5010621547699\n",
      "new alpha:  [ 0.02792577  0.04034447  0.02741685  0.07195787]\n",
      "on iter:  116\n",
      "B max diff:  4610.03918865\n",
      "[ 0.02792565  0.04034432  0.02741673  0.07195764]\n",
      "iter took:  44.642884492874146\n",
      "new alpha:  [ 0.02792565  0.04034432  0.02741673  0.07195764]\n",
      "on iter:  117\n",
      "B max diff:  4610.03919371\n",
      "[ 0.02792554  0.04034419  0.02741663  0.07195743]\n",
      "iter took:  43.63312482833862\n",
      "new alpha:  [ 0.02792554  0.04034419  0.02741663  0.07195743]\n",
      "on iter:  118\n",
      "B max diff:  4610.03919818\n",
      "[ 0.02792544  0.04034408  0.02741653  0.07195726]\n",
      "iter took:  43.7165048122406\n",
      "new alpha:  [ 0.02792544  0.04034408  0.02741653  0.07195726]\n",
      "on iter:  119\n",
      "B max diff:  4610.03920192\n",
      "[ 0.02792536  0.04034399  0.02741645  0.07195712]\n",
      "iter took:  42.93173050880432\n",
      "new alpha:  [ 0.02792536  0.04034399  0.02741645  0.07195712]\n",
      "on iter:  120\n",
      "B max diff:  4610.03920495\n",
      "[ 0.02792529  0.04034391  0.02741638  0.071957  ]\n",
      "iter took:  43.13888382911682\n",
      "new alpha:  [ 0.02792529  0.04034391  0.02741638  0.071957  ]\n",
      "on iter:  121\n",
      "B max diff:  4610.03920762\n",
      "[ 0.02792523  0.04034384  0.02741633  0.07195688]\n",
      "iter took:  44.29888653755188\n",
      "new alpha:  [ 0.02792523  0.04034384  0.02741633  0.07195688]\n",
      "on iter:  122\n",
      "B max diff:  4610.03921004\n",
      "[ 0.02792518  0.04034377  0.02741628  0.07195678]\n",
      "iter took:  43.80475425720215\n",
      "new alpha:  [ 0.02792518  0.04034377  0.02741628  0.07195678]\n",
      "on iter:  123\n",
      "B max diff:  4610.03921223\n",
      "[ 0.02792513  0.04034372  0.02741623  0.07195669]\n",
      "iter took:  44.317996978759766\n",
      "new alpha:  [ 0.02792513  0.04034372  0.02741623  0.07195669]\n",
      "on iter:  124\n",
      "B max diff:  4610.03921421\n",
      "[ 0.02792509  0.04034367  0.02741619  0.07195661]\n",
      "iter took:  42.586026191711426\n",
      "new alpha:  [ 0.02792509  0.04034367  0.02741619  0.07195661]\n",
      "on iter:  125\n",
      "B max diff:  4610.03921598\n",
      "[ 0.02792506  0.04034363  0.02741616  0.07195654]\n",
      "iter took:  43.02195596694946\n",
      "new alpha:  [ 0.02792506  0.04034363  0.02741616  0.07195654]\n",
      "on iter:  126\n",
      "B max diff:  4610.03921756\n",
      "[ 0.02792503  0.04034359  0.02741612  0.07195648]\n",
      "iter took:  43.11247682571411\n",
      "new alpha:  [ 0.02792503  0.04034359  0.02741612  0.07195648]\n",
      "on iter:  127\n",
      "B max diff:  4610.03921885\n",
      "[ 0.027925    0.04034356  0.02741608  0.07195643]\n",
      "iter took:  41.89464783668518\n",
      "new alpha:  [ 0.027925    0.04034356  0.02741608  0.07195643]\n",
      "on iter:  128\n",
      "B max diff:  4610.03921998\n",
      "[ 0.02792498  0.04034353  0.02741606  0.07195638]\n",
      "iter took:  43.80586767196655\n",
      "new alpha:  [ 0.02792498  0.04034353  0.02741606  0.07195638]\n",
      "on iter:  129\n",
      "B max diff:  4610.03922099\n",
      "[ 0.02792496  0.04034351  0.02741604  0.07195634]\n",
      "iter took:  44.018882751464844\n",
      "new alpha:  [ 0.02792496  0.04034351  0.02741604  0.07195634]\n",
      "on iter:  130\n",
      "B max diff:  4610.03922192\n",
      "[ 0.02792494  0.04034348  0.02741602  0.0719563 ]\n",
      "iter took:  44.24419045448303\n",
      "new alpha:  [ 0.02792494  0.04034348  0.02741602  0.0719563 ]\n",
      "on iter:  131\n",
      "B max diff:  4610.03922277\n",
      "[ 0.02792492  0.04034346  0.027416    0.07195627]\n",
      "iter took:  45.495744705200195\n",
      "new alpha:  [ 0.02792492  0.04034346  0.027416    0.07195627]\n",
      "on iter:  132\n",
      "B max diff:  4610.03922354\n",
      "[ 0.02792491  0.04034346  0.02741599  0.07195626]\n",
      "iter took:  44.11780524253845\n",
      "new alpha:  [ 0.02792491  0.04034346  0.02741599  0.07195626]\n",
      "on iter:  133\n",
      "B max diff:  4610.03922386\n",
      "[ 0.0279249   0.04034345  0.02741598  0.07195625]\n",
      "iter took:  44.999470233917236\n",
      "new alpha:  [ 0.0279249   0.04034345  0.02741598  0.07195625]\n",
      "on iter:  134\n",
      "B max diff:  4610.03922407\n",
      "[ 0.02792489  0.04034344  0.02741597  0.07195624]\n",
      "iter took:  44.64618158340454\n",
      "new alpha:  [ 0.02792489  0.04034344  0.02741597  0.07195624]\n",
      "on iter:  135\n",
      "B max diff:  4610.03922425\n",
      "[ 0.02792488  0.04034343  0.02741596  0.07195623]\n",
      "iter took:  45.4171929359436\n",
      "new alpha:  [ 0.02792488  0.04034343  0.02741596  0.07195623]\n",
      "on iter:  136\n",
      "B max diff:  4610.03922444\n",
      "[ 0.02792487  0.04034342  0.02741595  0.07195622]\n",
      "iter took:  43.706854820251465\n",
      "new alpha:  [ 0.02792487  0.04034342  0.02741595  0.07195622]\n",
      "on iter:  137\n",
      "B max diff:  4610.03922468\n",
      "[ 0.02792487  0.04034341  0.02741595  0.07195619]\n",
      "iter took:  42.249473333358765\n",
      "new alpha:  [ 0.02792487  0.04034341  0.02741595  0.07195619]\n",
      "on iter:  138\n",
      "B max diff:  4610.03922515\n",
      "[ 0.02792486  0.0403434   0.02741594  0.07195618]\n",
      "iter took:  42.1413300037384\n",
      "new alpha:  [ 0.02792486  0.0403434   0.02741594  0.07195618]\n",
      "on iter:  139\n",
      "B max diff:  4610.03922543\n",
      "[ 0.02792486  0.0403434   0.02741594  0.07195617]\n",
      "iter took:  42.85682678222656\n",
      "new alpha:  [ 0.02792486  0.0403434   0.02741594  0.07195617]\n",
      "on iter:  140\n",
      "B max diff:  4610.03922568\n",
      "[ 0.02792485  0.04034339  0.02741593  0.07195616]\n",
      "iter took:  44.04140543937683\n",
      "new alpha:  [ 0.02792485  0.04034339  0.02741593  0.07195616]\n",
      "on iter:  141\n",
      "B max diff:  4610.03922596\n",
      "[ 0.02792485  0.04034338  0.02741593  0.07195614]\n",
      "iter took:  42.358356952667236\n",
      "new alpha:  [ 0.02792485  0.04034338  0.02741593  0.07195614]\n",
      "on iter:  142\n",
      "B max diff:  4610.03922626\n",
      "[ 0.02792484  0.04034337  0.02741592  0.07195613]\n",
      "iter took:  43.449974060058594\n",
      "new alpha:  [ 0.02792484  0.04034337  0.02741592  0.07195613]\n",
      "on iter:  143\n",
      "B max diff:  4610.03922658\n",
      "[ 0.02792484  0.04034337  0.02741592  0.07195611]\n",
      "iter took:  43.84769868850708\n",
      "new alpha:  [ 0.02792484  0.04034337  0.02741592  0.07195611]\n",
      "on iter:  144\n",
      "B max diff:  4610.03922691\n",
      "[ 0.02792483  0.04034336  0.02741592  0.0719561 ]\n",
      "iter took:  45.055121421813965\n",
      "new alpha:  [ 0.02792483  0.04034336  0.02741592  0.0719561 ]\n",
      "on iter:  145\n",
      "B max diff:  4610.03922722\n",
      "[ 0.02792483  0.04034336  0.02741591  0.07195609]\n",
      "iter took:  45.752442598342896\n",
      "new alpha:  [ 0.02792483  0.04034336  0.02741591  0.07195609]\n",
      "on iter:  146\n",
      "B max diff:  4610.03922751\n",
      "[ 0.02792483  0.04034335  0.02741591  0.07195607]\n",
      "iter took:  44.66798114776611\n",
      "new alpha:  [ 0.02792483  0.04034335  0.02741591  0.07195607]\n",
      "on iter:  147\n",
      "B max diff:  4610.03922776\n",
      "[ 0.02792483  0.04034335  0.02741591  0.07195607]\n",
      "iter took:  45.44462990760803\n",
      "new alpha:  [ 0.02792483  0.04034335  0.02741591  0.07195607]\n",
      "on iter:  148\n",
      "B max diff:  4610.03922786\n",
      "[ 0.02792483  0.04034335  0.02741591  0.07195606]\n",
      "iter took:  43.89295291900635\n",
      "new alpha:  [ 0.02792483  0.04034335  0.02741591  0.07195606]\n",
      "on iter:  149\n",
      "B max diff:  4610.03922804\n",
      "[ 0.02792483  0.04034335  0.02741591  0.07195606]\n",
      "iter took:  45.115965604782104\n",
      "new alpha:  [ 0.02792483  0.04034335  0.02741591  0.07195606]\n",
      "on iter:  150\n",
      "B max diff:  4610.03922809\n",
      "[ 0.02792483  0.04034335  0.02741591  0.07195606]\n",
      "iter took:  44.21429204940796\n",
      "new alpha:  [ 0.02792483  0.04034335  0.02741591  0.07195606]\n",
      "on iter:  151\n",
      "B max diff:  4610.0392281\n",
      "[ 0.02792482  0.04034334  0.02741591  0.07195606]\n",
      "iter took:  45.42608428001404\n",
      "new alpha:  [ 0.02792482  0.04034334  0.02741591  0.07195606]\n",
      "on iter:  152\n",
      "B max diff:  4610.03922823\n",
      "[ 0.02792482  0.04034334  0.02741591  0.07195605]\n",
      "iter took:  44.959537506103516\n",
      "new alpha:  [ 0.02792482  0.04034334  0.02741591  0.07195605]\n",
      "on iter:  153\n",
      "B max diff:  4610.03922838\n",
      "[ 0.02792482  0.04034334  0.0274159   0.07195604]\n",
      "iter took:  44.831096172332764\n",
      "new alpha:  [ 0.02792482  0.04034334  0.0274159   0.07195604]\n",
      "on iter:  154\n",
      "B max diff:  4610.03922853\n",
      "[ 0.02792482  0.04034334  0.0274159   0.07195604]\n",
      "iter took:  43.16325283050537\n",
      "new alpha:  [ 0.02792482  0.04034334  0.0274159   0.07195604]\n",
      "on iter:  155\n",
      "B max diff:  4610.03922865\n",
      "[ 0.02792482  0.04034334  0.0274159   0.07195604]\n",
      "iter took:  44.44404745101929\n",
      "new alpha:  [ 0.02792482  0.04034334  0.0274159   0.07195604]\n",
      "on iter:  156\n",
      "B max diff:  4610.03922869\n",
      "[ 0.02792482  0.04034334  0.0274159   0.07195604]\n",
      "iter took:  45.242828607559204\n",
      "new alpha:  [ 0.02792482  0.04034334  0.0274159   0.07195604]\n",
      "on iter:  157\n",
      "B max diff:  4610.0392287\n",
      "[ 0.02792482  0.04034333  0.0274159   0.07195603]\n",
      "iter took:  45.288660287857056\n",
      "new alpha:  [ 0.02792482  0.04034333  0.0274159   0.07195603]\n",
      "on iter:  158\n",
      "B max diff:  4610.03922878\n",
      "[ 0.02792482  0.04034333  0.0274159   0.07195603]\n",
      "iter took:  43.109081983566284\n",
      "new alpha:  [ 0.02792482  0.04034333  0.0274159   0.07195603]\n",
      "on iter:  159\n",
      "B max diff:  4610.03922887\n",
      "[ 0.02792482  0.04034333  0.0274159   0.07195602]\n",
      "iter took:  42.709667444229126\n",
      "new alpha:  [ 0.02792482  0.04034333  0.0274159   0.07195602]\n",
      "on iter:  160\n",
      "B max diff:  4610.03922895\n",
      "[ 0.02792482  0.04034333  0.0274159   0.07195602]\n",
      "iter took:  42.86721086502075\n",
      "new alpha:  [ 0.02792482  0.04034333  0.0274159   0.07195602]\n",
      "on iter:  161\n",
      "B max diff:  4610.03922902\n",
      "[ 0.02792482  0.04034333  0.0274159   0.07195602]\n",
      "iter took:  44.99603986740112\n",
      "new alpha:  [ 0.02792482  0.04034333  0.0274159   0.07195602]\n",
      "on iter:  162\n",
      "B max diff:  4610.03922904\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195602]\n",
      "iter took:  44.93308067321777\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195602]\n",
      "on iter:  163\n",
      "B max diff:  4610.03922909\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195602]\n",
      "iter took:  44.400670289993286\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195602]\n",
      "on iter:  164\n",
      "B max diff:  4610.03922912\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195602]\n",
      "iter took:  43.18463110923767\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195602]\n",
      "on iter:  165\n",
      "B max diff:  4610.03922916\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "iter took:  42.37566137313843\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "on iter:  166\n",
      "B max diff:  4610.0392292\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "iter took:  45.036712408065796\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "on iter:  167\n",
      "B max diff:  4610.03922922\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "iter took:  43.47558808326721\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "on iter:  168\n",
      "B max diff:  4610.03922922\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "iter took:  43.39620518684387\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "on iter:  169\n",
      "B max diff:  4610.03922923\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "iter took:  42.52782917022705\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "on iter:  170\n",
      "B max diff:  4610.03922926\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "iter took:  43.76821446418762\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "on iter:  171\n",
      "B max diff:  4610.03922929\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "iter took:  43.00676512718201\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "on iter:  172\n",
      "B max diff:  4610.03922931\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "iter took:  43.06563925743103\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "on iter:  173\n",
      "B max diff:  4610.03922934\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "iter took:  42.08598208427429\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "on iter:  174\n",
      "B max diff:  4610.03922935\n",
      "[ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "iter took:  43.400572061538696\n",
      "new alpha:  [ 0.02792481  0.04034333  0.0274159   0.07195601]\n",
      "on iter:  175\n",
      "B max diff:  4610.03922937\n",
      "[ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "iter took:  42.16820502281189\n",
      "new alpha:  [ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "on iter:  176\n",
      "B max diff:  4610.03922938\n",
      "[ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "iter took:  43.42751407623291\n",
      "new alpha:  [ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "on iter:  177\n",
      "B max diff:  4610.03922939\n",
      "[ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "iter took:  45.50345849990845\n",
      "new alpha:  [ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "on iter:  178\n",
      "B max diff:  4610.0392294\n",
      "[ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "iter took:  44.27274203300476\n",
      "new alpha:  [ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "on iter:  179\n",
      "B max diff:  4610.0392294\n",
      "[ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "iter took:  44.11769723892212\n",
      "new alpha:  [ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "on iter:  180\n",
      "B max diff:  4610.03922941\n",
      "[ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "iter took:  44.81632351875305\n",
      "new alpha:  [ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "on iter:  181\n",
      "B max diff:  4610.03922941\n",
      "[ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "iter took:  44.00898098945618\n",
      "new alpha:  [ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "on iter:  182\n",
      "B max diff:  4610.03922942\n",
      "[ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "iter took:  43.720436096191406\n",
      "new alpha:  [ 0.02792481  0.04034332  0.0274159   0.07195601]\n",
      "on iter:  183\n",
      "B max diff:  4610.03922943\n",
      "[ 0.02792481  0.04034332  0.0274159   0.071956  ]\n",
      "iter took:  42.65188813209534\n",
      "new alpha:  [ 0.02792481  0.04034332  0.0274159   0.071956  ]\n",
      "on iter:  184\n",
      "B max diff:  4610.03922944\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  42.54602527618408\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  185\n",
      "B max diff:  4610.03922945\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  42.99135684967041\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  186\n",
      "B max diff:  4610.03922945\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  42.24438238143921\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  187\n",
      "B max diff:  4610.03922946\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  43.668758153915405\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  188\n",
      "B max diff:  4610.03922946\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  44.34520244598389\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  189\n",
      "B max diff:  4610.03922946\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  43.968087673187256\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  190\n",
      "B max diff:  4610.03922947\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  42.11973595619202\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  191\n",
      "B max diff:  4610.03922947\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  43.79196572303772\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  192\n",
      "B max diff:  4610.03922947\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  41.82699918746948\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  193\n",
      "B max diff:  4610.03922948\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  43.35914468765259\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  194\n",
      "B max diff:  4610.03922948\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  43.42845606803894\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  195\n",
      "B max diff:  4610.03922948\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  43.464261531829834\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  196\n",
      "B max diff:  4610.03922948\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  42.920249223709106\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  197\n",
      "B max diff:  4610.03922948\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  43.41048741340637\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  198\n",
      "B max diff:  4610.03922948\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  43.03132677078247\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "on iter:  199\n",
      "B max diff:  4610.03922949\n",
      "[ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "iter took:  44.01923489570618\n",
      "new alpha:  [ 0.02792481  0.04034332  0.02741589  0.071956  ]\n",
      "took:  44.019774436950684\n"
     ]
    }
   ],
   "source": [
    "eta = 10e-5 #threshold for convergence\n",
    "\n",
    "#hyperparamater init.\n",
    "V = len(vectorizer.get_feature_names()) #vocab. cardinality\n",
    "M = int(len(trainDocs)) #number of documents\n",
    "k = 4 #amount of emotions\n",
    "\n",
    "nIter=200 # number of iterations to run until parameter estimation is considered converged\n",
    "\n",
    "#initialize B matrix as random valid distr (most common according to https://profs.sci.univr.it/~bicego/papers/2015_SIMBAD.pdf)\n",
    "B = np.random.rand(k,V)\n",
    "\n",
    "#normalize B\n",
    "for i in range(k):\n",
    "    B[i,:] = B[i]/np.sum(B[i])\n",
    "    \n",
    "alpha = np.ones(k)\n",
    "#variational params (one for each doc)\n",
    "phi = [None]*M\n",
    "gamma = [None]*M\n",
    "\n",
    "#word lists for all docs\n",
    "Wd = [None]*M\n",
    "\n",
    "'''Since scikit gives a matrix of counts of all words, and we want a list of words,\n",
    "we do some quick transformations here. This gives us a representation of the documents \n",
    "as a list of numbers, where each number is the vocabulary index of a word. This way, to access\n",
    "B_ij where i is the ith topic and j is the nth word in the document d, you can simply write B[i][Wd[d][n]]. If you want\n",
    "replace this code a different representation for the words in a document, such as a one-hot vector for each word, you are\n",
    "of course free to do so but make sure to keep track of your indexes'''\n",
    "\n",
    "for d in range(M):\n",
    "    Wmat = vectorizer.transform([trainDocs[d]]).toarray()[0] #get vocabulary matrix for document\n",
    "    WVidxs = np.where(Wmat!=0)[0]\n",
    "    WVcounts = Wmat[WVidxs]\n",
    "    N = np.sum(WVcounts)\n",
    "    W = np.zeros((N)).astype(int)\n",
    "\n",
    "    i = 0\n",
    "    for WVidx, WV in enumerate(WVidxs):\n",
    "        for wordCount in range(WVcounts[WVidx]):\n",
    "            W[i] = WV\n",
    "            i+=1\n",
    "    Wd[d] = W #We save the list of words for the document for analysis later\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#start of parameter estimation loop\n",
    "for j in range(nIter):\n",
    "    print(\"on iter: \", j)\n",
    "    #Variational EM for gamma and phi (E-step from VI section)\n",
    "    start = time.time()\n",
    "    gamma, phi = maxVIParam(phi, gamma, B, alpha, M, k, Wd, eta)\n",
    "    end = time.time()\n",
    "    #Bold = np.copy(B)\n",
    "    B = MaxB(B,phi,k,V,M,Wd) #first half of M-step from VI section \n",
    "    #renormalize B\n",
    "    for i in range(k):\n",
    "        B[i,:] = B[i]/np.sum(B[i])\n",
    "    \n",
    "    #print(\"B max diff: \", np.amax(abs(B-Bold)))\n",
    "    end = time.time()\n",
    "    \n",
    "    alpha = MaxA(alpha) #second half of M-step from VI section \n",
    "    end = time.time()\n",
    "    print(\"iter took: \", end-start)\n",
    "    print(\"new alpha: \", alpha)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"took: \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI Parameter Estimation\n",
    "We can now begin with implementing the \"E step\" in the previous section which updates the variational parameters. The pseudo code for this is the following (remember these have to be calculated separately for each document):\n",
    "![VIPseudo](imgs/VIPseudo.png)\n",
    "\n",
    "Since we are working with four topics, k will be set to 4 and N will be the amount of words in the current document. Regarding the \"until convergence\" condition, it is sufficient to check if the largest difference between the previous and new gamma is less than $10^{-5}$. Now, use the pseudo code to fill in the missing code in the \"MaxVIParam\" function defined earlier and remember to use the provided diGamma function. To see that your implementation seems to be working, set nIter to 1 and add some printouts of the difference between updates for gamma, then check that they are converging to something smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beta Matrix Estimation\n",
    "After you have implemented the MaxVIParam function, it's time to update the Beta matrix. Recall that the update function for Beta was:\n",
    "\\begin{equation}\n",
    "\\beta_{ij}\\propto\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\phi^*_{dni}w^j_{dn}\n",
    "\\end{equation}\n",
    "Implement this in the definition for MaxB above. To verify your code, you may set nIter to something low such as 10 and uncomment the \"oldB\" and \"B max diff\" lines in the code. The diff might increase at first but should start decreasing before/around iteration 10. After you have verified this, set nIter to 100 (updates should be negligable by then) and let it run unattended as it might take a couple hours. You can use the code in the following cell to save/load the parameter values you calculated for later so you don't have to re-run everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(alpha, open(\"myAlphaNews100.p\", \"wb\"))\n",
    "pickle.dump(B, open(\"myBetaNews100.p\", \"wb\"))\n",
    "\n",
    "#alpha = pickle.load(open(\"myAlphaNews.p\", \"rb\"))\n",
    "#B = pickle.load(open(\"myBetaNews.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification\n",
    "Let's take a look at what we've done so far. We can get an idea of what our implementation has done up to this point by inspecting the B matrix. As you may remember, B$_{ij}$ holds the probability of a vocabulary word j being representative of a certain topic i. Using the code in the following cell we can see the most representative words for our 4 topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for topic  0 : \n",
      "['data' 'satellite' 'launch' 'spacecraft' 'development' 'national' 'world'\n",
      " 'question' 'new' 'science' 'year' 'distribution' '1993' 'high' 'program'\n",
      " 'nntp' 'nasa' 'post' 'use' 'space']\n",
      "top words for topic  1 : \n",
      "['reason' 'bible' 'word' 'like' 'make' 'use' 'come' 'way' 'christ' 'thing'\n",
      " 'question' 'good' 'think' 'time' 'believe' 'christian' 'know' 'say'\n",
      " 'people' 'god']\n",
      "top words for topic  2 : \n",
      "['blue' 'point' '92' '1992' '93' '90' 'star' '86' 'red' 'playoff' 'wing'\n",
      " 'contact' 'nhl' 'season' 'game' 'hockey' 'player' 'year' 'play' 'team']\n",
      "top words for topic  3 : \n",
      "['usa' 'work' 'make' 'look' 'really' 'thing' 'way' 'year' 'come' 'use'\n",
      " 'people' 'good' 'nntp' 'like' 'say' 'time' 'car' 'know' 'post' 'think']\n"
     ]
    }
   ],
   "source": [
    "#representation of top words for each topic:\n",
    "nTop = 20\n",
    "for i in range(k):\n",
    "    topVocabs = np.argsort(B[i])[-nTop:]\n",
    "    topWords = np.array(vectorizer.get_feature_names())[topVocabs]\n",
    "    print(\"top words for topic \",i,\": \")\n",
    "    print(topWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no guarantees regarding the order of the topics (LDA is unsupervised) or what your initial B matrix values were, it is difficult to say exactly what results you should be seeing. Hopefully, you can see the four original topics to some extent in your result. For example, one of my topics had top words like \"christ\", and \"god\", meaning it was most likely the topic for \"Christian Religion\" documents, while another literally had the words \"Hockey\" and \"NHL\" in it. Our vocabulary is as mentioned earlier quite limited, so it may be possible that one of your topics is a bit unclear or close to another. You can also load the $\\alpha$ and $\\beta$ values in the cell below which are pre-calculated for 200 iterations and compare your topic results to those available there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for topic  0 : \n",
      "['data' 'satellite' 'launch' 'spacecraft' 'development' 'national' 'world'\n",
      " 'question' 'new' 'science' 'year' 'distribution' '1993' 'high' 'program'\n",
      " 'nntp' 'nasa' 'post' 'use' 'space']\n",
      "top words for topic  1 : \n",
      "['reason' 'bible' 'word' 'like' 'make' 'use' 'come' 'way' 'christ' 'thing'\n",
      " 'question' 'good' 'think' 'time' 'believe' 'christian' 'know' 'say'\n",
      " 'people' 'god']\n",
      "top words for topic  2 : \n",
      "['blue' 'point' '92' '1992' '93' '90' 'star' '86' 'red' 'playoff' 'wing'\n",
      " 'contact' 'nhl' 'season' 'game' 'hockey' 'player' 'year' 'play' 'team']\n",
      "top words for topic  3 : \n",
      "['usa' 'work' 'make' 'look' 'really' 'thing' 'way' 'year' 'come' 'use'\n",
      " 'people' 'good' 'nntp' 'like' 'say' 'time' 'car' 'know' 'post' 'think']\n"
     ]
    }
   ],
   "source": [
    "alphaTest = pickle.load(open(\"CompareAlphaNews200.p\", \"rb\"))\n",
    "BTest = pickle.load(open(\"CompareBetaNews200.p\", \"rb\"))\n",
    "vecTest = pickle.load(open(\"vectorizerNews.p\", \"rb\"), encoding='latin1')\n",
    "nTop = 20\n",
    "for i in range(k):\n",
    "    topVocabs = np.argsort(BTest[i])[-nTop:]\n",
    "    topWords = np.array(vecTest.get_feature_names())[topVocabs]\n",
    "    print(\"top words for topic \",i,\": \")\n",
    "    print(topWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the topic of a test document\n",
    "\n",
    "In this section we will be using our estimated parameter values to infer the topic of some test documents. In order to do this, we will have to calculate the phi and gamma values for each new document we would like to do inference on. This is rather straight forward, and you should be able to reuse your code from the previous sections together with the test documents as a corpus instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "eta = 10e-5 #threshold for convergence\n",
    "\n",
    "#we are not re-initializing beta and alpha, we calculated them using the training docs.\n",
    "\n",
    "V = len(vectorizer.get_feature_names()) #vocab. cardinality\n",
    "M = int(len(testDocs)) #number of documents\n",
    "k = 4 #amount of emotions\n",
    "\n",
    "#variational params (one for each doc)\n",
    "phi = [None]*M\n",
    "gamma = [None]*M\n",
    "WdTest = [None]*M\n",
    "\n",
    "'''Same magic from before to get the word matrix correct, replace this if you redid this earlier.'''\n",
    "\n",
    "for d in range(M):\n",
    "    Wmat = vectorizer.transform([testDocs[d]]).toarray()[0] #get vocabulary matrix for document\n",
    "    WVidxs = np.where(Wmat!=0)[0]\n",
    "    WVcounts = Wmat[WVidxs]\n",
    "    N = np.sum(WVcounts)\n",
    "    W = np.zeros((N)).astype(int)\n",
    "\n",
    "    i = 0\n",
    "    for WVidx, WV in enumerate(WVidxs):\n",
    "        for wordCount in range(WVcounts[WVidx]):\n",
    "            W[i] = WV\n",
    "            i+=1\n",
    "    WdTest[d] = W #We save the list of words for the document for analysis later\n",
    "\n",
    "'''Now that you have your variables initialized for the test documents, you should be able to use your previous code for \n",
    "maximizing the VI parameters with those variables instead. Remember, we're just calculating the variational parameters\n",
    "gamma and phi for each test document so there is no iteration between maximizing Beta and maximizing gamma and phi.'''\n",
    "\n",
    "#Run the gamma/phi maximization here.\n",
    "gamma, phi = maxVIParam(phi, gamma, B, alpha, M, k, WdTest, eta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now calculated the variational parameters for our test documents, so let us see what information we can infer from them. If you take a look at the pseudo code we used for the MaxVIParam method, you can see that the posterior gamma parameter $\\gamma_i $we are calculating is approximately the prior Dichlet parameter $\\alpha_i$ added to the expected number of words that were generated by that $i^{th}$ topic for a certain document. Looking at the values for the different $\\gamma_i$ over all words for a test document tells us what mixture of topics form such a document. Let us now take a look at the mixtures for some of our test documents by running the code in the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated mixture for document  14  is: \n",
      "_______________________\n",
      "topic  0 :  0.000592033243315\n",
      "topic  1 :  0.450022968527\n",
      "topic  2 :  0.297347701863\n",
      "topic  3 :  0.252037296367\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: colombo@bronco.fnal.gov (Rick 'Open VMS 4ever' Colombo)\n",
      "Subject: Re: Do trains/busses have radar?\n",
      "Nntp-Posting-Host: bronco.fnal.gov\n",
      "Organization: Fermilab Computing Division\n",
      "Lines: 27\n",
      "\n",
      "In article <C5FqFy.Fpq@usenet.ucs.indiana.edu>, mliggett@silver.ucs.indiana.edu (matthew liggett) writes:\n",
      "> In <1993Apr13.111652@usho72.hou281.chevron.com> hhtra@usho72.hou281.chevron.com (T.M.Haddock) writes:\n",
      "> \n",
      "> \n",
      ">> While taking an extended Easter vacation, I was going north on I-45\n",
      ">> somewhere between Centerville, TX and Dallas, TX and I came upon a \n",
      ">> train parked on a trestle with its locomotive sitting directly over\n",
      ">> the northbound lanes.  There appeared to be movement within the cab \n",
      ">> and out of curiosity I slowed to 85 to get a better look.  Just as I\n",
      ">> passed from underneath the trestle, my radar detector went into full \n",
      ">> alert - all lights lit and all chirps, beeps, and buzzes going strong.\n",
      ">> I thought I had been nailed good but no police materialized.\n",
      "> \n",
      ">> Could this have been caused by the train's radio or what?\n",
      "> \n",
      ">\n",
      "\n",
      "I don't know about trains, but I've saw a sign on the back of a\n",
      "Greyhound bus that warns you that your radar detector may be set off.\n",
      "It doesn't explain why, but it does set off my radar detector.\n",
      "\n",
      "___________________________________________________________________________\n",
      "*****  *   *  From the e-net desk of: Rick Colombo CD/DCD/DSG    *    *\n",
      "*      **  *  Fermi Nat'l Acc'l Lab   708-840-8225 Fermilab     * *   *\n",
      "***    * * *  P.O. Box 500   MS 369   Feynman Computer Center  *****  *\n",
      "*      *  **  Batavia, Ill. USA 60510 Colombo@fnal.fnal.gov    *   *  *****\n",
      "*    Of course I speak for: Fermilab, Congress and the President... NOT!!!\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  15  is: \n",
      "_______________________\n",
      "topic  0 :  0.999291386588\n",
      "topic  1 :  0.000204614321639\n",
      "topic  2 :  0.000139048651822\n",
      "topic  3 :  0.000364950438086\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: buenneke@monty.rand.org (Richard Buenneke)\n",
      "Subject: White House outlines options for station, Russian cooperation\n",
      "X-Added: Forwarded by Space Digest\n",
      "Organization: [via International Space University]\n",
      "Original-Sender: isu@VACATION.VENARI.CS.CMU.EDU\n",
      "Distribution: sci\n",
      "Lines: 71\n",
      "\n",
      "------- Blind-Carbon-Copy\n",
      "\n",
      "To: spacenews@austen.rand.org, cti@austen.rand.org\n",
      "Subject: White House outlines options for station, Russian cooperation\n",
      "Date: Tue, 06 Apr 93 16:00:21 PDT\n",
      "From: Richard Buenneke <buenneke@austen.rand.org>\n",
      "\n",
      "4/06/93:  GIBBONS OUTLINES SPACE STATION REDESIGN GUIDANCE\n",
      "\n",
      "NASA Headquarters, Washington, D.C.\n",
      "April 6, 1993\n",
      "\n",
      "RELEASE:  93-64\n",
      "\n",
      "        Dr.  John H.  Gibbons, Director, Office of Science and Technology\n",
      "Policy, outlined to the members-designate of the Advisory Committee on the\n",
      "Redesign of the Space Station on April 3, three budget options as guidance\n",
      "to the committee in their deliberations on the redesign of the space\n",
      "station.\n",
      "\n",
      "        A low option of $5 billion, a mid-range option of $7 billion and a\n",
      "high option of $9 billion will be considered by the committee.  Each\n",
      "option would cover the total expenditures for space station from fiscal\n",
      "year 1994 through 1998 and would include funds for development,\n",
      "operations, utilization, Shuttle integration, facilities, research\n",
      "operations support, transition cost and also must include adequate program\n",
      "reserves to insure program implementation within the available funds.\n",
      "\n",
      "        Over the next 5 years, $4 billion is reserved within the NASA\n",
      "budget for the President's new technology investment.  As a result,\n",
      "station options above $7 billion must be accompanied by offsetting\n",
      "reductions in the rest of the NASA budget.  For example, a space station\n",
      "option of $9 billion would require $2 billion in offsets from the NASA\n",
      "budget over the next 5 years.\n",
      "\n",
      "        Gibbons presented the information at an organizational session of\n",
      "the advisory committee.  Generally, the members-designate focused upon\n",
      "administrative topics and used the session to get acquainted.  They also\n",
      "received a legal and ethics briefing and an orientation on the process the\n",
      "Station Redesign Team is following to develop options for the advisory\n",
      "committee to consider.\n",
      "\n",
      "        Gibbons also announced that the United States and its\n",
      "international partners -- the Europeans, Japanese and Canadians -- have\n",
      "decided, after consultation, to give \"full consideration\" to use of\n",
      "Russian assets in the course of the space station redesign process.\n",
      "\n",
      "        To that end, the Russians will be asked to participate in the\n",
      "redesign effort on an as-needed consulting basis, so that the redesign\n",
      "team can make use of their expertise in assessing the capabilities of MIR\n",
      "and the possible use of MIR and other Russian capabilities and systems.\n",
      "The U.S. and international partners hope to benefit from the expertise of\n",
      "the Russian participants in assessing Russian systems and technology.  The\n",
      "overall goal of the redesign effort is to develop options for reducing\n",
      "station costs while preserving key research and exploration capabilitiaes.\n",
      "Careful integration of Russian assets could be a key factor in achieving\n",
      "that goal.\n",
      "\n",
      "        Gibbons reiterated that, \"President Clinton is committed to the\n",
      "redesigned space station and to making every effort to preserve the\n",
      "science, the technology and the jobs that the space station program\n",
      "represents.  However, he also is committed to a space station that is well\n",
      "managed and one that does not consume the national resources which should\n",
      "be used to invest in the future of this industry and this nation.\"\n",
      "\n",
      "        NASA Administrator Daniel S.  Goldin said the Russian\n",
      "participation will be accomplished through the East-West Space Science\n",
      "Center at the University of Maryland under the leadership of Roald\n",
      "Sagdeev.\n",
      "\n",
      "------- End of Blind-Carbon-Copy\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  16  is: \n",
      "_______________________\n",
      "topic  0 :  0.000464116772615\n",
      "topic  1 :  0.709513230286\n",
      "topic  2 :  0.215741760974\n",
      "topic  3 :  0.0742808919672\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: jodfishe@silver.ucs.indiana.edu (joseph dale fisher)\n",
      "Subject: Re: anger\n",
      "Organization: Indiana University\n",
      "Lines: 34\n",
      "\n",
      "In article <Apr.17.01.10.44.1993.2232@geneva.rutgers.edu> news@cbnewsk.att.com writes:\n",
      ">>Paul Conditt writes:\n",
      "[insert deletion of Paul's and Aaron's discourse on anger, ref Galatians\n",
      "5:19-20]\n",
      ">\n",
      ">I don't know why it is so obvious.  We are not speaking of acts of the \n",
      ">flesh.  We are just speaking of emotions.  Emotions are not of themselves\n",
      ">moral or immoral, good or bad.  Emotions just are.  The first step is\n",
      ">not to label his emotion as good or bad or to numb ourselves so that\n",
      ">we hide our true feelings, it is to accept ourselves as we are, as God\n",
      ">accepts us.  \n",
      "\n",
      "Oh, but they definitely can be.  Please look at Colossians 3:5-10 and\n",
      "Ephesians 4:25-27.  Emotions can be controlled and God puts very strong\n",
      "emphasis on self-control, otherwise, why would he have Paul write to\n",
      "Timothy so much about making sure to teach self-control? \n",
      "\n",
      "[insert deletion of remainder of paragraph]\n",
      "\n",
      ">\n",
      ">Re-think it, Aaron.  Don't be quick to judge.  He has forgiven those with\n",
      ">AIDS, he has dealt with and taken responsibility for his feelings and made\n",
      ">appropriate choices for action on such feelings.  He has not given in to\n",
      ">his anger.\n",
      "\n",
      "Please, re-think and re-read for yourself, Joe.  Again, the issue is\n",
      "self-control especially over feelings and actions, for our actions stem\n",
      "from our feelings in many instances.  As for God giving in to his anger,\n",
      "that comes very soon.\n",
      "\n",
      ">\n",
      ">Joe Moore\n",
      "\n",
      "Joe Fisher\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  17  is: \n",
      "_______________________\n",
      "topic  0 :  0.164994728868\n",
      "topic  1 :  0.000705702087701\n",
      "topic  2 :  0.833040873657\n",
      "topic  3 :  0.00125869538758\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: gballent@hudson.UVic.CA (Greg  Ballentine)\n",
      "Subject: Re: plus minus stat\n",
      "Nntp-Posting-Host: hudson.uvic.ca\n",
      "Reply-To: gballent@hudson.UVic.CA\n",
      "Organization: University of Victoria, Victoria, BC, Canada\n",
      "Lines: 24\n",
      "\n",
      "\n",
      "In article 1@tnclus.tele.nokia.fi, hahietanen@tnclus.tele.nokia.fi () writes:\n",
      ">In article <1993Apr14.174139.6604@sol.UVic.CA>, gballent@vancouver.UVic.CA (Greg  Ballentine) writes:\n",
      ">> \n",
      ">> \n",
      ">> +/- is a good stat because it is the only stat that I am aware of that\n",
      ">> takes into account defensive play.  It isn't a measure of defensive\n",
      ">> play- it takes into account offense and defence- all aspects of play.\n",
      ">                                                   \n",
      ">  If we are interested of real all-round players, the power play stats\n",
      ">  should be considered, too. Because the power play is also one aspect \n",
      ">  of play! There is still something to be done with these player evaluation\n",
      ">  tools!!\n",
      "\n",
      "IMO any good player should score on power plays because of the man\n",
      "advantage.  Very good power play scorers tend to become overrated\n",
      "because their point totals are inflated by power play points.\n",
      "+/- tends to expose these overrated players such as Brett Hull,\n",
      "John Cullen and Dave Andreychuck.\n",
      "\n",
      "Given the opportunity to play power play consistently, any player can\n",
      "inflate his totals.\n",
      "\n",
      "Gregmeister\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  18  is: \n",
      "_______________________\n",
      "topic  0 :  0.0538582801966\n",
      "topic  1 :  0.000731286002036\n",
      "topic  2 :  0.00049695608719\n",
      "topic  3 :  0.944913477714\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: kking@cs.uah.edu (Ken King)\n",
      "Subject: Re: The Kuebelwagen??!!          \n",
      "Reply-To: kking@uahcs2.uah.edu (Ken King)\n",
      "Organization: Computer Science Dept. - Univ. of Alabama in Huntsville\n",
      "Lines: 36\n",
      "\n",
      "In article <C5K5Co.F09@mentor.cc.purdue.edu> thwang@mentor.cc.purdue.edu (Tommy Hwang) writes:\n",
      ">\tSorry for the mis-spelling, but I forgot how to spell it after \n",
      ">my series of exams and NO-on hand reference here.\n",
      ">\n",
      ">\tIs it still possible to get those cute WWII VW Jeep-wanna-be's?\n",
      ">A replica would be great I think.  \n",
      "\n",
      "  greetings:\n",
      "  you may be in luck.  i seem to recall seeing a blurb in one of\n",
      "the kit car magazines about a company in norway who pulled a\n",
      "mould (sp?) off a real kubel, and has adapted it to the beetle\n",
      "floorpan.  as for the suspension, all i can remember about the\n",
      "vw thing i used to own is that it had about 3\" more suspension\n",
      "travel than a stock beetle, but i'd heard that there were after-\n",
      "market parts for off-road use that were as good or better.  note\n",
      "that the major difference (looks wise) between a kubel & a thing\n",
      "are the hood and the fenders.  the kubel had an external spare\n",
      "mounted *on* the hood, and the hood sloped down (for visibility?)\n",
      "sharply, and had rounded fenders.  the thing has a lightly sloped \n",
      "hood with the spare mounted inside (unless moved to make for more\n",
      "luggage space...) and has half-hexagon shaped fenders (imagine a\n",
      "nut large enough to put a tire *in*, and cut off the bottom half\n",
      "of it...).\n",
      "  unfortunately, i don't have that info anymore.  try stopping\n",
      "at a local bookstore and copying down the phone numbers for the\n",
      "two big mag's and calling them.  they might be able to get the\n",
      "number for you (don't forget to calculate the time difference to\n",
      "norway before calling...).\n",
      "\n",
      "later,\n",
      "kc\n",
      "-- \n",
      "          ___==A==___          | Quick Bones, help me get | #include \n",
      "  .---====   ( o )   ====---.  | this Klingon off my *ss! |  <std/disclaimer.h>\n",
      " /        ~~~~~~~~~~~        \\ | Damn it, Jim, I'm a      | \n",
      " ()     kking@cs.uah.edu    () | doctor, not a bidet!  :) | \n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  19  is: \n",
      "_______________________\n",
      "topic  0 :  0.000646892229619\n",
      "topic  1 :  0.00093457327956\n",
      "topic  2 :  0.349951329202\n",
      "topic  3 :  0.648467205289\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: LMARSHA@cms.cc.wayne.edu (Laurie Marshall)\n",
      "Subject: Re: If You Were Pat Burns ...\n",
      "Organization: Wayne State University, Detroit MI  U.S.A.\n",
      "Lines: 30\n",
      "NNTP-Posting-Host: cms.cc.wayne.edu\n",
      "\n",
      "In article <1r1chb$5l2@jethro.Corp.Sun.COM>\n",
      "jake@rambler.Eng.Sun.COM (Jason Cockroft) writes:\n",
      " \n",
      ">Suggestions:  Clarke-Anderson-Gilmour vs. Sheppard-Yserbeart-??\n",
      ">              Andreychuck-Borchevsy-??  vs. Detroit checking line\n",
      ">              Toronto's checking line  vs. Yzerman-Fedorov-Probert (pray lots)\n",
      ">\n",
      " \n",
      " Well, I'm a Wings fan and I think the FIRST thing that you should do is to\n",
      "get the opponent's line combinations correct before you try to match up anyone\n",
      "with them.  There is no Yzerman-Fedorov-Probert line, except for maybe on a\n",
      "powerplay.  These three players usually play on three different lines.\n",
      "Which would mean that Toronto's checking line would have to pull a triple\n",
      "shift.\n",
      "The Wings' lines usually look like this:\n",
      " \n",
      "                      Gallant-Yzerman-Ciccarelli\n",
      " \n",
      "                      Kozlov-Fedorov-Drake\n",
      " \n",
      "                      Kennedy-Burr-Probert\n",
      " \n",
      "                      Ysebaert-Primeau-Sheppard\n",
      " \n",
      "Oh by the way:  Start praying! : )\n",
      " \n",
      "Laurie Marshall\n",
      "Wayne State University\n",
      "Detroit, Michigan\n",
      "Go Wings!!!!!\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  20  is: \n",
      "_______________________\n",
      "topic  0 :  0.029203914572\n",
      "topic  1 :  0.970481782518\n",
      "topic  2 :  8.67131580151e-05\n",
      "topic  3 :  0.000227589751729\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: jasons@atlastele.com (Jason Smith)\n",
      "Subject: Re: Atheist's views on Christianity (was: Re: \"Accepting Jeesus in your heart...\")\n",
      "Organization: Atlas Telecom Inc.\n",
      "Lines: 169\n",
      "\n",
      "In article <Apr.19.05.13.48.1993.29266@athos.rutgers.edu> kempmp@phoenix.oulu.fi (Petri Pihko) writes:\n",
      "= Jason Smith (jasons@atlastele.com) wrote:\n",
      "=  \n",
      "= : [ The discussion begins: why does the universe exist at all?  ]\n",
      "=  \n",
      "= : One of the Laws of Nature, specifying cause and effect seems to dictate \n",
      "= : (at least to this layman's mind) there must be a causal event.  No\n",
      "= : reasonable alternative exists.\n",
      "= \n",
      "= I would argue that causality is actually a property of spacetime; \n",
      "= causes precede their effects. \n",
      "\n",
      "And I must concede here.  Cause *before* effect, implies time, time is part\n",
      "of spacetime.  Hense, the argument would be valid.  I could return and say \n",
      "that this does not infer the cause and effect relationship being *unique*\n",
      "to *this* spacetime, but I won't 8^), because the point is moot.  Doesn't\n",
      "address why (which Petri Pikho addresses below).  \n",
      "\n",
      "I also concede that I was doubly remiss, as I asserted \"No reasonable\n",
      "alternative exists\", an entirely subjective statement on my part (and one\n",
      "that could  be invalidated, given time and further discovery by the\n",
      "scientist).  I also understand that a proving a theory does not necessarily\n",
      "specify that \"this is how it happened\", but proposes a likely description of\n",
      "the phenomena in question.  Am I mistaken with this understanding?\n",
      "\n",
      "= But if you claim that there must be\n",
      "= an answer to \"how\" did the universe (our spacetime)  emerge from \n",
      "= \"nothing\", science has some good candidates for an answer.\n",
      "\n",
      "All of which require something we Christians readily admit to: ``Faith''.\n",
      "\n",
      "The fact that there are several candidates belies that *none* are conclusive.  \n",
      "With out conclusive evidence, we are left with faith.\n",
      "\n",
      "It could even be argued that one of these hypotheses may one day be proven (as\n",
      "best as a non-repeatable event can be \"proven\").  But I ask, what holds  \n",
      "someone *today* to the belief that any or all of them are correct, except by \n",
      "faith?\n",
      "\n",
      "[ a couple of paragraphs deleted.  Summary: we ask \"Why does the\n",
      "universe exist\" ]\n",
      "\n",
      "= I think this question should actually be split into two parts, namely\n",
      "= \n",
      "= 1) Why is there existence? Why anything exists?\n",
      "= \n",
      "= and\n",
      "= \n",
      "= 2) How did the universe emerge from nothing?\n",
      "= \n",
      "= It is clear science has nothing to say about the first question. However,\n",
      "= is it a meaningful question, after all?\n",
      "=\n",
      "= I would say it isn't. Consider the following:\n",
      "\n",
      "Apparently it *is* for many persons.  Hence, we *have* religions.\n",
      "\n",
      "= The question \"why anything exists\" can be countered by\n",
      "= demanding answer to a question \"why there is nothing in nothingness,\n",
      "= or in non-existence\".  Actually, both questions turn out to be\n",
      "= devoid of meaning. Things that exist do, and things that don't exist\n",
      "= don't exist. Tautology at its best.\n",
      "\n",
      "Carefully examine the original question, and then the \"counter-question\". \n",
      "The first asks \"Why\", while the second is a request for definition.  It \n",
      "doesn't address why something does or does not exist, but asks to define \n",
      "the lack of existence.  The second question is unanswerable indeed, for\n",
      "how do we identify something as \"nothing\" (aren't they mutually exclusive\n",
      "terms)?.  How do we identify a state of non-existence (again, this is\n",
      "nearing the limits of this simple layman's ability to comprehend, and I\n",
      "would appreciate an explanation). \n",
      "\n",
      "I might add, the worldview of \"Things that exist do, and things that\n",
      "don't...don't\" is as grounded in the realm of the non-falsifiable,\n",
      "as does the theist's belief in God.  It is based on the assumption\n",
      "that there is *not* a reason for being, something as ultimately\n",
      "(un)supportable as the position of there being a reason.  Its very\n",
      "foundation exists in the same soil as that of one who claims there *is* a\n",
      "reason.\n",
      "\n",
      "We come to this. Either \"I am, therefore I am.\", or \"I am for a reason.\"\n",
      "If the former is a satisfactory answer, then you are done, for you are\n",
      "satisfied, and need not a doctor.  If the latter, your search is just\n",
      "beginning.  \n",
      "\n",
      "= I seriously doubt God could have an answer to this question.\n",
      "\n",
      "Time will tell. 8^)\n",
      "\n",
      "= \n",
      "= Some Christians I have talked to have said that actually, God is\n",
      "= Himself the existence. However, I see several problems with this\n",
      "= answer. First, it inevitably leads to the conclusion that God is\n",
      "= actually _all_ existence, good and evil, devils and angels, us and\n",
      "= them. This is pantheism, not Christianity.\n",
      "\n",
      "Agreed.  It would lead me to question their definition of Christianity as\n",
      "well.\n",
      "\n",
      "= Another answer is that God is the _source_ of all existence.\n",
      "= This sounds much better, but I am tempted to ask: Does God\n",
      "= Himself exist, then? If God is the source of His own existence,\n",
      "= it can only mean that He has, in terms of human time, always\n",
      "= existed. But this is not the same as the source of all existence.\n",
      "\n",
      "This does not preclude His existence.  It only seeks to identify His\n",
      "*qualities* (implying He exists to *have* qualities, BTW).\n",
      "\n",
      "= The best answer I have heard is that human reasoning is incapable\n",
      "= of understanding such questions. Being an atheist myself, I do not\n",
      "= accept such answers, since I do not have any other methods.\n",
      "\n",
      "Like the theist, we come to a statement of faith, for this position assumes \n",
      "that the evidence at hand is conclusive.  Note, I am not arguing against \n",
      "scientific endeavor, for science is useful for understanding the universe in\n",
      "which we exist. But I differ from the atheist in a matter of perspective.  I\n",
      "seek to understand what exists to understand and appreciate the art of the\n",
      "Creator.\n",
      "\n",
      "I also have discovered science is an inadequate tool to answer \"why\".   It\n",
      "appears that M. Pihko agrees (as we shall see).  But because a tool is\n",
      "inadequate to answer a question does not preclude the question.  Asserting\n",
      "that 'why' is an invalid question does not provide an answer.  \n",
      "\n",
      "= : As far as I can tell, the very laws of nature demand a \"why\".  That isn't\n",
      "= : true of something outside of nature (i.e., *super*natural).\n",
      "= \n",
      "= This is not true. Science is a collection of models telling us \"how\",\n",
      "= not why, something happens. I cannot see any good reason why the \"why\"\n",
      "= questions would be bound only to natural things, assuming that the\n",
      "= supernatural domain exists. If supernatural beings exist, it is\n",
      "= as appropriate to ask why they do so as it is to ask why we exist.\n",
      "\n",
      "My apologies.  I was using why as \"why did this come to be\".  Why did\n",
      "pre-existence become existence.  Why did pre-spacetime become spacetime.\n",
      "\n",
      "But we come to the admission that science fails to answer \"Why?\".  Because\n",
      "it can't be answered in the realm of modern science, does that make the\n",
      "question invalid?\n",
      "= : I don't believe *any*\n",
      "= : technology would be able to produce that necessary *spark* of life, despite\n",
      "= : having all of the parts available. Just my opinion.\n",
      "= \n",
      "= This opinion is also called vitalism; namely, that living systems are\n",
      "= somehow _fundamentally_ different from inanimate systems. Do Christians\n",
      "= in general adopt this position? What would happen when scientists announce\n",
      "= they have created primitive life (say, small bacteria) in a lab?\n",
      "\n",
      "I suppose we would do the same thing as when Galileo or Capernicus was \n",
      "*vindicated*  (before someone starts jumping up and down screaming\n",
      "\"Inquisition!\", note I said *vindicated*.  I certainly hope we've gotten\n",
      "beyond the \"shooting the messenger\" stage).\n",
      "\n",
      "M. Pihko does present a good point though.  We may need to ask \"What do I \n",
      "as an individual Christian base my faith on?\"  Will it be shaken by the\n",
      "production of evidence that shatters our \"sacred cows\" or will we seek to\n",
      "understand if a new discovery truly disagrees with what God *said* (and\n",
      "continues to say) in his Word?\n",
      "\n",
      "\"Why do I ask why?\" (apologies to Budweiser and company 8^]).\n",
      "\n",
      "Jason.\n",
      "\n",
      "\n",
      "\n",
      "-- \n",
      "Jason D. Smith  \t|\n",
      "jasons@atlastele.com\t|    I'm not young enough to know everything.\n",
      "     1x1        \t| \n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  21  is: \n",
      "_______________________\n",
      "topic  0 :  0.815547281697\n",
      "topic  1 :  4.01359149604e-05\n",
      "topic  2 :  0.0554824578514\n",
      "topic  3 :  0.128930124537\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "Subject: Space FAQ 05/15 - References\n",
      "From: leech@cs.unc.edu (Jon Leech)\n",
      "Expires: 6 May 1993 19:56:44 GMT\n",
      "Organization: University of North Carolina, Chapel Hill\n",
      "Keywords: Frequently Asked Questions\n",
      "Supersedes: <references_730956466@cs.unc.edu>\n",
      "NNTP-Posting-Host: mahler.cs.unc.edu\n",
      "Lines: 665\n",
      "\n",
      "Archive-name: space/references\n",
      "Last-modified: $Date: 93/04/01 14:39:21 $\n",
      "\n",
      "REFERENCES ON SPECIFIC AREAS\n",
      "\n",
      "    PUBLISHERS OF SPACE/ASTRONOMY MATERIAL\n",
      "\n",
      "    Astronomical Society of the Pacific\n",
      "    1290 24th Avenue\n",
      "    San Francisco, CA 94122\n",
      "\n",
      "\tMore expensive but better organized slide sets.\n",
      "\n",
      "    Cambridge University Press\n",
      "    32 East 57th Street\n",
      "    New York, NY 10022\n",
      "\n",
      "    Crawford-Peters Aeronautica\n",
      "    P.O. Box 152528\n",
      "    San Diego, CA 92115\n",
      "    (619) 287-3933\n",
      "\n",
      "\tAn excellent source of all kinds of space publications. They publish\n",
      "\ta number of catalogs, including:\n",
      "\t    Aviation and Space, 1945-1962\n",
      "\t    Aviation and Space, 1962-1990\n",
      "\t    Space and Related Titles\n",
      "\n",
      "    European Southern Observatory\n",
      "    Information and Photographic Service\n",
      "    Dr R.M. West\n",
      "    Karl Scharzschild Strasse 2\n",
      "    D-8046 Garching bei Munchen\n",
      "    FRG\n",
      "\n",
      "\tSlide sets, posters, photographs, conference proceedings.\n",
      "\n",
      "    Finley Holiday Film Corporation\n",
      "    12607 East Philadelphia Street\n",
      "    Whittier, California 90601\n",
      "    (213)945-3325\n",
      "    (800)FILMS-07\n",
      "\n",
      "\tWide selection of Apollo, Shuttle, Viking, and Voyager slides at ~50\n",
      "\tcents/slide. Call for a catalog.\n",
      "\n",
      "    Hansen Planetarium (Utah)\n",
      "\n",
      "\tSaid to hold sales on old slide sets. Look in Sky & Telescope\n",
      "\tfor contact info.\n",
      "\n",
      "    Lunar and Planetary Institute\n",
      "    3303 NASA Road One\n",
      "    Houston, TX 77058-4399\n",
      "\n",
      "\tTechnical, geology-oriented slide sets, with supporting\n",
      "\tbooklets.\n",
      "\n",
      "    John Wiley & Sons\n",
      "    605 Third Avenue\n",
      "    New York, NY 10158-0012\n",
      "\n",
      "    Sky Publishing Corporation\n",
      "    PO Box 9111\n",
      "    Belmont, MA  02178-9111\n",
      "\n",
      "\tOffers \"Sky Catalogue 2000.0\" on PC floppy with information\n",
      "\t(including parallax) for 45000 stars.\n",
      "\n",
      "    Roger Wheate\n",
      "    Geography Dept.\n",
      "    University of Calgary, Alberta\n",
      "    Canada T2N 1N4\n",
      "    (403)-220-4892\n",
      "    (403)-282-7298 (FAX)\n",
      "    wheate@uncamult.bitnet\n",
      "\n",
      "\tOffers a 40-slide set called \"Mapping the Planets\" illustrating\n",
      "\trecent work in planetary cartography, comes with a booklet and\n",
      "\tinformation on getting your own copies of the maps. $50 Canadian,\n",
      "\tshipping included.\n",
      "\n",
      "    Superintendent of Documents\n",
      "    US Government Printing Office\n",
      "    Washington, DC 20402\n",
      "\n",
      "    Univelt, Inc.\n",
      "    P. O. Box 28130\n",
      "    San Diego, Ca. 92128\n",
      "\n",
      "\tPublishers for the American Astronomical Society.\n",
      "\n",
      "    US Naval Observatory\n",
      "\t202-653-1079 (USNO Bulletin Board via modem)\n",
      "\t202-653-1507 General\n",
      "\n",
      "    Willmann-Bell\n",
      "    P.O. Box 35025\n",
      "    Richmond, Virginia 23235 USA\n",
      "    (804)-320-7016 9-5 EST M-F\n",
      "\n",
      "\n",
      "    CAREERS IN THE SPACE INDUSTRY\n",
      "\n",
      "    In 1990 the Princeton Planetary Society published the first edition of\n",
      "    \"Space Jobs: The Guide to Careers in Space-Related Fields.\" The\n",
      "    publication was enormously successful: we distributed 2000 copies to\n",
      "    space enthusiasts across the country and even sent a few to people in\n",
      "    Great Britain, Australia, and Ecuador. Due to the tremendous response to\n",
      "    the first edition, PPS has published an expanded, up-to-date second\n",
      "    edition of the guide.\n",
      "\n",
      "    The 40-page publication boasts 69 listings for summer and full-time job\n",
      "    opportunities as well as graduate school programs. The second edition of\n",
      "    \"Space Jobs\" features strategies for entering the space field and\n",
      "    describes positions at consulting and engineering firms, NASA, and\n",
      "    non-profit organizations. The expanded special section on graduate\n",
      "    schools highlights a myriad of programs ranging from space manufacturing\n",
      "    to space policy. Additional sections include tips on becoming an\n",
      "    astronaut and listings of NASA Space Grant Fellowships and Consortia, as\n",
      "    well as NASA Centers for the Commercial Development of Space.\n",
      "\n",
      "    To order send check or money order made payable to Princeton Planetary\n",
      "    Society for $4 per copy, plus $1 per copy for shipping and handling\n",
      "    (non-US customers send an International Money Order payable in US\n",
      "    dollars) to:\n",
      "\n",
      "    Princeton Planetary Society\n",
      "    315 West College\n",
      "    Princeton University\n",
      "    Princeton, NJ  08544\n",
      "\n",
      "\n",
      "    DC-X SINGLE-STAGE TO ORBIT (SSTO) PROGRAM\n",
      "\n",
      "    SDI's SSRT (Single Stage Rocket Technology) project has funded a\n",
      "    suborbital technology demonstrator called DC-X that should fly in\n",
      "    mid-1993. Further development towards an operational single-stage to\n",
      "    orbit vehicle (called Delta Clipper) is uncertain at present.\n",
      "\n",
      "    An collection of pictures and files relating to DC-X is available by\n",
      "    anonymous FTP or email server in the directory\n",
      "\n",
      "\tbongo.cc.utexas.edu:pub/delta-clipper\n",
      "\n",
      "    Chris W. Johnson (chrisj@emx.cc.utexas.edu) maintains the archive.\n",
      "\n",
      "\n",
      "    HOW TO NAME A STAR AFTER A PERSON\n",
      "\n",
      "    Official names are decided by committees of the International\n",
      "    Astronomical Union, and are not for sale. There are purely commercial\n",
      "    organizations which will, for a fee, send you pretty certificates and\n",
      "    star maps describing where to find \"your\" star. These organizations have\n",
      "    absolutely no standing in the astronomical community and the names they\n",
      "    assign are not used by anyone else. It's also likely that you won't be\n",
      "    able to see \"your\" star without binoculars or a telescope. See the back\n",
      "    pages of Astronomy or other amateur astronomy publications for contact\n",
      "    info; one such organization may be found at:\n",
      "\n",
      "\tInternational Star Registry\n",
      "\t34523 Wilson Road\n",
      "\tIngleside, IL 60041\n",
      "\n",
      "    This is not an endorsement of ISR.\n",
      "\n",
      "\n",
      "    LLNL \"GREAT EXPLORATION\"\n",
      "\n",
      "    The LLNL \"Great Exploration\", a plan for an on-the-cheap space station,\n",
      "    Lunar base, and Mars mission using inflatable space structures, excited\n",
      "    a lot of interest on the net and still comes up from time to time. Some\n",
      "    references cited during net discussion were:\n",
      "\n",
      "\tAvation Week Jan 22, 1990 for an article on the overall Great\n",
      "\tExploration\n",
      "\n",
      "\tNASA Assessment of the LLNL Space Exploration Proposal and LLNL\n",
      "\tResponses by Dr. Lowell Wood LLNL Doc. No. SS 90-9. Their address\n",
      "\tis: PO Box 808 Livermore, CA 94550 (the NASA authors are unknown).\n",
      "\n",
      "\tBriefing slides of a presentation to the NRC last December may be\n",
      "\tavailable. Write LLNL and ask.\n",
      "\n",
      "\tConceptual Design Study for Modular Inflatable Space Structures, a\n",
      "\tfinal report for purchase order B098747 by ILC Dover INC. I don't\n",
      "\tknow how to get this except from LLNL or ILC Dover. I don't have an\n",
      "\taddress for ILC.\n",
      "\n",
      "\n",
      "    LUNAR PROSPECTOR\n",
      "\n",
      "    Lunar Exploration Inc. (LEI) is a non-profit corporation working on a\n",
      "    privately funded lunar polar orbiter. Lunar Prospector is designed to\n",
      "    perform a geochemical survey and search for frozen volatiles at the\n",
      "    poles. A set of reference files describing the project is available in\n",
      "\n",
      "\tames.arc.nasa.gov:pub/SPACE/LEI/*\n",
      "\n",
      "\n",
      "    LUNAR SCIENCE AND ACTIVITIES\n",
      "\n",
      "    Grant H Heiken, David T Vaniman, and Bevan M French (editors), \"Lunar\n",
      "    Sourcebook, A User's Guide to the Moon\", Cambridge University Press\n",
      "    1991, ISBN 0-521-33444-6; hardcover; expensive. A one-volume\n",
      "    encyclopedia of essentially everything known about the Moon, reviewing\n",
      "    current knowledge in considerable depth, with copious references. Heavy\n",
      "    emphasis on geology, but a lot more besides, including considerable\n",
      "    discussion of past lunar missions and practical issues relevant to\n",
      "    future mission design. *The* reference book for the Moon; all others are\n",
      "    obsolete.\n",
      "\n",
      "    Wendell Mendell (ed), \"Lunar Bases and Space Activities of the 21st\n",
      "    Century\", $15. \"Every serious student of lunar bases *must* have this\n",
      "    book\" - Bill Higgins. Available from:\n",
      "\n",
      "\tLunar and Planetary Institute\n",
      "\t3303 NASA Road One\n",
      "\tHouston, TX 77058-4399\n",
      "\tIf you want to order books, call (713)486-2172.\n",
      "\n",
      "    Thomas A. Mutch, \"Geology of the Moon: A Stratigraphic View\", Princeton\n",
      "    University Press, 1970. Information about the Lunar Orbiter missions,\n",
      "    including maps of the coverage of the lunar nearside and farside by\n",
      "    various Orbiters.\n",
      "\n",
      "\n",
      "    ORBITING EARTH SATELLITE HISTORIES\n",
      "\n",
      "    A list of Earth orbiting satellites (that are still in orbit) is\n",
      "    available by anonymous FTP in:\n",
      "\n",
      "\tames.arc.nasa.gov:pub/SPACE/FAQ/Satellites\n",
      "\n",
      "\n",
      "    SPACECRAFT MODELS\n",
      "\n",
      "    \"Space in Miniature #2: Gemini\" by\n",
      "\tMichael J. Mackowski\n",
      "\t1621 Waterwood Lane, St. Louis, MO 63146\n",
      "\t$7.50\n",
      "\n",
      "    Only 34pp but enough pictures & diagrams to interest more than just the\n",
      "    modelling community, I feel.\n",
      "\n",
      "    Marco's Miniatures of Dracut, Mass. have produced a 1/144 Skylab in an\n",
      "    edition of 500 & a 1/48 Lunar Rover (same scale as Monogram and Revell\n",
      "    Lunar Modules) in a similar edition. Prices are $45 for Skylab, $24 for\n",
      "    LRV. Check with them for postage etc. I have no connection with them,\n",
      "    but have found their service to be good and their stock of rare/old kits\n",
      "    *is* impressive. Prices range from reasonable ($35 for Monogram 1/32\n",
      "    scale Apollo CSM with cutaway details) to spectacular ($145 for Airfix\n",
      "    Vostok).\n",
      "\n",
      "\t Four Star Collectibles\n",
      "\t P.O. Box 658\n",
      "\t Dracut Mass 01826, USA.\n",
      "\t (508)-957-0695.\n",
      "\n",
      "    Voyager, HST, Viking, Lunar Rover etc. kits from:\n",
      "\n",
      "\tLunar Models\n",
      "\t5120 Grisham\n",
      "\tRowlett, Texas 75088\n",
      "\t(214)-475-4230\n",
      "\n",
      "    As reviewed by Bob Kaplow:\n",
      "\n",
      "\tPeter Alway's book \"Scale Model Rocketry\" is now available. Mine\n",
      "\tarrived in the mail earlier this week. To get your own copy, send\n",
      "\t$19.95 + $2.50 s/h ($22.45 total) to:\n",
      "\n",
      "\t\t\tPeter Alway\n",
      "\t\t\t2830 Pittsfield\n",
      "\t\t\tAnn Arbor, MI 48104\n",
      "\n",
      "\tThe book includes information on collecting scale data, construction\n",
      "\tof scale models, and several handy tables. Appendicies include plans\n",
      "\tfor 3 sport scale models, a 1:9.22 D Region Tomahawk (BT50), a 1/40\n",
      "\tV-2 (BT60), and a 1/9.16 Aerobee 150A (BT55/60).\n",
      "\n",
      "\tI've only begun to study the book, but it certainly will be a\n",
      "\tvaluable data source for many modellers. Most vehicles include\n",
      "\tseveral paragraphs of text describing the missions flown by the\n",
      "\trocket, various specs including \"NAR\" engine classification, along\n",
      "\twith a dimensioned drawing, color layouts & paint pattern, and a\n",
      "\tblack & white photograph.\n",
      "\n",
      "\tThe vehicles included are the Aerobee 150A, Aerobee 300, Aerobee Hi,\n",
      "\tArcas, Asp, Astrobee 1500, Astrobee D, Atlas Centaur, Atlas-Agena,\n",
      "\tAtlas-Score, Baby WAC, D-Region Tomahawk, Deacon Rockoon, Delta B,\n",
      "\tDelta E, Gemini-Titan II, Iris, Javelin, Juno 1, Juno 2, Little Joe\n",
      "\t1, Little Joe 2, Mercury-Atlas, Mercury-Redstone, Nike-Apache,\n",
      "\tNike-Asp, Nike-Cajun, Nike-Deacon, Nike-Tomahawk, RAM B, Saturn 1\n",
      "\tBlock 1, Saturn 1 Block 2, Saturn 1B, Saturn 5, Scout, Standard\n",
      "\tAerobee, Terrapin, Thor-Able, Titan III C, Titan III E, Trailblazer\n",
      "\t1, V-2, Vanguard, Viking Model 1, Viking Model 2, and Wac Corporal.\n",
      "\n",
      "\n",
      "    ROCKET PROPULSION\n",
      "\n",
      "\tGeorge P. Sutton, \"Rocket Propulsion Elements\", 5th edn,\n",
      "\tWiley-Interscience 1986, ISBN 0-471-80027-9. Pricey textbook. The\n",
      "\tbest (nearly the only) modern introduction to the technical side of\n",
      "\trocketry. A good place to start if you want to know the details. Not\n",
      "\tfor the math-shy. Straight chemical rockets, essentially nothing on\n",
      "\tmore advanced propulsion (although earlier editions reportedly had\n",
      "\tsome coverage).\n",
      "\n",
      "\tDieter K. Huzel and David H. Huang, \"Design of Liquid Propellant\n",
      "\tRocket Engines\", NASA SP-125.\n",
      "\tNTIS N71-29405\t\tPC A20/MF A01\t1971  461p\n",
      "\tOut of print; reproductions may be obtained through the NTIS\n",
      "\t(expensive). The complete and authoritative guide to designing\n",
      "\tliquid-fuel engines. Reference #1 in most chapters of Sutton. Heavy\n",
      "\temphasis on practical issues, what works and what doesn't, what the\n",
      "\ttypical values of the fudge factors are. Stiff reading, massive\n",
      "\tdetail; written for rocket engineers by rocket engineers.\n",
      "\n",
      "\n",
      "    SPACECRAFT DESIGN\n",
      "\n",
      "\tBrij N. Agrawal, \"Design of Geosynchronous Spacecraft\",\n",
      "\tPrentice-Hall, ISBN 0-13-200114-4.\n",
      "\n",
      "\tJames R. Wertz ed, \"Spacecraft Attitude Determination and\n",
      "\tControl\", Kluwer, ISBN 90-277-1204-2.\n",
      "\n",
      "\tP.R.K. Chetty, \"Satellite Technology and its Applications\",\n",
      "\tMcGraw-Hill, ISBN 0-8306-9688-1.\n",
      "\n",
      "\tJames R. Wertz and Wiley J. Larson (editors), \"Space Mission\n",
      "\tAnalysis and Design\", Kluwer Academic Publishers\n",
      "\t(Dordrecht/Boston/London) 1991, ISBN 0-7923-0971-5 (paperback), or\n",
      "\t0-7923-0970-7 (hardback).\n",
      "\n",
      "\t    This looks at system-level design of a spacecraft, rather than\n",
      "\t    detailed design. 23 chapters, 4 appendices, about 430 pages. It\n",
      "\t    leads the reader through the mission design and system-level\n",
      "\t    design of a fictitious earth-observation satellite, to\n",
      "\t    illustrate the principles that it tries to convey. Warning:\n",
      "\t    although the book is chock-full of many useful reference tables,\n",
      "\t    some of the numbers in at least one of those tables (launch\n",
      "\t    costs for various launchers) appear to be quite wrong. Can be\n",
      "\t    ordered by telephone, using a credit card; Kluwer's phone number\n",
      "\t    is (617)-871-6600. Cost $34.50.\n",
      "\n",
      "\n",
      "    ESOTERIC PROPULSION SCHEMES (SOLAR SAILS, LASERS, FUSION...)\n",
      "\n",
      "    This needs more and more up-to-date references, but it's a start.\n",
      "\n",
      "    ANTIMATTER:\n",
      "\n",
      "\t\"Antiproton Annihilation Propulsion\", Robert Forward\n",
      "\t    AFRPL TR-85-034 from the Air Force Rocket Propulsion Laboratory\n",
      "\t    (AFRPL/XRX, Stop 24, Edwards Air Force Base, CA 93523-5000).\n",
      "\t    NTIS AD-A160 734/0\t   PC A10/MF A01\n",
      "\t    PC => Paper copy, A10 => $US57.90 -- or maybe Price Code?\n",
      "\t    MF => MicroFiche, A01 => $US13.90\n",
      "\n",
      "\t    Technical study on making, holding, and using antimatter for\n",
      "\t    near-term (30-50 years) propulsion systems. Excellent\n",
      "\t    bibliography. Forward is the best-known proponent\n",
      "\t    of antimatter.\n",
      "\n",
      "\t    This also may be available as UDR-TR-85-55 from the contractor,\n",
      "\t    the University of Dayton Research Institute, and DTIC AD-A160\n",
      "\t    from the Defense Technical Information Center, Defense Logistics\n",
      "\t    Agency, Cameron Station, Alexandria, VA 22304-6145. And it's\n",
      "\t    also available from the NTIS, with yet another number.\n",
      "\n",
      "\t\"Advanced Space Propulsion Study, Antiproton and Beamed Power\n",
      "\t    Propulsion\", Robert Forward\n",
      "\n",
      "\t    AFAL TR-87-070 from the Air Force Astronautics Laboratory, DTIC\n",
      "\t    #AD-A189 218.\n",
      "\t    NTIS AD-A189 218/1\t  PC A10/MF A01\n",
      "\n",
      "\t    Summarizes the previous paper, goes into detail on beamed power\n",
      "\t    systems including \" 1) pellet, microwave, and laser beamed power\n",
      "\t    systems for intersteller transport; 2) a design for a\n",
      "\t    near-relativistic laser-pushed lightsail using near-term laser\n",
      "\t    technology; 3) a survey of laser thermal propulsion, tether\n",
      "\t    transportation systems, antiproton annihilation propulsion,\n",
      "\t    exotic applications of solar sails, and laser-pushed\n",
      "\t    interstellar lightsails; 4) the status of antiproton\n",
      "\t    annihilation propulsion as of 1986; and 5) the prospects for\n",
      "\t    obtaining antimatter ions heavier than antiprotons.\" Again,\n",
      "\t    there is an extensive bibliography.\n",
      "\n",
      "\t    \"Application of Antimatter - Electric Power to Interstellar\n",
      "\t    Propulsion\", G. D. Nordley, JBIS Interstellar Studies issue of\n",
      "\t    6/90.\n",
      "\n",
      "    BUSSARD RAMJETS AND RELATED METHODS:\n",
      "\n",
      "\tG. L. Matloff and A. J. Fennelly, \"Interstellar Applications and\n",
      "\tLimitations of Several Electrostatic/Electromagnetic Ion Collection\n",
      "\tTechniques\", JBIS 30 (1977):213-222\n",
      "\n",
      "\tN. H. Langston, \"The Erosion of Interstellar Drag Screens\", JBIS 26\n",
      "\t(1973): 481-484\n",
      "\n",
      "\tC. Powell, \"Flight Dynamics of the Ram-Augmented Interstellar\n",
      "\tRocket\", JBIS 28 (1975):553-562\n",
      "\n",
      "\tA. R. Martin, \"The Effects of Drag on Relativistic Spacefight\", JBIS\n",
      "\t25 (1972):643-652\n",
      "\n",
      "    FUSION:\n",
      "\n",
      "\t\"A Laser Fusion Rocket for Interplanetary Propulsion\", Roderick Hyde,\n",
      "\tLLNL report UCRL-88857. (Contact the Technical Information Dept. at\n",
      "\tLivermore)\n",
      "\n",
      "\t    Fusion Pellet design: Fuel selection. Energy loss mechanisms.\n",
      "\t    Pellet compression metrics. Thrust Chamber: Magnetic nozzle.\n",
      "\t    Shielding. Tritium breeding. Thermal modeling. Fusion Driver\n",
      "\t    (lasers, particle beams, etc): Heat rejection. Vehicle Summary:\n",
      "\t    Mass estimates. Vehicle Performance: Interstellar travel\n",
      "\t    required exhaust velocities at the limit of fusion's capability.\n",
      "\t    Interplanetary missions are limited by power/weight ratio.\n",
      "\t    Trajectory modeling. Typical mission profiles. References,\n",
      "\t    including the 1978 report in JBIS, \"Project Daedalus\", and\n",
      "\t    several on ICF and driver technology.\n",
      "\n",
      "\t\"Fusion as Electric Propulsion\", Robert W. Bussard, Journal of\n",
      "\tPropulsion and Power, Vol. 6, No. 5, Sept.-Oct. 1990\n",
      "\n",
      "\t    Fusion rocket engines are analyzed as electric propulsion\n",
      "\t    systems, with propulsion thrust-power-input-power ratio (the\n",
      "\t    thrust-power \"gain\" G(t)) much greater than unity. Gain values\n",
      "\t    of conventional (solar, fission) electric propulsion systems are\n",
      "\t    always quite small (e.g., G(t)<0.8). With these, \"high-thrust\"\n",
      "\t    interplanetary flight is not possible, because system\n",
      "\t    acceleration (a(t)) capabilities are always less than the local\n",
      "\t    gravitational acceleration. In contrast, gain values 50-100\n",
      "\t    times higher are found for some fusion concepts, which offer\n",
      "\t    \"high-thrust\" flight capability. One performance example shows a\n",
      "\t    53.3 day (34.4 powered; 18.9 coast), one-way transit time with\n",
      "\t    19% payload for a single-stage Earth/Mars vehicle. Another shows\n",
      "\t    the potential for high acceleration (a(t)=0.55g(o)) flight in\n",
      "\t    Earth/moon space.\n",
      "\n",
      "\t\"The QED Engine System: Direct Electric Fusion-Powered Systems for\n",
      "\tAerospace Flight Propulsion\" by Robert W. Bussard, EMC2-1190-03,\n",
      "\tavailable from Energy/Matter Conversion Corp., 9100 A. Center\n",
      "\tStreet, Manassas, VA 22110.\n",
      "\n",
      "\t    [This is an introduction to the application of Bussard's version\n",
      "\t    of the Farnsworth/Hirsch electrostatic confinement fusion\n",
      "\t    technology to propulsion. 1500<Isp<5000 sec. Farnsworth/Hirsch\n",
      "\t    demonstrated a 10**10 neutron flux with their device back in\n",
      "\t    1969 but it was dropped when panic ensued over the surprising\n",
      "\t    stability of the Soviet Tokamak. Hirsch, responsible for the\n",
      "\t    panic, has recently recanted and is back working on QED. -- Jim\n",
      "\t    Bowery]\n",
      "\n",
      "\t\"PLASMAKtm Star Power for Energy Intensive Space Applications\", by\n",
      "\tPaul M. Koloc, Eight ANS Topical Meeting on Technology of Fusion\n",
      "\tEnergy, special issue FUSION TECHNOLOGY, March 1989.\n",
      "\n",
      "\t    Aneutronic energy (fusion with little or negligible neutron\n",
      "\t    flux) requires plasma pressures and stable confinement times\n",
      "\t    larger than can be delivered by current approaches. If plasma\n",
      "\t    pressures appropriate to burn times on the order of milliseconds\n",
      "\t    could be achieved in aneutronic fuels, then high power densities\n",
      "\t    and very compact, realtively clean burning engines for space and\n",
      "\t    other special applications would be at hand. The PLASMAKtm\n",
      "\t    innovation will make this possible; its unique pressure\n",
      "\t    efficient structure, exceptional stability, fluid-mechanically\n",
      "\t    compressible Mantle and direct inductive MHD electric power\n",
      "\t    conversion advantages are described. Peak burn densities of tens\n",
      "\t    of megawats per cc give it compactness even in the\n",
      "\t    multi-gigawatt electric output size. Engineering advantages\n",
      "\t    indicate a rapid development schedule at very modest cost. [I\n",
      "\t    strongly recommend that people take this guy seriously. Bob\n",
      "\t    Hirsch, the primary proponent of the Tokamak, has recently\n",
      "\t    declared Koloc's PLASMAKtm precursor, the spheromak, to be one\n",
      "\t    of 3 promising fusion technologies that should be pursued rather\n",
      "\t    than Tokamak. Aside from the preceeding appeal to authority, the\n",
      "\t    PLASMAKtm looks like it finally models ball-lightning with solid\n",
      "\t    MHD physics. -- Jim Bowery]\n",
      "\n",
      "    ION DRIVES:\n",
      "\n",
      "\tRetrieve files pub/SPACE/SPACELINK/6.5.2.* from the Ames SPACE\n",
      "\tarchive; these deal with many aspects of ion drives and describe the\n",
      "\tSERT I and II missions, which flight-tested cesium ion thrusters in\n",
      "\tthe 1960s and 70s. There are numerous references.\n",
      "\n",
      "    MASS DRIVERS (COILGUNS, RAILGUNS):\n",
      "\n",
      "\tIEEE Transactions on Magnetics (for example, v. 27 no. 1, January\n",
      "\t1991 issue). Every so often they publish the proceedings of the\n",
      "\tSymposium on Electromagnetic Launcher Technology, including hundreds\n",
      "\tof papers on the subject. It's a good look at the state of the art,\n",
      "\tthough perhaps not a good tutorial for beginners. Anybody know some\n",
      "\tgood review papers?\n",
      "\n",
      "    NUCLEAR ROCKETS (FISSION):\n",
      "\n",
      "\t\"Technical Notes on Nuclear Rockets\", by Bruce W. Knight and Donald\n",
      "\tKingsbury, unpublished. May be available from: Donald Kingsbury,\n",
      "\tMath Dept., McGill University, PO Box 6070, Station A, Montreal,\n",
      "\tQuebec M3C 3G1 Canada.\n",
      "\n",
      "    SOLAR SAILS:\n",
      "\n",
      "\tStarsailing. Solar Sails and Interstellar Travel. Louis Friedman,\n",
      "\tWiley, New York, 1988, 146 pp., paper $9.95. (Not very technical,\n",
      "\tbut an adequate overview.)\n",
      "\n",
      "\t\"Roundtrip Interstellar Travel Using Laser-Pushed Lightsails\n",
      "\t(Journal of Spacecraft and Rockets, vol. 21, pp. 187-95, Jan.-Feb.\n",
      "\t1984)\n",
      "\n",
      "    TETHERS:\n",
      "\n",
      "\t_Tethers and Asteroids for Artificial Gravity Assist in the Solar\n",
      "\tSystem,_ by P.A. Penzo and H.L. Mayer., _Journal of Spacecraft\n",
      "\tand Rockets_ for Jan-Feb 1986.\n",
      "\n",
      "\t    Details how a spacecraft with a kevlar tether of the same mass\n",
      "\t    can change its velocity by up to slightly less than 1 km/sec. if\n",
      "\t    it is travelling under that velocity wrt a suitable asteroid.\n",
      "\n",
      "    GENERAL:\n",
      "\n",
      "\t\"Alternate Propulsion Energy Sources\", Robert Forward\n",
      "\t    AFPRL TR-83-067.\n",
      "\t    NTIS AD-B088 771/1\t  PC A07/MF A01   Dec 83 138p\n",
      "\n",
      "\t    Keywords: Propulsion energy, metastable helium, free-radical\n",
      "\t    hydrogen, solar pumped (sic) plasmas, antiproton annihiliation,\n",
      "\t    ionospheric lasers, solar sails, perforated sails, microwave\n",
      "\t    sails, quantum fluctuations, antimatter rockets... It's a wide,\n",
      "\t    if not deep, look at exotic energy sources which might be useful\n",
      "\t    for space propulsion. It also considers various kinds of laser\n",
      "\t    propulsion, metallic hydrogen, tethers, and unconventional\n",
      "\t    nuclear propulsion. The bibliographic information, pointing to\n",
      "\t    the research on all this stuff, belongs on every daydreamer's\n",
      "\t    shelf.\n",
      "\n",
      "\tFuture Magic. Dr. Robert L. Forward, Avon, 1988. ISBN 0-380-89814-4.\n",
      "\n",
      "\t    Nontechnical discussion of tethers, antimatter, gravity control,\n",
      "\t    and even futher-out topics.\n",
      "\n",
      "\n",
      "    SPY SATELLITES\n",
      "\n",
      "    *Deep Black*, by William Burrows;\n",
      "\t\"best modern general book for spysats.\"\n",
      "\n",
      "    1) A Base For Debate: The US Satellite Station at Nurrungar, Des Ball,\n",
      "    Allen and Unwin Australia, 1987 ISBN 0 04 355027 4 [ covers DSP early\n",
      "    warning satellites]\n",
      "\n",
      "    2) Pine Gap: Australia and the US Geostationary Signals intelligence\n",
      "    satellite program, Des Ball, Allen and Unwin Australia, 1988 ISBN 0 04\n",
      "    363002 5. [covers RHYOLITE/AQUACADE, CHALET/VORTEX, and MAGNUM signals\n",
      "    intelligence satellites]\n",
      "\n",
      "    3) Guardians: Strategic Reconnaissance Satellites, Curtis Peebles, 1987,\n",
      "    Ian Allan, ISBN 0 7110 17654 [ good on MOL, military Salyut and Soviet\n",
      "    satellites, less so on others. Tends to believe what he's told so flaws\n",
      "    in discussion of DSP, RHYOLITE et al..]\n",
      "\n",
      "    4) America's Secret Eyes In Space: The Keyhole Spy Satellite Program,\n",
      "    Jeffrey Richelson, 1990, Harper and Row, ISBN 0 88730 285 8 [ in a class\n",
      "    of its own, *the* historical reference on the KEYHOLE satellites]\n",
      "\n",
      "    5) Secret Sentries in Space, Philip J Klass, 1971.\n",
      "\t\"long out of print but well worth a look\"\n",
      "\n",
      "\n",
      "    SPACE SHUTTLE COMPUTER SYSTEMS\n",
      "\n",
      "    %J Communications of the ACM\n",
      "    %V 27\n",
      "    %N 9\n",
      "    %D September 1984\n",
      "    %K Special issue on space [shuttle] computers\n",
      "\n",
      "    %A Myron Kayton\n",
      "    %T Avionics for Manned Spacecraft\n",
      "    %J IEEE Transactions on Aerospace and Electronic Systems\n",
      "    %V 25\n",
      "    %N 6\n",
      "    %D November 1989\n",
      "    %P 786-827\n",
      "\n",
      "    Other various AIAA and IEEE publications.\n",
      "\n",
      "    Computers in Spaceflight: The NASA Experience\n",
      "    James E.  Tomayko\n",
      "    1988?\n",
      "\n",
      "\n",
      "    SETI COMPUTATION (SIGNAL PROCESSING)\n",
      "\n",
      "    %A D. K. Cullers\n",
      "    %A Ivan R. Linscott\n",
      "    %A Bernard M. Oliver\n",
      "    %T Signal Processing in SETI\n",
      "    %J Communications of the ACM\n",
      "    %V 28\n",
      "    %N 11\n",
      "    %D November 1984\n",
      "    %P 1151-1163\n",
      "    %K CR Categories and Subject Descriptors: D.4.1 [Operating Systems]:\n",
      "    Process Management - concurrency; I.5.4 [Pattern Recognition]:\n",
      "    Applications - signal processing; J.2 [Phsyical Sciences and Engineering]:\n",
      "    astronomy\n",
      "    General Terms: Design\n",
      "    Additional Key Words and Phrases: digital Fourier transforms,\n",
      "    finite impulse-response filters, interstellar communications,\n",
      "    Search for Extra-terrestrial Intelligence, signal detection,\n",
      "    spectrum analysis\n",
      "\n",
      "\n",
      "    AMATEUR SATELLIES & WEATHER SATELLITES\n",
      "\n",
      "    A fairly long writeup on receiving and interpreting weather satellite\n",
      "    photos is available from the Ames SPACE archive in\n",
      "    pub/SPACE/FAQ/WeatherPhotos.\n",
      "\n",
      "    The American Radio Relay League publication service offers the following\n",
      "    references (also see the section on AMSAT in the space groups segment of\n",
      "    the FAQ):\n",
      "\n",
      "\tARRL Satellite Experimenters Handbook,\t\t#3185, $20\n",
      "\tARRL Weather Satellite Handbook,\t\t#3193, $20\n",
      "\tIBM-PC software for Weather Satellite Handbook, #3290, $10\n",
      "\n",
      "\tAMSAT NA 5th Space Symposium,\t\t\t#0739, $12\n",
      "\tAMSAT NA 6th Space Symposium,\t\t\t#2219, $12\n",
      "\n",
      "\tShipping is extra.\n",
      "\n",
      "    The American Radio Relay League\n",
      "    Publications Department\n",
      "    225 Main Street\n",
      "    Newington, CT 06111\n",
      "    (203)-666-1541\n",
      "\n",
      "\n",
      "    TIDES\n",
      "\n",
      "    Srinivas Bettadpur contributed a writeup on tides, available from the\n",
      "    Ames SPACE archive in pub/SPACE/FAQ/Tides. It covers the following\n",
      "    areas:\n",
      "\n",
      "\t- 2-D Example of Tidal Deformation\n",
      "\t- Treatment of Tidal Fields in Practice\n",
      "\t- Long term evolution of the Earth-Moon system under tides\n",
      "\n",
      "    The writeup refers to the following texts:\n",
      "\n",
      "\t\"Geophysical Geodesy\" by K. Lambeck\n",
      "\t\"Tides of the planet Earth\" by P. Melchior\n",
      "\n",
      "NEXT: FAQ #6/15 - Constants and equations for calculations\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  22  is: \n",
      "_______________________\n",
      "topic  0 :  0.000545751394634\n",
      "topic  1 :  0.00078845385275\n",
      "topic  2 :  0.118406204747\n",
      "topic  3 :  0.880259590006\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: steve.dunham@uuserv.cc.utah.edu (STEVE LEE DUNHAM)\n",
      "Subject: Re: Car buying story, was: Christ, another dealer service scam...\n",
      "Lines: 18\n",
      "Organization: The University of Utah\n",
      "Distribution: usa\n",
      "\n",
      "\n",
      ">While not exactly a service incident, I had a similar experience recently  \n",
      ">when I bought a new truck.\n",
      ">\n",
      ">I had picked out the vehicle I wanted and after a little haggling we  \n",
      ">agreed on a price. I wrote them a check for the down payment plus tax\n",
      ">and license and told them I'd be back that evening to pick up the truck.  \n",
      ">When I returned, I had to wait about an hour before the finance guy could  \n",
      ">get to me. When I finally got in there, everything went smoothly until he  \n",
      ">started adding up the numbers. He then discovered that they had  \n",
      ">miscalculated the tax & license by about $150. \n",
      "\n",
      "This seems to be a popular scam with dealers.  Last month my brother bought \n",
      "a new Audi 90 series quatro from a local dealer.  They came back with the \n",
      "final price, tax and all, and he added it up for himself.  There happened to \n",
      "be an extra $300 tagged on under the tax part. He pointed out their error \n",
      "and asked them to re-think their addition.  They came back with the right \n",
      "price the next time.\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  23  is: \n",
      "_______________________\n",
      "topic  0 :  0.890740975198\n",
      "topic  1 :  0.00111545356245\n",
      "topic  2 :  0.000758022765815\n",
      "topic  3 :  0.107385548474\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: C.O.EGALON@LARC.NASA.GOV (CLAUDIO OLIVEIRA EGALON)\n",
      "Subject: Portuguese Launch Complex (was:*Doppelganger*)\n",
      "Organization: NASA Langley Research Center\n",
      "Lines: 14\n",
      "Distribution: world\n",
      "Reply-To: C.O.EGALON@LARC.NASA.GOV (CLAUDIO OLIVEIRA EGALON)\n",
      "NNTP-Posting-Host: tahiti.larc.nasa.gov\n",
      "\n",
      "> Portugese launch complex were *wonderful\n",
      "\n",
      "Portuguese launch complex??? Gosh.... Polish are for American in the \n",
      "same way as Portuguese are for Brazilians (I am from Brazil). There is \n",
      "a joke about the Portuguese Space Agency that wanted to send a \n",
      "Portuguese  astronaut to the surface of the Sun (if there is such a thing).\n",
      "How did they solve all problems of sending a man to the surface of the \n",
      "Sun??? Simple... their astronauts travelled during the night...\n",
      "\n",
      " C.O.EGALON@LARC.NASA.GOV\n",
      "\n",
      "C.O.Egalon@larc.nasa.gov\n",
      "\n",
      "Claudio Oliveira Egalon\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n"
     ]
    }
   ],
   "source": [
    "#take a look at some example test documents (14-24 has a nice mix of topics, with a couple difficult ones)\n",
    "dStart = 14\n",
    "dEnd = 24\n",
    "for d in range(dStart,dEnd):\n",
    "    print(\"Estimated mixture for document \", d,\" is: \")\n",
    "    print(\"_______________________\")\n",
    "    for i in range(len(gamma[d])):\n",
    "        print(\"topic \", i,\": \", gamma[d][i]/np.sum(gamma[d]))\n",
    "    print(\"_______________________\")\n",
    "    print(\"Which has the following text:\")\n",
    "    print(\" \")\n",
    "    print(origTestDocs[d])\n",
    "    print(\"__________________________________________\")\n",
    "    print(\"__________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisit the cell that presented the top words for your topics. Do the presented mixtures above make sense if you look at the document content? Recall which original categories (\"Religion\", \"Cars\", \"Hockey\", \"Space\") you (to your best ability) assigned to which numbers. Do the texts seem to be discussing those topics?\n",
    "\n",
    "<span style=\"color:blue\">If you're re-doing this test with the MoodyLyrics dataset from the bonus section, you may be noticing some weird results. LDA can experience some issues in this setting, as for example many words that would be present in a happy song could also be present in a sad song ('love', 'hold', 'forever') but in different order or with certain \"negating\" words between them. It is possible to alleviate this problem by using a vocabulary of n-grams, however this increases the total size of the vocabulary (and therefore the run time as well) substantially. </span>\n",
    "\n",
    "It is also possible to gain some more insight by examining the $\\phi$ values for the documents. Recall that the $\\phi$ values for the document approximate $p(z_n | \\mathbf{w})$, showing how the topics are mixed for the words in the document. The cell below provides a method for printing the phi values for each word in a document. Apart from just examining how the topics are mixed for specific words, take a look at the topic mixtures for the same word that appears in several different documents. As stated in the theory section, in LDA the distribution of topic mixtures that are assigned to each word is sampled differently for each document. This means that hopefully it should be apparent from your results how the topic mixture for the same word can be differ in different test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ucs\t: [  2.59726755e-19   5.02561244e-01   5.62327252e-19   4.97438756e-01]\n",
      "27\t: [  5.86807521e-18   1.80996807e-01   5.83163425e-01   2.35839768e-01]\n",
      "know\t: [  1.90743410e-18   5.60808416e-01   1.00444381e-01   3.38747203e-01]\n",
      "explain\t: [  9.28633566e-19   6.31275223e-01   1.07737769e-02   3.57951000e-01]\n",
      "open\t: [  5.59818834e-18   6.35751703e-01   5.40271583e-02   3.10221139e-01]\n",
      "sign\t: [  2.17991145e-35   7.24872425e-02   5.82853519e-01   3.44659238e-01]\n",
      "strong\t: [  5.16714643e-20   5.77298820e-01   4.13754990e-01   8.94619003e-03]\n",
      "usa\t: [  5.03755456e-19   3.47041142e-02   2.24883836e-01   7.40412050e-01]\n",
      "indiana\t: [  4.01707639e-19   7.77288771e-01   5.10691350e-19   2.22711229e-01]\n",
      "dallas\t: [  7.13459885e-36   4.78322208e-14   7.55501129e-01   2.44498871e-01]\n",
      "nntp\t: [  5.27721318e-18   9.67768185e-02   3.72268236e-01   5.30954946e-01]\n",
      "gov\t: [  1.51998664e-17   7.64540936e-01   3.06874785e-02   2.04771586e-01]\n",
      "radio\t: [  9.83470753e-18   6.97552131e-02   6.35577975e-01   2.94666812e-01]\n",
      "box\t: [  5.96529962e-18   2.01975967e-01   7.86531229e-01   1.14928037e-02]\n",
      "center\t: [  5.83194551e-18   1.49796961e-01   6.81731206e-01   1.68471834e-01]\n",
      "post\t: [  4.26645660e-18   2.76856473e-01   2.64071997e-01   4.59071531e-01]\n",
      "north\t: [  1.83475227e-17   2.12434815e-01   3.42922858e-01   4.44642327e-01]\n",
      "think\t: [  7.57705134e-19   4.50402839e-01   1.99526426e-01   3.50070735e-01]\n",
      "look\t: [  7.30990593e-19   2.49463207e-01   3.10600653e-01   4.39936140e-01]\n",
      "cause\t: [  7.20642745e-36   7.36056149e-01   5.73822811e-02   2.06561570e-01]\n",
      "pass\t: [  4.70000459e-18   1.55347310e-01   3.61993348e-01   4.82659343e-01]\n",
      "rick\t: [  6.56065962e-36   2.68831968e-02   9.28025496e-01   4.50913076e-02]\n",
      "set\t: [  2.31880484e-19   3.38714905e-02   6.30375711e-01   3.35752799e-01]\n",
      "come\t: [  6.95509640e-19   5.10730963e-01   2.00219000e-01   2.89050038e-01]\n",
      "president\t: [  2.46192367e-18   4.08388201e-01   5.36814141e-01   5.47976578e-02]\n",
      "500\t: [  1.33991942e-18   2.60398280e-01   6.47422493e-01   9.21792272e-02]\n",
      "light\t: [  2.57292077e-19   7.74021236e-01   1.68953723e-02   2.09083392e-01]\n",
      "appear\t: [  4.78048578e-36   9.76448811e-01   2.32365623e-19   2.35511890e-02]\n",
      "45\t: [  1.39450238e-17   4.76689001e-02   9.14496465e-01   3.78346349e-02]\n",
      "course\t: [  8.17956797e-19   6.33378605e-01   2.34527226e-02   3.43168673e-01]\n",
      "good\t: [  1.87677283e-18   5.01540462e-01   2.10254302e-01   2.88205237e-01]\n",
      "division\t: [  5.26448792e-18   1.31848152e-01   8.63544571e-01   4.60727798e-03]\n",
      "net\t: [  9.03276940e-19   4.57896989e-01   3.19126052e-01   2.22976959e-01]\n",
      "fnal\t: [  2.34529341e-19   9.82789777e-01   2.11089662e-19   1.72102228e-02]\n",
      "speak\t: [  2.14831286e-19   6.76513306e-01   2.57810168e-19   3.23486694e-01]\n",
      "computer\t: [  1.76235683e-18   3.33603062e-01   9.15254088e-02   5.74871529e-01]\n",
      "________________________________\n",
      "goal\t: [  9.99999998e-01   8.18053271e-15   4.59591102e-18   1.55750816e-09]\n",
      "key\t: [  9.99999999e-01   3.60162461e-15   1.05516793e-20   9.98405221e-10]\n",
      "copy\t: [  1.00000000e+00   1.77522136e-14   1.44862913e-37   1.29206890e-17]\n",
      "93\t: [  9.99999999e-01   1.31516694e-16   5.65906732e-19   6.57235599e-10]\n",
      "operation\t: [  9.99999999e-01   1.33355660e-15   2.24464869e-19   1.02011928e-09]\n",
      "science\t: [  1.00000000e+00   1.06624113e-14   5.13428761e-20   3.65686583e-10]\n",
      "cost\t: [  9.99999998e-01   3.52233522e-27   2.03541441e-20   2.18098381e-09]\n",
      "national\t: [  9.99999999e-01   1.87550089e-15   3.57934935e-20   6.85109339e-10]\n",
      "add\t: [  9.99999993e-01   2.97777379e-14   4.77818595e-19   6.85645223e-09]\n",
      "billion\t: [  9.99999996e-01   6.20544090e-27   5.44410961e-37   4.01110915e-09]\n",
      "budget\t: [  9.99999998e-01   4.58756477e-27   3.75316337e-37   1.97486406e-09]\n",
      "say\t: [  9.99999993e-01   1.42754157e-13   5.03570476e-19   7.34520079e-09]\n",
      "president\t: [  9.99999999e-01   6.89842232e-14   1.14293076e-18   9.47994621e-10]\n",
      "21\t: [  9.99999999e-01   3.45762702e-14   4.27089636e-19   9.74619777e-10]\n",
      "develop\t: [  9.99999999e-01   1.07398409e-14   1.39686966e-37   5.86152647e-10]\n",
      "mid\t: [  9.99999991e-01   1.64250032e-26   1.75502347e-18   9.37152411e-09]\n",
      "original\t: [  1.00000000e+00   3.44817427e-14   3.36332657e-37   4.67878081e-10]\n",
      "require\t: [  1.00000000e+00   1.21898610e-14   8.28141690e-21   3.16984833e-10]\n",
      "need\t: [  9.99999995e-01   7.21037032e-14   4.04461741e-19   5.09654869e-09]\n",
      "job\t: [  9.99999999e-01   7.60945812e-15   1.29631987e-20   1.23618445e-09]\n",
      "consider\t: [  9.99999997e-01   1.01220072e-13   5.97908069e-20   2.89296905e-09]\n",
      "option\t: [  9.99999996e-01   6.15096339e-27   8.57747695e-20   3.86346994e-09]\n",
      "new\t: [  9.99999998e-01   3.94429933e-14   7.88314089e-20   1.96944388e-09]\n",
      "center\t: [  9.99999999e-01   1.06817085e-14   6.12731592e-19   1.23036029e-09]\n",
      "international\t: [  1.00000000e+00   8.55734708e-15   1.30252837e-20   3.82833841e-10]\n",
      "receive\t: [  9.99999998e-01   1.98298197e-14   3.34470766e-20   2.01974589e-09]\n",
      "sci\t: [  9.99999999e-01   1.69304625e-14   2.50245539e-21   1.05195471e-09]\n",
      "generally\t: [  9.99999997e-01   2.66984682e-14   2.80997852e-37   2.52362138e-09]\n",
      "program\t: [  9.99999999e-01   3.97490121e-15   6.54304760e-21   7.06272469e-10]\n",
      "russian\t: [  9.99999999e-01   1.06245753e-26   2.21712299e-19   1.46171643e-09]\n",
      "member\t: [  9.99999998e-01   1.23566230e-14   2.21645907e-19   2.18254207e-09]\n",
      "total\t: [  9.99999999e-01   1.80262318e-14   7.80035468e-19   1.02828447e-09]\n",
      "hope\t: [  9.99999997e-01   2.69230566e-13   8.30971583e-19   3.06743785e-09]\n",
      "16\t: [  9.99999998e-01   6.87153888e-14   8.70276343e-19   1.87187212e-09]\n",
      "nasa\t: [  1.00000000e+00   6.48525987e-16   3.06114216e-21   2.95309435e-10]\n",
      "technology\t: [  9.99999999e-01   1.23510949e-14   2.26012630e-20   1.17441527e-09]\n",
      "development\t: [  1.00000000e+00   1.05388719e-15   1.25024528e-37   2.16651328e-10]\n",
      "team\t: [  9.99999998e-01   4.35086960e-15   1.50883608e-18   1.77700854e-09]\n",
      "washington\t: [  9.99999999e-01   2.14045783e-16   3.25774359e-19   8.75071953e-10]\n",
      "forward\t: [  9.99999999e-01   2.98829890e-14   2.29952297e-19   1.12637421e-09]\n",
      "follow\t: [  9.99999996e-01   7.49885302e-14   2.66383578e-19   4.04059901e-09]\n",
      "distribution\t: [  9.99999997e-01   4.88316055e-16   1.47165425e-19   2.57877072e-09]\n",
      "use\t: [  9.99999997e-01   4.98725965e-14   1.39233822e-19   2.98653079e-09]\n",
      "cmu\t: [  9.99999992e-01   1.98420391e-15   6.17912581e-19   7.88110919e-09]\n",
      "state\t: [  9.99999997e-01   5.08915286e-15   3.66300266e-19   3.05493266e-09]\n",
      "reserve\t: [  9.99999995e-01   7.90483309e-15   9.89675523e-21   5.01439181e-09]\n",
      "decide\t: [  9.99999991e-01   1.06123680e-13   2.94368576e-18   9.00751871e-09]\n",
      "date\t: [  1.00000000e+00   2.56311535e-15   1.33617266e-37   1.27897850e-10]\n",
      "topic\t: [  9.99999998e-01   2.54472852e-14   2.55361811e-37   1.52332193e-09]\n",
      "announce\t: [  9.99999988e-01   1.48584266e-13   2.72551830e-19   1.17441756e-08]\n",
      "process\t: [  1.00000000e+00   7.79012959e-15   9.60253046e-38   3.63616940e-10]\n",
      "gibbon\t: [  9.99999996e-01   3.40747722e-27   2.88450405e-37   4.16373421e-09]\n",
      "john\t: [  9.99999992e-01   1.60105668e-13   1.46846566e-18   7.87621653e-09]\n",
      "shuttle\t: [  9.99999999e-01   4.22849187e-16   1.30099861e-37   6.05762756e-10]\n",
      "office\t: [  9.99999999e-01   2.92459397e-14   4.24345216e-20   5.73615344e-10]\n",
      "information\t: [  9.99999998e-01   2.00000426e-14   1.47193685e-19   1.67324600e-09]\n",
      "redesign\t: [  1.00000000e+00   4.62216246e-16   1.55325243e-37   1.40121327e-17]\n",
      "april\t: [  9.99999999e-01   6.95403078e-16   4.97803472e-20   1.07247800e-09]\n",
      "available\t: [  9.99999999e-01   1.79478651e-14   9.40319405e-21   1.37914373e-09]\n",
      "00\t: [  1.00000000e+00   3.77277245e-14   5.94588307e-19   1.49613501e-10]\n",
      "end\t: [  9.99999997e-01   6.02541947e-14   6.44289540e-19   2.82891493e-09]\n",
      "course\t: [  9.99999982e-01   3.22020998e-13   1.50291165e-19   1.78688138e-08]\n",
      "sender\t: [  9.99999992e-01   1.29535019e-15   9.64090686e-37   8.31292636e-09]\n",
      "station\t: [  9.99999999e-01   2.30786009e-16   8.05456689e-21   1.09932969e-09]\n",
      "include\t: [  9.99999999e-01   1.12959939e-14   2.10101891e-19   1.26021191e-09]\n",
      "committee\t: [  1.00000000e+00   2.06887717e-16   8.11480732e-38   7.49723419e-18]\n",
      "space\t: [  9.99999999e-01   2.85400743e-15   2.32352824e-21   1.09850552e-09]\n",
      "apr\t: [  9.99999993e-01   7.54412111e-13   4.59789280e-19   6.92356190e-09]\n",
      "capability\t: [  1.00000000e+00   1.70039780e-27   1.09753982e-37   3.27723294e-10]\n",
      "isu\t: [  1.00000000e+00   3.89135114e-16   1.84531871e-37   2.95457963e-10]\n",
      "exploration\t: [  1.00000000e+00   5.19757443e-28   4.43323909e-38   4.25704483e-18]\n",
      "high\t: [  9.99999998e-01   1.08579644e-14   2.09064908e-19   1.73664419e-09]\n",
      "result\t: [  1.00000000e+00   6.71368450e-14   8.66484258e-20   4.59962071e-10]\n",
      "ask\t: [  9.99999998e-01   7.62445408e-14   1.65814139e-19   1.50720583e-09]\n",
      "fund\t: [  9.99999999e-01   1.93057031e-27   6.00808182e-21   5.83480864e-10]\n",
      "effort\t: [  1.00000000e+00   2.33943775e-16   3.19590749e-20   1.39846117e-10]\n",
      "example\t: [  9.99999988e-01   1.32615474e-13   3.09982534e-19   1.24782028e-08]\n",
      "low\t: [  9.99999998e-01   2.77264224e-27   6.20971827e-20   1.89839612e-09]\n",
      "year\t: [  9.99999996e-01   2.60916233e-14   5.17727599e-19   3.57120972e-09]\n",
      "research\t: [  9.99999999e-01   1.53324862e-14   8.45956726e-21   9.67848827e-10]\n",
      "make\t: [  9.99999993e-01   1.23240957e-13   6.65656458e-19   6.57534075e-09]\n",
      "possible\t: [  9.99999997e-01   4.99410377e-14   3.43266297e-21   2.87622975e-09]\n",
      "cover\t: [  9.99999999e-01   1.10379662e-14   4.25291948e-20   1.24264849e-09]\n",
      "future\t: [  9.99999997e-01   5.12319470e-14   9.29509683e-19   3.18071056e-09]\n",
      "white\t: [  9.99999998e-01   3.96374226e-14   4.98418391e-20   1.91371101e-09]\n",
      "european\t: [  9.99999999e-01   1.77478467e-27   3.36965009e-20   5.69212163e-10]\n",
      "support\t: [  9.99999998e-01   1.15797954e-14   1.99771509e-37   2.11579621e-09]\n",
      "1993\t: [  9.99999999e-01   3.80232315e-14   1.45174643e-19   1.28581804e-09]\n",
      "________________________________\n",
      "20\t: [  3.28008014e-18   7.36612175e-01   2.49964278e-01   1.34235473e-02]\n",
      "think\t: [  6.19459390e-19   7.49484342e-01   1.50530507e-01   9.99851507e-02]\n",
      "oh\t: [  8.09329507e-20   8.63344306e-01   7.09144239e-02   6.57412697e-02]\n",
      "know\t: [  1.41030147e-18   8.43967600e-01   6.85329698e-02   8.74994299e-02]\n",
      "look\t: [  7.71026034e-19   5.35565497e-01   3.02322944e-01   1.62111559e-01]\n",
      "sure\t: [  8.81855442e-19   8.05449400e-01   7.11789001e-02   1.23371700e-01]\n",
      "indiana\t: [  2.42007794e-19   9.53126383e-01   2.83915599e-19   4.68736168e-02]\n",
      "27\t: [  5.93370208e-18   3.72520189e-01   5.44166539e-01   8.33132723e-02]\n",
      "apr\t: [  2.61088630e-19   9.64038267e-01   2.11342936e-02   1.48274398e-02]\n",
      "teach\t: [  2.33643147e-18   9.99999999e-01   1.72699881e-19   7.09136340e-10]\n",
      "joseph\t: [  1.02572158e-18   9.92570906e-01   3.44817800e-19   7.42909388e-03]\n",
      "ucs\t: [  2.17036950e-19   8.54780915e-01   4.33628291e-19   1.45219085e-01]\n",
      "19\t: [  2.23873891e-18   6.51997522e-01   3.39952420e-01   8.05005855e-03]\n",
      "read\t: [  2.39717953e-19   8.06449773e-01   1.06040138e-01   8.75100883e-02]\n",
      "17\t: [  1.89586143e-18   6.43606522e-01   3.52260137e-01   4.13334022e-03]\n",
      "step\t: [  1.38872592e-18   6.28873534e-01   3.48135544e-01   2.29909211e-02]\n",
      "issue\t: [  4.28165566e-18   7.98574863e-01   1.25963478e-01   7.54616593e-02]\n",
      "paul\t: [  2.88685024e-36   8.78804704e-01   1.11707596e-01   9.48770025e-03]\n",
      "quick\t: [  3.19481010e-17   9.58917997e-02   3.08126950e-18   9.04108200e-01]\n",
      "rutgers\t: [  4.65809598e-20   9.96254996e-01   2.28524275e-19   3.74500360e-03]\n",
      "control\t: [  4.62585988e-19   2.92290753e-02   8.54695337e-01   1.16075587e-01]\n",
      "news\t: [  5.37854473e-18   3.82750048e-01   4.47527767e-01   1.69722185e-01]\n",
      "especially\t: [  4.39149880e-18   9.71695802e-01   3.23410222e-19   2.83041984e-02]\n",
      "44\t: [  8.68678488e-19   2.41064222e-01   6.35329498e-01   1.23606280e-01]\n",
      "appropriate\t: [  5.45438373e-18   9.55826877e-01   2.56723098e-19   4.41731232e-02]\n",
      "10\t: [  3.39939036e-18   6.94466543e-01   2.75261488e-01   3.02719689e-02]\n",
      "true\t: [  1.64963631e-18   9.36584758e-01   1.64953969e-02   4.69198446e-02]\n",
      "act\t: [  1.89638985e-18   9.95310782e-01   1.41312766e-19   4.68921817e-03]\n",
      "joe\t: [  4.80338848e-19   6.14642402e-01   3.81229214e-01   4.12838431e-03]\n",
      "make\t: [  1.29146788e-18   7.78997981e-01   1.51347306e-01   6.96547125e-02]\n",
      "1993\t: [  4.50034072e-18   8.37514006e-01   1.15020989e-01   4.74650057e-02]\n",
      "deal\t: [  1.30871760e-18   8.64140282e-01   5.67353517e-02   7.91243667e-02]\n",
      "bad\t: [  2.46861771e-19   6.58665192e-01   2.27782508e-01   1.13552300e-01]\n",
      "accept\t: [  2.77221560e-36   9.55638674e-01   1.04035613e-19   4.43613261e-02]\n",
      "speak\t: [  1.44183625e-19   9.24152386e-01   1.59672433e-19   7.58476136e-02]\n",
      "god\t: [  1.49514947e-20   9.77473617e-01   2.90770284e-03   1.96186802e-02]\n",
      "att\t: [  6.44416424e-36   1.02963249e-13   8.60179131e-01   1.39820869e-01]\n",
      "25\t: [  4.48612497e-18   6.66707572e-01   2.77748969e-01   5.55434583e-02]\n",
      "01\t: [  6.68846407e-18   7.64974576e-01   2.35025423e-01   9.18659381e-10]\n",
      "34\t: [  1.10394964e-18   3.94000214e-01   5.13324625e-01   9.26751605e-02]\n",
      "strong\t: [  3.31232979e-20   7.53238146e-01   2.44758360e-01   2.00349383e-03]\n",
      "good\t: [  1.42661389e-18   7.75978254e-01   1.47486110e-01   7.65356362e-02]\n",
      "come\t: [  5.24800284e-19   7.84389785e-01   1.39414416e-01   7.61957987e-02]\n",
      "________________________________\n",
      "man\t: [  1.37333842e-01   4.15796459e-13   8.62666135e-01   2.29174980e-08]\n",
      "aspect\t: [  9.99999994e-01   3.51560677e-13   7.35784882e-19   6.30901057e-09]\n",
      "good\t: [  1.22099299e-01   2.98395677e-13   8.77900683e-01   1.75588469e-08]\n",
      "canada\t: [  3.99828405e-01   4.59210418e-14   6.00171590e-01   5.06921567e-09]\n",
      "ca\t: [  1.85531177e-01   1.10334406e-13   8.14468804e-01   1.84229199e-08]\n",
      "score\t: [  1.11579771e-19   1.58142882e-14   1.00000000e+00   2.22108197e-10]\n",
      "john\t: [  5.26867410e-02   1.85499164e-13   9.47313250e-01   9.12416619e-09]\n",
      "dave\t: [  1.55205129e-03   8.55893016e-15   9.98447949e-01   2.74646691e-17]\n",
      "tool\t: [  9.99999995e-01   2.40300655e-13   4.68563346e-18   4.78500052e-09]\n",
      "interested\t: [  2.40464967e-01   9.24891241e-14   7.59535026e-01   7.06218629e-09]\n",
      "scorer\t: [  1.42469952e-19   6.11549430e-27   1.00000000e+00   7.58981389e-17]\n",
      "reply\t: [  1.33757936e-01   6.67838856e-13   8.66241980e-01   8.34067936e-08]\n",
      "post\t: [  2.01109725e-01   1.19345352e-13   7.98890255e-01   2.02646340e-08]\n",
      "play\t: [  4.54890654e-03   4.23043455e-15   9.95451090e-01   3.49978488e-09]\n",
      "consider\t: [  5.77338073e-01   1.28508174e-12   4.22661890e-01   3.67238485e-08]\n",
      "player\t: [  1.99317636e-03   1.80575030e-15   9.98006822e-01   1.85880594e-09]\n",
      "plus\t: [  1.52684171e-01   1.35955677e-15   8.47315821e-01   8.20687536e-09]\n",
      "real\t: [  5.01065381e-01   9.40537737e-13   4.98934562e-01   5.72764818e-08]\n",
      "nntp\t: [  1.80916368e-01   3.03409834e-14   8.19083615e-01   1.70460421e-08]\n",
      "account\t: [  6.27753238e-01   4.69484197e-13   3.72246706e-01   5.61244715e-08]\n",
      "fi\t: [  3.46364929e-02   4.60598280e-13   9.65363506e-01   1.25382538e-09]\n",
      "total\t: [  9.47789347e-02   3.75708212e-14   9.05221063e-01   2.14288715e-09]\n",
      "point\t: [  3.94824340e-02   2.09826146e-13   9.60517557e-01   9.26637657e-09]\n",
      "24\t: [  1.14433584e-01   6.36728295e-14   8.85566415e-01   7.13129899e-10]\n",
      "defensive\t: [  9.89079448e-20   3.93178797e-15   1.00000000e+00   1.91307765e-10]\n",
      "power\t: [  1.27333899e-01   8.05199768e-14   8.72666097e-01   4.37386939e-09]\n",
      "________________________________\n",
      "unless\t: [  2.47650092e-03   4.90330903e-14   2.34385935e-19   9.97523499e-01]\n",
      "dept\t: [  9.33521255e-02   2.90398052e-13   9.72459586e-19   9.06647875e-01]\n",
      "info\t: [  1.10852811e-02   4.98854342e-14   2.85871922e-19   9.88914719e-01]\n",
      "real\t: [  2.65828223e-02   1.79259810e-13   1.70787362e-19   9.73417178e-01]\n",
      "look\t: [  5.50826317e-03   6.17581816e-14   9.69191826e-19   9.94491737e-01]\n",
      "remember\t: [  5.61092488e-03   1.87544011e-13   3.32502709e-18   9.94389075e-01]\n",
      "note\t: [  1.00443859e-01   2.33984271e-13   1.94329400e-18   8.99556141e-01]\n",
      "science\t: [  2.79663696e-01   2.35572969e-13   1.13435820e-18   7.20336304e-01]\n",
      "try\t: [  1.29109474e-02   2.70010578e-13   9.88664347e-19   9.87089053e-01]\n",
      "mi\t: [  6.19547213e-03   7.46643905e-14   1.58823589e-20   9.93804528e-01]\n",
      "quick\t: [  3.95247615e-02   1.91488170e-15   1.71059378e-36   9.60475239e-01]\n",
      "jim\t: [  4.46596489e-02   1.13686773e-13   2.43659879e-18   9.55340351e-01]\n",
      "cc\t: [  1.81927788e-03   1.40663330e-14   3.30436486e-19   9.98180722e-01]\n",
      "computer\t: [  1.01157668e-02   6.29100596e-14   2.17546011e-19   9.89884233e-01]\n",
      "road\t: [  6.71930559e-02   1.08830912e-26   2.27552479e-18   9.32806944e-01]\n",
      "good\t: [  2.12458836e-02   1.86532354e-13   9.85626711e-19   9.78754116e-01]\n",
      "great\t: [  2.30675774e-02   2.01952542e-13   1.07918062e-18   9.76932423e-01]\n",
      "stop\t: [  1.11733184e-03   3.76277314e-13   3.19597390e-18   9.98882668e-01]\n",
      "number\t: [  3.56918172e-02   1.07489357e-13   2.44016604e-18   9.64308183e-01]\n",
      "major\t: [  7.40401468e-02   6.99287653e-14   2.94560351e-18   9.25959853e-01]\n",
      "car\t: [  7.40713565e-05   1.32829538e-16   2.34433842e-20   9.99925929e-01]\n",
      "local\t: [  5.20241545e-02   1.00289692e-14   1.27193412e-18   9.47975845e-01]\n",
      "later\t: [  1.60813013e-02   2.10035150e-13   1.29502202e-18   9.83918699e-01]\n",
      "hand\t: [  2.52139389e-02   7.26917924e-13   5.08530249e-18   9.74786061e-01]\n",
      "mount\t: [  6.23450640e-20   4.67556173e-13   4.06038049e-18   1.00000000e+00]\n",
      "copy\t: [  9.99999909e-01   1.40244682e-12   1.14443493e-35   9.10072130e-08]\n",
      "difference\t: [  4.84290553e-04   9.19912178e-14   8.84633170e-19   9.99515709e-01]\n",
      "time\t: [  1.92062865e-02   1.80918408e-13   7.96383669e-19   9.80793713e-01]\n",
      "large\t: [  5.80419945e-02   5.29216709e-14   1.60491415e-20   9.41958006e-01]\n",
      "phone\t: [  1.65930503e-01   6.36894149e-14   3.60458330e-18   8.34069497e-01]\n",
      "use\t: [  4.53808834e-02   1.78800552e-13   4.99173612e-19   9.54619117e-01]\n",
      "reply\t: [  4.98119382e-03   8.93481533e-14   2.08141442e-19   9.95018806e-01]\n",
      "reference\t: [  1.71912290e-01   3.00435049e-13   4.00361467e-20   8.28087710e-01]\n",
      "company\t: [  5.54214797e-02   5.78626979e-14   1.44720302e-36   9.44578520e-01]\n",
      "space\t: [  1.14451135e-01   2.58053131e-14   2.10088358e-20   8.85548865e-01]\n",
      "king\t: [  1.45257296e-19   3.12024769e-13   9.43510539e-18   1.00000000e+00]\n",
      "possible\t: [  4.70393394e-02   1.85589203e-13   1.27563465e-20   9.52960661e-01]\n",
      "big\t: [  2.30738130e-03   9.44001185e-15   7.25662841e-19   9.97692619e-01]\n",
      "help\t: [  3.87132960e-02   2.56578445e-13   4.70578983e-19   9.61286704e-01]\n",
      "think\t: [  7.16330755e-03   1.39894162e-13   7.81120933e-19   9.92836692e-01]\n",
      "thing\t: [  7.64646773e-03   2.25718863e-13   9.46356643e-19   9.92353532e-01]\n",
      "forget\t: [  3.01275507e-03   1.77079704e-13   3.11070098e-19   9.96987245e-01]\n",
      "able\t: [  6.02727191e-04   1.70694855e-13   9.51465231e-19   9.99397273e-01]\n",
      "make\t: [  2.11355763e-02   2.05779964e-13   1.11147110e-18   9.78864424e-01]\n",
      "include\t: [  1.01252118e-01   9.03571438e-14   1.68061411e-18   8.98747882e-01]\n",
      "________________________________\n",
      "cc\t: [  2.04586780e-19   2.38979127e-14   9.69069450e-02   9.03093055e-01]\n",
      "maybe\t: [  4.79126734e-19   4.20204016e-13   1.40850412e-01   8.59149588e-01]\n",
      "thing\t: [  7.31592138e-19   3.26269571e-13   2.36130118e-01   7.63869882e-01]\n",
      "start\t: [  3.60705973e-18   1.38128918e-13   3.56804277e-01   6.43195723e-01]\n",
      "pat\t: [  2.09811445e-18   7.38218095e-14   8.49583667e-02   9.15041633e-01]\n",
      "mi\t: [  7.70876234e-19   1.40353847e-13   5.15363474e-03   9.94846365e-01]\n",
      "try\t: [  1.22730006e-18   3.87769792e-13   2.45092431e-01   7.54907569e-01]\n",
      "lot\t: [  3.83012660e-19   2.85020766e-13   1.75327465e-01   8.24672535e-01]\n",
      "think\t: [  7.14559410e-19   2.10826348e-13   2.03203602e-01   7.96796398e-01]\n",
      "line\t: [  7.49059559e-19   2.40782094e-14   3.92788195e-01   6.07211805e-01]\n",
      "toronto\t: [  6.31718162e-19   5.47504126e-26   2.07505034e-01   7.92494966e-01]\n",
      "like\t: [  1.66635575e-18   2.22863175e-13   1.95553970e-01   8.04446030e-01]\n",
      "jason\t: [  3.86018920e-36   1.49555749e-13   8.23411715e-01   1.76588285e-01]\n",
      "look\t: [  5.23172868e-19   8.86187420e-14   2.40064894e-01   7.59935106e-01]\n",
      "check\t: [  1.23670064e-19   3.58306982e-14   3.37816783e-01   6.62183217e-01]\n",
      "different\t: [  1.93727201e-19   3.99695762e-13   6.74123335e-02   9.32587667e-01]\n",
      "way\t: [  7.20960156e-19   2.93294461e-13   2.48567513e-01   7.51432487e-01]\n",
      "gilmour\t: [  7.23133544e-36   2.07635138e-26   9.69439698e-01   3.05603020e-02]\n",
      "mean\t: [  7.44536676e-19   2.96358273e-13   2.09323062e-01   7.90676938e-01]\n",
      "oh\t: [  1.50667004e-19   3.91934622e-13   1.54492974e-01   8.45507026e-01]\n",
      "fan\t: [  8.28887141e-36   2.77090812e-26   3.52902530e-01   6.47097470e-01]\n",
      "wing\t: [  3.77036348e-20   1.54034919e-26   9.56292300e-01   4.37076999e-02]\n",
      "play\t: [  1.76520368e-19   8.90991077e-15   6.49982355e-01   3.50017645e-01]\n",
      "post\t: [  3.06242617e-18   9.86367219e-14   2.04697958e-01   7.95302042e-01]\n",
      "sun\t: [  4.37549620e-18   6.57105732e-14   2.19139768e-01   7.80860232e-01]\n",
      "state\t: [  4.02279810e-18   2.44347675e-14   3.03590222e-01   6.96409778e-01]\n",
      "usually\t: [  3.89353989e-18   1.47993360e-13   2.87828813e-01   7.12171187e-01]\n",
      "30\t: [  8.13483170e-18   9.84027281e-14   6.74789062e-01   3.25210938e-01]\n",
      "correct\t: [  5.98689545e-36   1.97883814e-13   8.57627141e-02   9.14237286e-01]\n",
      "player\t: [  9.23467230e-20   4.54081724e-15   7.78042108e-01   2.21957892e-01]\n",
      "nntp\t: [  3.13466967e-18   2.85327834e-14   2.38800917e-01   7.61199083e-01]\n",
      "________________________________\n",
      "reason\t: [  1.96554783e-03   9.98034451e-01   1.14550890e-20   9.38292424e-10]\n",
      "stage\t: [  9.99999986e-01   1.25979213e-12   3.37667862e-36   1.42658466e-08]\n",
      "law\t: [  2.49235599e-20   9.99999999e-01   1.53219566e-37   9.09103862e-10]\n",
      "limit\t: [  1.40677829e-03   9.98593219e-01   3.45164160e-19   2.38449352e-09]\n",
      "position\t: [  1.45855652e-02   9.85414433e-01   2.22834012e-19   1.93875497e-09]\n",
      "small\t: [  1.64926564e-01   8.35073432e-01   4.70247133e-20   4.28380368e-09]\n",
      "assume\t: [  9.56479203e-04   9.99043521e-01   2.83247661e-20   4.04288969e-11]\n",
      "fact\t: [  1.31901071e-02   9.86809892e-01   1.98909022e-21   9.18843590e-10]\n",
      "thing\t: [  3.98053728e-03   9.96019461e-01   1.40252829e-19   1.64928706e-09]\n",
      "second\t: [  1.47676022e-02   9.85232396e-01   5.23245675e-19   1.94082033e-09]\n",
      "understanding\t: [  1.26260863e-02   9.87373914e-01   5.59755855e-21   1.14169154e-10]\n",
      "necessarily\t: [  3.25285540e-03   9.96747141e-01   4.28596914e-20   3.81738749e-09]\n",
      "say\t: [  1.03523457e-02   9.89647653e-01   1.17248938e-19   1.70994295e-09]\n",
      "nature\t: [  3.12845506e-02   9.68715449e-01   2.05312245e-37   4.00804893e-10]\n",
      "original\t: [  4.15093203e-02   9.58490679e-01   3.13996313e-37   4.36734501e-10]\n",
      "appreciate\t: [  9.51226688e-03   9.90487712e-01   2.33651890e-36   2.11722680e-08]\n",
      "create\t: [  3.15261469e-20   9.99999998e-01   1.95327303e-19   1.66226306e-09]\n",
      "likely\t: [  3.62664160e-02   9.63733581e-01   5.48533223e-19   2.55606112e-09]\n",
      "happen\t: [  3.89961939e-03   9.96100380e-01   1.31359439e-19   5.76902872e-10]\n",
      "belief\t: [  5.97838107e-04   9.99402161e-01   1.30388203e-37   9.17864098e-10]\n",
      "couple\t: [  2.16753202e-02   9.78324675e-01   8.79479925e-19   4.71709399e-09]\n",
      "human\t: [  1.27915086e-02   9.87208491e-01   1.29119744e-37   6.08368932e-10]\n",
      "exist\t: [  2.24467250e-20   9.99999999e-01   5.27217866e-20   1.07146522e-09]\n",
      "far\t: [  1.44551738e-02   9.85544826e-01   1.25137026e-19   6.78847663e-10]\n",
      "fi\t: [  2.46332276e-03   9.97536677e-01   1.26112953e-19   9.11988672e-11]\n",
      "base\t: [  6.48595778e-02   9.35140421e-01   1.05352805e-19   1.66340738e-09]\n",
      "following\t: [  2.09151882e-02   9.79084811e-01   4.83588514e-20   1.13247538e-09]\n",
      "live\t: [  2.72237301e-03   9.97277627e-01   1.42620407e-37   2.82815693e-10]\n",
      "need\t: [  2.02902222e-02   9.79709775e-01   1.84575537e-19   2.32542406e-09]\n",
      "non\t: [  1.74504639e-02   9.82549536e-01   1.58445630e-37   3.84048185e-10]\n",
      "new\t: [  3.64786219e-02   9.63521376e-01   6.46766624e-20   1.61555301e-09]\n",
      "company\t: [  1.01523592e-01   8.98476402e-01   7.54734635e-37   5.52428304e-09]\n",
      "state\t: [  2.26860664e-01   7.73139320e-01   1.86898718e-18   1.55847565e-08]\n",
      "leave\t: [  5.50401592e-03   9.94495981e-01   2.43368444e-19   2.96902680e-09]\n",
      "discussion\t: [  3.12778679e-03   9.96872212e-01   1.40834183e-37   9.26805183e-10]\n",
      "time\t: [  1.23690460e-02   9.87630952e-01   1.46012859e-19   2.01659863e-09]\n",
      "cause\t: [  3.51564352e-20   9.99999999e-01   3.30022133e-20   9.65143696e-10]\n",
      "devil\t: [  3.54032416e-20   9.99999999e-01   4.14967269e-19   5.98988210e-10]\n",
      "general\t: [  3.92217822e-02   9.60778217e-01   3.43402656e-20   1.11917677e-09]\n",
      "christianity\t: [  2.71426324e-20   1.00000000e+00   1.40981617e-37   7.56516569e-11]\n",
      "propose\t: [  4.81660780e-02   9.51833922e-01   7.18114168e-20   1.99697434e-17]\n",
      "useful\t: [  2.65959039e-03   9.97340408e-01   2.15607410e-20   1.41979669e-09]\n",
      "claim\t: [  3.71575744e-04   9.99628424e-01   2.74574223e-20   2.67282113e-10]\n",
      "return\t: [  6.65358598e-02   9.33464139e-01   4.65963265e-19   9.08825030e-10]\n",
      "understand\t: [  1.24801162e-02   9.87519883e-01   2.32817398e-20   7.33826374e-10]\n",
      "different\t: [  8.63112618e-04   9.99136886e-01   3.27871602e-20   1.64881044e-09]\n",
      "certainly\t: [  6.07508045e-03   9.93924918e-01   1.68063274e-19   2.01398562e-09]\n",
      "effect\t: [  1.39575957e-20   9.99999998e-01   6.22427606e-38   1.56982309e-09]\n",
      "problem\t: [  2.55044123e-03   9.97449555e-01   2.70510083e-19   3.27024725e-09]\n",
      "appear\t: [  1.75799772e-20   1.00000000e+00   1.00739224e-37   8.29500699e-11]\n",
      "note\t: [  4.82017169e-02   9.51798282e-01   2.65493313e-19   1.37821314e-09]\n",
      "source\t: [  5.27536778e-02   9.47246320e-01   2.74071837e-20   1.77027938e-09]\n",
      "appropriate\t: [  4.00373548e-02   9.59962644e-01   2.40742956e-37   8.88931604e-10]\n",
      "correct\t: [  5.39228469e-20   9.99999997e-01   8.43251719e-20   3.26764047e-09]\n",
      "summary\t: [  2.71227603e-01   7.28772387e-01   6.68001051e-18   9.12206447e-09]\n",
      "believe\t: [  3.70324633e-04   9.99629675e-01   6.03344717e-20   7.19653564e-10]\n",
      "today\t: [  7.30004620e-03   9.92699953e-01   1.47958115e-21   3.53181282e-10]\n",
      "view\t: [  8.53575234e-03   9.91464247e-01   1.43059095e-37   8.17346790e-10]\n",
      "require\t: [  1.09134110e-01   8.90865889e-01   2.03270824e-20   7.77925797e-10]\n",
      "mind\t: [  1.89211709e-03   9.98107882e-01   1.31747558e-19   1.30415443e-09]\n",
      "ask\t: [  1.92094293e-02   9.80790570e-01   7.16384058e-20   6.51067785e-10]\n",
      "clear\t: [  2.05117151e-20   9.99999999e-01   1.79200023e-19   1.34803484e-09]\n",
      "question\t: [  2.09797076e-02   9.79020292e-01   6.89579974e-20   8.15529573e-10]\n",
      "use\t: [  2.90718095e-02   9.70928189e-01   9.10388362e-20   1.95244245e-09]\n",
      "outside\t: [  9.25322144e-03   9.90746773e-01   5.86327957e-20   5.21810644e-09]\n",
      "lead\t: [  1.42264749e-03   9.98577352e-01   4.52389263e-19   4.12700360e-10]\n",
      "tool\t: [  1.20225663e-01   8.79774336e-01   1.03477733e-36   5.88361369e-10]\n",
      "person\t: [  2.24069396e-02   9.77593059e-01   1.15471039e-21   9.84780918e-10]\n",
      "know\t: [  1.20658734e-02   9.87934125e-01   7.49057770e-20   2.05230820e-09]\n",
      "begin\t: [  2.43062782e-02   9.75693719e-01   1.79333067e-37   2.35643381e-09]\n",
      "technology\t: [  1.07863118e-01   8.92136879e-01   5.48296618e-20   2.84861554e-09]\n",
      "statement\t: [  2.66009211e-03   9.97339907e-01   1.17546111e-37   9.96195856e-10]\n",
      "tell\t: [  7.26415789e-03   9.92735841e-01   4.88901021e-20   7.73451515e-10]\n",
      "1993\t: [  3.77892368e-02   9.62210762e-01   1.23386821e-19   1.09266445e-09]\n",
      "term\t: [  3.35655384e-02   9.66434460e-01   1.76483228e-20   1.45964486e-09]\n",
      "agree\t: [  2.97618293e-20   9.99999999e-01   4.27432368e-20   1.02040111e-09]\n",
      "think\t: [  6.00453350e-03   9.93995464e-01   1.86406010e-19   2.65700572e-09]\n",
      "make\t: [  1.19718479e-02   9.88028150e-01   1.79234344e-19   1.77018492e-09]\n",
      "available\t: [  7.68112186e-02   9.23188779e-01   1.62446344e-20   2.38217336e-09]\n",
      "actually\t: [  7.45651151e-03   9.92543486e-01   2.14905693e-19   2.43367052e-09]\n",
      "add\t: [  4.77534376e-02   9.52246555e-01   5.13189283e-19   7.36280461e-09]\n",
      "event\t: [  2.26817701e-02   9.77318229e-01   5.11797184e-21   5.55773329e-10]\n",
      "smith\t: [  5.41039484e-20   9.99999998e-01   2.62078681e-19   1.67906673e-09]\n",
      "matter\t: [  3.38615891e-03   9.96613840e-01   1.76564936e-20   1.00779604e-09]\n",
      "evil\t: [  1.47345070e-03   9.98526549e-01   9.80966180e-38   2.43426071e-10]\n",
      "turn\t: [  7.42070666e-04   9.99257928e-01   1.22925543e-19   9.52886394e-10]\n",
      "hand\t: [  4.07532899e-03   9.95924671e-01   2.33999392e-19   5.03014244e-10]\n",
      "apr\t: [  1.97551112e-03   9.98024489e-01   2.04290518e-20   3.07573021e-10]\n",
      "doubt\t: [  4.45598209e-20   9.99999999e-01   5.69283861e-20   1.11164308e-09]\n",
      "19\t: [  2.44814770e-02   9.75518523e-01   4.74920877e-19   2.41336917e-10]\n",
      "argument\t: [  3.26838446e-04   9.99673161e-01   1.22543965e-37   2.90712020e-10]\n",
      "hope\t: [  5.51595090e-03   9.94484049e-01   1.03089980e-19   3.80483075e-10]\n",
      "accept\t: [  2.12020512e-20   9.99999999e-01   1.01648708e-37   9.30134916e-10]\n",
      "hear\t: [  1.75889864e-03   9.98241099e-01   2.19448380e-19   2.01967478e-09]\n",
      "05\t: [  6.00051817e-02   9.39994818e-01   4.87808205e-20   1.46708063e-10]\n",
      "talk\t: [  1.18482445e-03   9.98815175e-01   2.37341686e-20   7.67351326e-10]\n",
      "life\t: [  1.50971668e-02   9.84902833e-01   1.33163960e-21   4.81632890e-10]\n",
      "announce\t: [  9.95018495e-03   9.90049812e-01   6.09944261e-20   2.62780343e-09]\n",
      "athos\t: [  5.09318376e-04   9.99490682e-01   2.50380608e-37   1.12259648e-10]\n",
      "atheist\t: [  2.88415704e-20   1.00000000e+00   1.25504913e-37   1.43459960e-17]\n",
      "suppose\t: [  5.39734008e-03   9.94602660e-01   1.86777838e-37   2.55253123e-10]\n",
      "able\t: [  4.16390050e-04   9.99583608e-01   1.87132165e-19   2.20427977e-09]\n",
      "answer\t: [  1.86231987e-02   9.81376801e-01   7.01145624e-22   6.35121268e-10]\n",
      "true\t: [  1.27095637e-02   9.87290435e-01   1.62358176e-20   9.91036281e-10]\n",
      "good\t: [  1.32588161e-02   9.86741182e-01   1.75112994e-19   1.95008053e-09]\n",
      "request\t: [  1.65615116e-02   9.83438486e-01   3.05857003e-19   1.93090310e-09]\n",
      "provide\t: [  3.68216787e-01   6.31783199e-01   1.03074408e-36   1.34672936e-08]\n",
      "start\t: [  4.44726746e-02   9.55527322e-01   4.80239133e-19   3.14693129e-09]\n",
      "universe\t: [  4.60188661e-02   9.53981134e-01   1.17206651e-37   1.19945305e-17]\n",
      "necessary\t: [  1.43055221e-02   9.85694475e-01   1.06657055e-37   2.44430361e-09]\n",
      "day\t: [  2.35217265e-02   9.76478272e-01   3.12012510e-19   1.68149605e-09]\n",
      "mean\t: [  4.45769319e-03   9.95542305e-01   1.36813432e-19   1.87856985e-09]\n",
      "hold\t: [  5.41521628e-02   9.45847835e-01   8.88752202e-19   1.87200359e-09]\n",
      "opinion\t: [  2.59779736e-02   9.74022023e-01   5.86984318e-20   2.98705557e-09]\n",
      "13\t: [  2.08316133e-02   9.79168386e-01   4.25035998e-19   5.35564373e-10]\n",
      "consider\t: [  1.45385117e-02   9.85461487e-01   1.95507874e-20   9.45807853e-10]\n",
      "faith\t: [  1.56057868e-04   9.99843941e-01   1.31363623e-37   7.69816294e-10]\n",
      "model\t: [  2.80599265e-03   9.97194004e-01   4.35923293e-37   3.80590080e-09]\n",
      "existence\t: [  4.26489513e-04   9.99573510e-01   8.28993736e-38   8.99220711e-18]\n",
      "48\t: [  5.81686151e-20   1.00000000e+00   2.11009036e-18   2.29885142e-17]\n",
      "come\t: [  4.86618046e-03   9.95133818e-01   1.65147038e-19   1.93693803e-09]\n",
      "foundation\t: [  3.10561107e-02   9.68943889e-01   1.44550241e-37   1.61120846e-10]\n",
      "near\t: [  5.43624537e-01   4.56375451e-01   8.68864743e-19   1.18936295e-08]\n",
      "word\t: [  2.63626185e-03   9.97363738e-01   3.57041100e-22   5.55525933e-10]\n",
      "science\t: [  1.22847571e-01   8.77152428e-01   1.41858921e-19   1.01021724e-09]\n",
      "sound\t: [  2.66728588e-02   9.73327138e-01   4.19150171e-21   2.78314982e-09]\n",
      "evidence\t: [  9.87069181e-04   9.99012930e-01   8.87085535e-21   5.02262455e-10]\n",
      "address\t: [  5.59970337e-01   4.40029643e-01   3.15886770e-18   1.98765557e-08]\n",
      "god\t: [  1.11782961e-04   9.99888217e-01   2.77721803e-21   4.02115988e-10]\n",
      "jason\t: [  4.60030780e-20   9.99999999e-01   1.07123048e-18   8.35111348e-10]\n",
      "like\t: [  1.31510822e-02   9.86848915e-01   1.68479857e-19   2.51938697e-09]\n",
      "point\t: [  6.14115234e-03   9.93858846e-01   2.74430686e-19   1.47407940e-09]\n",
      "religion\t: [  2.24187286e-20   1.00000000e+00   6.29631170e-21   3.80810484e-11]\n",
      "rutgers\t: [  3.41612795e-04   9.99658387e-01   2.14105089e-37   7.52954073e-11]\n",
      "best\t: [  1.44770916e-03   9.98552289e-01   4.52898811e-19   1.93593433e-09]\n",
      "christian\t: [  1.35449228e-04   9.99864551e-01   1.59937788e-37   1.13055288e-10]\n",
      "________________________________\n",
      "44\t: [  3.87823067e-01   5.27486923e-15   2.52392890e-01   3.59784044e-01]\n",
      "future\t: [  7.60045590e-01   9.34076690e-15   1.10672067e-01   1.29282343e-01]\n",
      "son\t: [  2.33091318e-01   7.02243213e-14   2.77729458e-02   7.39135736e-01]\n",
      "feel\t: [  8.13577718e-01   4.23782969e-14   2.19076103e-19   1.86422282e-01]\n",
      "send\t: [  8.70608125e-01   7.78550574e-15   5.35463024e-02   7.58455729e-02]\n",
      "st\t: [  8.14202272e-01   5.17861365e-15   3.53955398e-02   1.50402188e-01]\n",
      "12\t: [  8.82992866e-01   3.98253296e-15   6.93763579e-02   4.76307766e-02]\n",
      "concept\t: [  9.35609171e-01   9.93153060e-15   5.27551269e-20   6.43908288e-02]\n",
      "fax\t: [  9.61701095e-01   4.10720186e-15   3.82989037e-02   1.04134992e-09]\n",
      "radio\t: [  8.91813553e-01   6.31019084e-16   4.73256376e-02   6.08608094e-02]\n",
      "note\t: [  9.03863935e-01   6.39343759e-15   3.46759772e-02   6.14600877e-02]\n",
      "spacecraft\t: [  9.93273284e-01   5.99066223e-17   1.81265480e-20   6.72671627e-03]\n",
      "stage\t: [  9.67187124e-01   4.36473988e-28   2.27475596e-20   3.28128764e-02]\n",
      "total\t: [  8.49482605e-01   3.67334290e-15   1.03803811e-01   4.67135837e-02]\n",
      "15\t: [  8.48868889e-01   8.06949126e-15   7.62781871e-02   7.48529243e-02]\n",
      "ca\t: [  7.70605846e-01   4.99913054e-15   4.32818520e-02   1.86112302e-01]\n",
      "john\t: [  6.05602858e-01   2.32593007e-14   1.39314453e-01   2.55082689e-01]\n",
      "use\t: [  8.46363469e-01   1.01256035e-14   1.84606128e-02   1.35175918e-01]\n",
      "near\t: [  9.40590335e-01   2.82860758e-16   1.04710011e-02   4.89386637e-02]\n",
      "hold\t: [  8.35753903e-01   5.22917101e-15   9.55383787e-02   6.87077179e-02]\n",
      "power\t: [  8.53803686e-01   5.88959926e-15   7.48648700e-02   7.13314442e-02]\n",
      "design\t: [  9.63740220e-01   2.05511635e-15   2.27247686e-20   3.62597804e-02]\n",
      "knowledge\t: [  4.27445617e-17   5.08147058e-13   8.57627142e-01   1.42372858e-01]\n",
      "score\t: [  8.36775201e-18   1.29372241e-14   9.59487491e-01   4.05125090e-02]\n",
      "53\t: [  7.95181949e-02   1.14551771e-14   5.11642219e-01   4.08839586e-01]\n",
      "boston\t: [  2.84156469e-01   5.42874921e-27   5.94175810e-01   1.21667721e-01]\n",
      "society\t: [  8.81512657e-01   4.22713565e-15   4.51581301e-20   1.18487343e-01]\n",
      "vehicle\t: [  7.99050627e-01   1.16202775e-15   1.24021537e-03   1.99709158e-01]\n",
      "aspect\t: [  9.84887062e-01   3.77706834e-15   9.27156990e-21   1.51129383e-02]\n",
      "32\t: [  8.77916312e-01   5.46027292e-15   5.35228551e-02   6.85608334e-02]\n",
      "conference\t: [  9.70344604e-01   7.58324150e-17   1.87699752e-02   1.08854208e-02]\n",
      "energy\t: [  9.35525039e-01   6.70406868e-15   3.69288179e-20   6.44749605e-02]\n",
      "60\t: [  8.82591587e-01   1.24886521e-15   2.88714381e-02   8.85369750e-02]\n",
      "corporation\t: [  8.79784189e-01   1.06236297e-27   6.38095254e-03   1.13834858e-01]\n",
      "school\t: [  8.77056020e-01   1.00511038e-14   1.14807922e-02   1.11463188e-01]\n",
      "word\t: [  6.65745624e-01   9.02240490e-14   6.28018674e-04   3.33626357e-01]\n",
      "non\t: [  9.50265262e-01   1.91664330e-14   6.00969890e-20   4.97347381e-02]\n",
      "astronomy\t: [  9.87925739e-01   2.33579095e-28   1.01597975e-20   1.20742611e-02]\n",
      "26\t: [  5.99535532e-01   8.51332296e-15   1.36627297e-01   2.63837171e-01]\n",
      "support\t: [  8.98352640e-01   2.49545512e-15   2.81141436e-20   1.01647360e-01]\n",
      "mass\t: [  9.74960061e-01   5.84728487e-16   3.06062516e-20   2.50399389e-02]\n",
      "28\t: [  8.85765591e-01   5.24985528e-15   5.88695298e-02   5.53648787e-02]\n",
      "file\t: [  3.36683624e-01   4.19883416e-15   3.51893585e-02   6.28127017e-01]\n",
      "gravity\t: [  1.00000000e+00   1.94345779e-28   1.51043877e-20   3.37427121e-10]\n",
      "moon\t: [  9.45290513e-01   1.33249741e-15   6.74530985e-04   5.40349565e-02]\n",
      "past\t: [  2.60816517e-01   7.56043982e-15   4.61856678e-01   2.77326805e-01]\n",
      "various\t: [  9.79080251e-01   4.60435311e-15   5.97698348e-04   2.03220505e-02]\n",
      "22\t: [  8.33935681e-01   7.37828811e-15   1.01550731e-01   6.45135876e-02]\n",
      "defense\t: [  9.13246100e-01   4.74972261e-28   7.92147232e-02   7.53917692e-03]\n",
      "nuclear\t: [  7.37095768e-01   1.95781204e-27   1.29685580e-19   2.62904232e-01]\n",
      "pp\t: [  1.97246549e-18   1.52544745e-27   9.99999998e-01   2.29578825e-09]\n",
      "control\t: [  2.33642272e-01   7.23566401e-16   3.84126092e-01   3.82231636e-01]\n",
      "56\t: [  8.04537658e-01   1.62909573e-15   1.22357549e-01   7.31047929e-02]\n",
      "science\t: [  9.73141943e-01   2.48905018e-15   7.82709929e-03   1.90309574e-02]\n",
      "person\t: [  9.05077729e-01   1.41452429e-14   3.24871027e-04   9.45974002e-02]\n",
      "faq\t: [  9.96282085e-01   3.35090698e-15   3.71791454e-03   3.39115429e-10]\n",
      "effect\t: [  3.73872620e-18   9.59537625e-14   1.16127871e-19   1.00000000e+00]\n",
      "nasa\t: [  9.83988431e-01   1.53080335e-16   4.71865200e-04   1.55397037e-02]\n",
      "unc\t: [  9.94983325e-01   1.16815470e-15   3.65289805e-03   1.36377689e-03]\n",
      "white\t: [  9.00779599e-01   8.56497259e-15   7.03326627e-03   9.21871347e-02]\n",
      "date\t: [  9.93206739e-01   6.10674573e-16   2.07896270e-20   6.79326131e-03]\n",
      "engine\t: [  5.56631619e-01   1.92850266e-27   1.08430476e-19   4.43368381e-01]\n",
      "ad\t: [  8.75926590e-01   7.14607950e-15   1.08491925e-20   1.24073410e-01]\n",
      "24\t: [  8.97530595e-01   5.44776073e-15   8.88654367e-02   1.36039687e-02]\n",
      "flight\t: [  8.36070812e-01   1.72028850e-27   6.51685295e-03   1.57412335e-01]\n",
      "level\t: [  9.86590287e-01   1.35499864e-15   2.15059499e-20   1.34097125e-02]\n",
      "washington\t: [  9.10886737e-01   4.67706153e-17   4.64863810e-02   4.26268816e-02]\n",
      "likely\t: [  7.85570195e-01   7.47801472e-15   8.27595017e-02   1.31670303e-01]\n",
      "money\t: [  8.13277675e-01   4.74778229e-15   4.65184428e-02   1.40203882e-01]\n",
      "earth\t: [  9.76131826e-01   6.17018096e-15   4.55667519e-04   2.34125068e-02]\n",
      "computer\t: [  5.60031538e-01   1.05755234e-14   2.38822550e-02   4.16086207e-01]\n",
      "cc\t: [  1.80964878e-01   4.24858438e-15   6.51769121e-02   7.53858210e-01]\n",
      "100\t: [  8.05415059e-01   2.86278242e-15   1.01688160e-01   9.28967808e-02]\n",
      "book\t: [  9.80274814e-01   1.84351473e-14   7.62589259e-04   1.89625965e-02]\n",
      "19\t: [  8.63138175e-01   1.23204676e-14   1.16626849e-01   2.02349767e-02]\n",
      "exploration\t: [  1.00000000e+00   1.24681709e-28   6.94489385e-21   2.27658381e-10]\n",
      "offer\t: [  9.40645609e-01   1.34860647e-14   6.10333981e-03   5.32510511e-02]\n",
      "satellite\t: [  9.89209593e-01   3.18757166e-28   1.92784643e-20   1.07904074e-02]\n",
      "shuttle\t: [  9.68621506e-01   9.82520535e-17   1.97412841e-20   3.13784939e-02]\n",
      "sport\t: [  1.51890907e-01   8.75472830e-15   2.89708311e-01   5.58400782e-01]\n",
      "text\t: [  7.37577731e-17   4.40764343e-13   1.41910021e-01   8.58089979e-01]\n",
      "27\t: [  8.52409860e-01   2.62287088e-15   6.95597619e-02   7.80303783e-02]\n",
      "center\t: [  8.60744636e-01   2.20555085e-15   8.26207356e-02   5.66346279e-02]\n",
      "reference\t: [  9.64288257e-01   5.11703808e-15   4.45310317e-04   3.52664325e-02]\n",
      "good\t: [  6.93595368e-01   1.84907129e-14   6.38049514e-02   2.42599681e-01]\n",
      "station\t: [  9.43350147e-01   5.22257188e-17   1.19030861e-03   5.54595446e-02]\n",
      "summary\t: [  7.99023592e-01   7.69071271e-16   1.37068428e-01   6.39079804e-02]\n",
      "able\t: [  5.98100214e-02   5.14329819e-14   1.87221891e-01   7.52968087e-01]\n",
      "view\t: [  8.14518100e-01   3.38910446e-14   9.50843357e-20   1.85481900e-01]\n",
      "anybody\t: [  1.90891959e-01   5.02030273e-14   4.36293213e-02   7.65478720e-01]\n",
      "johnson\t: [  4.49227403e-01   1.64713967e-14   3.40582675e-01   2.10189922e-01]\n",
      "plus\t: [  8.32130624e-01   8.08282088e-17   5.90824898e-02   1.08786886e-01]\n",
      "technical\t: [  9.83236434e-01   2.23260740e-28   1.53388571e-20   1.67635656e-02]\n",
      "star\t: [  7.76386098e-01   1.24818088e-27   1.91891475e-01   3.17224274e-02]\n",
      "come\t: [  4.58089922e-01   3.35577609e-14   1.08284897e-01   4.33625181e-01]\n",
      "50\t: [  8.53155001e-01   6.13126697e-15   1.17485991e-01   2.93590083e-02]\n",
      "following\t: [  8.73460732e-01   1.46470743e-14   1.40666978e-02   1.12472571e-01]\n",
      "general\t: [  9.31135670e-01   8.17067154e-15   5.67837226e-03   6.31859574e-02]\n",
      "21\t: [  8.93633882e-01   7.41207488e-15   5.97892517e-02   4.65768663e-02]\n",
      "joe\t: [  5.67453740e-01   3.55884963e-14   4.00749003e-01   3.17972574e-02]\n",
      "canada\t: [  9.52342963e-01   1.19316246e-15   1.82898923e-02   2.93671445e-02]\n",
      "cover\t: [  9.31865004e-01   2.46742573e-15   6.20847050e-03   6.19265259e-02]\n",
      "little\t: [  1.80966796e-01   1.61813083e-14   3.60085969e-01   4.58947235e-01]\n",
      "orbit\t: [  9.71053420e-01   7.07976158e-28   2.30014618e-04   2.87165649e-02]\n",
      "project\t: [  9.54242411e-01   6.60866588e-17   9.09561240e-04   4.48480275e-02]\n",
      "set\t: [  1.53132831e-01   2.23147296e-15   3.41836633e-01   5.05030536e-01]\n",
      "nntp\t: [  7.76946858e-01   1.42138334e-15   4.50046795e-02   1.78048463e-01]\n",
      "american\t: [  8.08509934e-01   7.90477570e-28   1.17253428e-01   7.42366376e-02]\n",
      "institute\t: [  9.00461355e-01   3.89085241e-15   2.37541770e-03   9.71632273e-02]\n",
      "long\t: [  8.74723896e-01   7.56623196e-15   3.16616296e-02   9.36144744e-02]\n",
      "number\t: [  7.45878572e-01   6.82076421e-15   1.01118212e-01   1.53003216e-01]\n",
      "office\t: [  9.64019511e-01   6.76321873e-15   6.40839973e-03   2.95720889e-02]\n",
      "place\t: [  8.52608096e-01   9.01036251e-15   5.93646984e-02   8.80272054e-02]\n",
      "dept\t: [  9.13746057e-01   8.63103948e-15   1.88748452e-02   6.73790975e-02]\n",
      "work\t: [  7.46777802e-01   9.13476898e-15   5.19554346e-02   2.01266764e-01]\n",
      "black\t: [  8.69119183e-01   7.85716193e-15   1.20125746e-01   1.07550716e-02]\n",
      "18\t: [  8.92301974e-01   4.54275733e-15   8.42655455e-02   2.34324804e-02]\n",
      "calgary\t: [  2.73683392e-17   3.44635287e-13   9.99999961e-01   3.92461793e-08]\n",
      "large\t: [  8.89864666e-01   2.46367424e-15   4.87914779e-04   1.09647419e-01]\n",
      "structure\t: [  9.57627144e-01   7.52354596e-16   1.44777782e-20   4.23728564e-02]\n",
      "sail\t: [  9.88807885e-01   2.47922973e-28   7.46565007e-21   1.11921154e-02]\n",
      "begin\t: [  8.12641984e-01   1.16853914e-14   4.17614662e-20   1.87358016e-01]\n",
      "study\t: [  8.80889654e-01   4.62008539e-15   6.52180999e-04   1.18458165e-01]\n",
      "state\t: [  8.19165469e-01   1.00004412e-15   4.70060038e-02   1.33828527e-01]\n",
      "reading\t: [  9.69309213e-01   6.00297857e-15   1.25142636e-20   3.06907872e-02]\n",
      "check\t: [  1.23000232e-01   7.16249230e-15   2.55472830e-01   6.21526937e-01]\n",
      "key\t: [  9.47826222e-01   8.18896837e-16   1.56673217e-03   5.06070454e-02]\n",
      "laboratory\t: [  9.75628463e-01   5.20544578e-16   1.72112697e-03   2.26504097e-02]\n",
      "30\t: [  9.08430118e-01   2.20860291e-15   5.72971653e-02   3.42727170e-02]\n",
      "1993\t: [  9.16165906e-01   8.35651465e-15   2.08357603e-02   6.29983335e-02]\n",
      "committee\t: [  1.00000000e+00   4.96291383e-17   1.27122572e-20   4.00937332e-10]\n",
      "say\t: [  6.79489191e-01   2.32687657e-14   5.36027750e-02   2.66908034e-01]\n",
      "james\t: [  3.20293152e-01   1.57319465e-14   3.55070918e-19   6.79706848e-01]\n",
      "certainly\t: [  5.04775852e-01   2.95834677e-14   9.72644035e-02   3.97959744e-01]\n",
      "department\t: [  9.59059623e-01   1.59053982e-15   1.15850582e-02   2.93553187e-02]\n",
      "point\t: [  5.31325252e-01   3.08023788e-14   1.65378035e-01   3.03296712e-01]\n",
      "technology\t: [  9.37781903e-01   2.77849317e-15   3.32031292e-03   5.88977844e-02]\n",
      "communication\t: [  7.56443134e-01   1.27049027e-14   1.22506538e-19   2.43556866e-01]\n",
      "drive\t: [  5.23474896e-18   2.92730964e-15   3.56535677e-01   6.43464323e-01]\n",
      "jim\t: [  7.87027423e-01   6.08348722e-15   8.51469538e-02   1.27825623e-01]\n",
      "similar\t: [  9.04545660e-01   1.79471264e-14   1.58477511e-02   7.96065888e-02]\n",
      "example\t: [  5.82794707e-01   1.85400899e-14   2.83007071e-02   3.88904586e-01]\n",
      "po\t: [  8.74126171e-01   7.75182809e-28   1.60458567e-03   1.24269243e-01]\n",
      "college\t: [  8.96138373e-01   2.34622540e-15   2.96016859e-02   7.42599408e-02]\n",
      "25\t: [  8.80426745e-01   6.41300558e-15   4.85040320e-02   7.10692233e-02]\n",
      "michael\t: [  6.17001245e-01   3.33113403e-15   1.05012863e-01   2.77985892e-01]\n",
      "16\t: [  8.08775381e-01   1.33316517e-14   1.10262938e-01   8.09616812e-02]\n",
      "drag\t: [  9.13193737e-01   1.44104166e-27   4.39692379e-03   8.24093390e-02]\n",
      "bitnet\t: [  6.53742470e-01   5.72969934e-15   2.06264283e-02   3.25631101e-01]\n",
      "day\t: [  7.92143137e-01   1.17800197e-14   7.31881965e-02   1.34668667e-01]\n",
      "solar\t: [  9.74776110e-01   3.75112444e-28   9.79670159e-04   2.42442196e-02]\n",
      "23\t: [  9.22681439e-01   4.38443977e-15   5.77768778e-02   1.95416828e-02]\n",
      "thomas\t: [  2.03557011e-01   9.93462387e-15   5.52816465e-02   7.41161342e-01]\n",
      "mail\t: [  4.82978251e-01   8.59498279e-15   1.65917057e-01   3.51104692e-01]\n",
      "physic\t: [  9.67847366e-01   3.70122333e-15   1.70552291e-03   3.04471107e-02]\n",
      "fly\t: [  9.03981296e-01   3.09483892e-16   4.72034459e-02   4.88152586e-02]\n",
      "titan\t: [  9.91933432e-01   3.86643780e-28   1.86348368e-20   8.06656840e-03]\n",
      "mi\t: [  4.49845826e-01   1.64615570e-14   2.28672965e-03   5.47867445e-01]\n",
      "source\t: [  9.23000726e-01   5.93691991e-15   3.34001395e-03   7.36592598e-02]\n",
      "mid\t: [  5.63030092e-01   2.21839615e-27   1.54795738e-01   2.82174170e-01]\n",
      "look\t: [  3.67688119e-01   1.25177858e-14   1.28287750e-01   5.04024131e-01]\n",
      "engineering\t: [  8.95403678e-01   1.09540225e-15   2.35046916e-20   1.04596322e-01]\n",
      "commercial\t: [  9.65542264e-01   4.86800930e-28   8.24547305e-04   3.36331889e-02]\n",
      "phone\t: [  9.24863153e-01   1.07792019e-15   3.98397721e-02   3.52970754e-02]\n",
      "math\t: [  8.82206629e-01   7.07667219e-15   4.12539493e-02   7.65394221e-02]\n",
      "quebec\t: [  6.31884346e-18   4.16707440e-27   8.99333193e-01   1.00666807e-01]\n",
      "response\t: [  9.77493350e-01   4.16090152e-14   1.22301882e-03   2.12836310e-02]\n",
      "email\t: [  9.09856872e-01   4.20013853e-15   1.19733372e-02   7.81697908e-02]\n",
      "forward\t: [  9.12192804e-01   6.53902059e-15   3.28600980e-02   5.49470981e-02]\n",
      "space\t: [  9.44189072e-01   6.46421955e-16   3.43677724e-04   5.54672498e-02]\n",
      "try\t: [  5.77260486e-01   3.66574792e-14   8.76542795e-02   3.35085235e-01]\n",
      "order\t: [  3.72415168e-01   5.93510506e-14   2.12879789e-01   4.14705043e-01]\n",
      "fund\t: [  9.68856491e-01   4.48690738e-28   9.11884325e-04   3.02316249e-02]\n",
      "make\t: [  6.86853404e-01   2.03058487e-14   7.16240156e-02   2.41522581e-01]\n",
      "plan\t: [  9.52510620e-01   2.04294235e-16   4.14957429e-02   5.99363723e-03]\n",
      "position\t: [  7.02977401e-01   1.70131990e-14   7.48056068e-02   2.22216992e-01]\n",
      "possible\t: [  8.66286247e-01   1.03781755e-14   4.65840286e-04   1.33247913e-01]\n",
      "area\t: [  8.75616498e-01   1.74012772e-15   2.91047680e-03   1.21473025e-01]\n",
      "include\t: [  9.08837112e-01   2.46270578e-15   2.99130346e-02   6.12498533e-02]\n",
      "previous\t: [  1.82568331e-01   1.69981713e-15   3.38630459e-02   7.83568623e-01]\n",
      "fuel\t: [  8.37034260e-01   1.24867105e-27   3.09384940e-03   1.59871890e-01]\n",
      "45\t: [  9.43370709e-01   3.21700299e-16   5.07995863e-02   5.82970470e-03]\n",
      "program\t: [  9.62653785e-01   9.17906596e-16   9.86721659e-04   3.63594931e-02]\n",
      "research\t: [  9.49591969e-01   3.49262247e-15   1.25843167e-03   4.91495992e-02]\n",
      "group\t: [  8.93022390e-01   1.43377137e-14   3.43072897e-04   1.06634537e-01]\n",
      "believe\t: [  1.48011472e-01   1.43120182e-13   1.67962706e-01   6.84025823e-01]\n",
      "70\t: [  8.83092360e-01   1.57877741e-15   6.17731957e-02   5.51344446e-02]\n",
      "bob\t: [  3.27578282e-01   3.03664653e-27   3.86448915e-01   2.85972803e-01]\n",
      "maybe\t: [  3.42964376e-01   6.04541832e-14   7.66617770e-02   5.80373847e-01]\n",
      "current\t: [  9.09001996e-01   1.08699796e-14   1.12471686e-02   7.97508358e-02]\n",
      "local\t: [  8.42580443e-01   4.93208713e-16   4.08490058e-02   1.16570552e-01]\n",
      "sky\t: [  9.84872965e-01   6.28173072e-16   1.25940168e-20   1.51270348e-02]\n",
      "new\t: [  8.94717417e-01   8.46560082e-15   1.10491696e-02   9.42334133e-02]\n",
      "ask\t: [  9.03686839e-01   1.65283210e-14   2.34738312e-02   7.28393300e-02]\n",
      "driver\t: [  8.35963557e-01   7.24116611e-28   3.84040980e-20   1.64036443e-01]\n",
      "lead\t: [  2.56097876e-01   6.43931028e-14   5.67225226e-01   1.76676898e-01]\n",
      "term\t: [  9.03278136e-01   9.31642413e-15   3.30800297e-03   9.34138615e-02]\n",
      "data\t: [  9.90140992e-01   6.05429792e-17   1.26410221e-20   9.85900822e-03]\n",
      "information\t: [  8.98843649e-01   4.31238080e-15   2.07261096e-02   8.04302411e-02]\n",
      "service\t: [  8.97639444e-01   5.24341865e-15   4.95099905e-03   9.74095574e-02]\n",
      "student\t: [  9.80889434e-01   7.60122509e-15   2.54112444e-20   1.91105663e-02]\n",
      "publish\t: [  9.69628100e-01   1.82830767e-16   2.43669618e-03   2.79352035e-02]\n",
      "address\t: [  8.89913040e-01   2.50503297e-16   3.49662096e-02   7.51207505e-02]\n",
      "40\t: [  9.18248343e-01   4.62752973e-15   6.96192952e-02   1.21323614e-02]\n",
      "appear\t: [  8.91179754e-17   1.81591688e-12   3.55696976e-18   1.00000000e+00]\n",
      "discussion\t: [  5.86622815e-01   6.69746207e-14   1.83977392e-19   4.13377185e-01]\n",
      "report\t: [  8.66806809e-01   6.20835572e-15   1.74809652e-03   1.31445095e-01]\n",
      "list\t: [  8.12072870e-01   1.23406900e-15   1.06801361e-01   8.11257693e-02]\n",
      "north\t: [  9.34102900e-01   1.07893724e-15   1.43360245e-02   5.15610757e-02]\n",
      "scale\t: [  9.39882588e-01   9.25749263e-15   2.96784846e-20   6.01174119e-02]\n",
      "stop\t: [  7.42966354e-02   7.59736703e-14   4.21406014e-01   5.04297351e-01]\n",
      "20\t: [  9.13665905e-01   1.00564854e-14   6.19561524e-02   2.43779429e-02]\n",
      "500\t: [  6.43728857e-01   1.24800295e-14   2.55403647e-01   1.00867496e-01]\n",
      "high\t: [  8.88396605e-01   2.31396791e-15   2.90959466e-02   8.25074485e-02]\n",
      "model\t: [  2.36654170e-01   3.01270254e-14   2.56078005e-19   7.63345830e-01]\n",
      "small\t: [  9.40065543e-01   1.70506584e-15   1.86693093e-03   5.80675261e-02]\n",
      "paul\t: [  1.79021914e-17   2.67102950e-13   6.16407869e-01   3.83592131e-01]\n",
      "standard\t: [  8.89527406e-01   9.05195013e-15   4.21102689e-20   1.10472594e-01]\n",
      "price\t: [  8.25399131e-01   7.23491949e-28   9.10696743e-04   1.73690172e-01]\n",
      "propulsion\t: [  9.93670513e-01   5.36485516e-28   2.51348916e-20   6.32948746e-03]\n",
      "software\t: [  9.32237552e-01   3.21976250e-16   5.15242144e-04   6.72472059e-02]\n",
      "93\t: [  8.89838187e-01   2.80733314e-17   7.88860769e-02   3.12757359e-02]\n",
      "assist\t: [  8.60503910e-01   3.34542959e-28   1.27943201e-01   1.15528888e-02]\n",
      "contact\t: [  8.60811111e-01   5.87343325e-28   9.60888639e-02   4.31000251e-02]\n",
      "peter\t: [  4.53646393e-18   5.72928695e-14   8.72595949e-01   1.27404051e-01]\n",
      "35\t: [  8.73584427e-01   4.82100160e-15   8.91272237e-02   3.72883496e-02]\n",
      "34\t: [  5.09923779e-01   8.91983517e-15   2.10985022e-01   2.79091198e-01]\n",
      "14\t: [  8.64843547e-01   8.20419802e-15   8.24939330e-02   5.26625202e-02]\n",
      "planetary\t: [  9.95269557e-01   1.96016262e-28   2.71075019e-04   4.45936788e-03]\n",
      "road\t: [  8.52837538e-01   4.19432698e-28   5.72709012e-02   8.98915608e-02]\n",
      "great\t: [  7.07054361e-01   1.87961215e-14   6.55926968e-02   2.27352942e-01]\n",
      "single\t: [  9.25669885e-01   2.48210363e-15   5.24215483e-02   2.19085670e-02]\n",
      "cost\t: [  8.92998027e-01   7.54541643e-28   2.84739556e-03   1.04154578e-01]\n",
      "95\t: [  9.66631481e-01   1.95302300e-28   2.99333958e-02   3.43512354e-03]\n",
      "48\t: [  3.95779319e-18   2.43732440e-14   9.99999996e-01   3.71973084e-09]\n",
      "box\t: [  8.98750486e-01   3.03570648e-15   9.73056063e-02   3.94390774e-03]\n",
      "matter\t: [  5.73360258e-01   6.04499754e-14   2.08237664e-02   4.05815976e-01]\n",
      "tell\t: [  7.69177521e-01   3.76551146e-14   3.60576006e-02   1.94764878e-01]\n",
      "topic\t: [  9.24672259e-01   5.64457587e-15   3.69903328e-20   7.53277409e-02]\n",
      "info\t: [  5.78548882e-01   7.90560672e-15   2.95852595e-02   3.91865859e-01]\n",
      "mar\t: [  9.85757973e-01   1.77728773e-15   1.58848726e-20   1.42420271e-02]\n",
      "pressure\t: [  7.25596521e-01   1.58072495e-15   8.88479426e-02   1.85555537e-01]\n",
      "useful\t: [  4.29920911e-01   5.77518890e-14   2.42757172e-02   5.45803372e-01]\n",
      "robert\t: [  4.35691472e-02   2.78603145e-15   2.85024686e-01   6.71406166e-01]\n",
      "dc\t: [  9.53432662e-01   5.78727039e-28   2.71453098e-20   4.65673380e-02]\n",
      "orbiter\t: [  9.73568715e-01   1.57477198e-28   1.44019629e-20   2.64312846e-02]\n",
      "question\t: [  8.96589653e-01   1.49876985e-14   2.05264338e-02   8.28839129e-02]\n",
      "special\t: [  6.17452886e-01   2.72574225e-14   2.37863023e-01   1.44684091e-01]\n",
      "publication\t: [  9.99141162e-01   2.23567266e-28   8.58837994e-04   2.71473746e-10]\n",
      "league\t: [  1.09666013e-01   2.50044234e-15   6.74297602e-01   2.16036385e-01]\n",
      "gov\t: [  9.68670623e-01   4.86060473e-15   1.60587811e-03   2.97234989e-02]\n",
      "post\t: [  7.71662339e-01   4.99537227e-15   3.92190743e-02   1.89118587e-01]\n",
      "montreal\t: [  1.19868459e-17   1.03541738e-26   9.99999985e-01   1.54890197e-08]\n",
      "net\t: [  5.39851786e-01   2.73007521e-14   1.56614113e-01   3.03534101e-01]\n",
      "like\t: [  6.47325734e-01   1.74004713e-14   5.77622279e-02   2.94912038e-01]\n",
      "process\t: [  9.80925397e-01   1.83308536e-15   1.47559145e-20   1.90746029e-02]\n",
      "require\t: [  9.82078008e-01   2.87175091e-15   1.27407529e-03   1.66479165e-02]\n",
      "10\t: [  8.84868912e-01   8.85999744e-15   6.37568539e-02   5.13742340e-02]\n",
      "want\t: [  7.63768199e-01   1.52194968e-14   8.15651848e-02   1.54666616e-01]\n",
      "best\t: [  1.57254443e-01   3.88545046e-14   3.42655220e-01   5.00090337e-01]\n",
      "air\t: [  8.83426132e-01   7.49637428e-28   6.44262415e-02   5.21476268e-02]\n",
      "consider\t: [  8.59049831e-01   2.08586899e-14   8.04631872e-03   1.32903851e-01]\n",
      "free\t: [  9.62695131e-01   6.60086143e-15   4.50401649e-20   3.73048688e-02]\n",
      "issue\t: [  8.76359938e-01   8.01107474e-15   2.29413488e-02   1.00698713e-01]\n",
      "application\t: [  1.00000000e+00   6.29439791e-15   1.21769501e-20   4.64734508e-10]\n",
      "european\t: [  9.65512777e-01   4.11060479e-28   5.09667909e-03   2.93905435e-02]\n",
      "55\t: [  3.05114481e-18   1.87586141e-27   7.48905757e-01   2.51094243e-01]\n",
      "km\t: [  8.49958531e-01   1.52342500e-27   1.04662485e-03   1.48994844e-01]\n",
      "job\t: [  9.36207335e-01   1.70894387e-15   1.90120381e-03   6.18914613e-02]\n",
      "launch\t: [  9.79644663e-01   7.82520771e-17   1.89501790e-20   2.03553372e-02]\n",
      "old\t: [  7.88073355e-01   1.41523680e-14   5.88523674e-02   1.53074278e-01]\n",
      "rocket\t: [  9.83943756e-01   1.04013072e-16   1.51974247e-20   1.60562441e-02]\n",
      "value\t: [  9.41579514e-01   2.00976913e-14   8.99072149e-03   4.94297642e-02]\n",
      "05\t: [  9.88653539e-01   5.54791493e-15   5.59807924e-03   5.74838164e-03]\n",
      "wrong\t: [  7.36457254e-18   7.28952259e-14   8.06868252e-03   9.91931317e-01]\n",
      "time\t: [  6.80298287e-01   1.94583867e-14   5.59356707e-02   2.63766043e-01]\n",
      "version\t: [  8.77252559e-01   1.93580830e-16   3.30082247e-02   8.97392164e-02]\n",
      "deal\t: [  6.97957344e-01   2.25876765e-14   2.69240394e-02   2.75118617e-01]\n",
      "keywords\t: [  8.51810812e-01   6.78797500e-28   4.51780156e-02   1.03011173e-01]\n",
      "week\t: [  3.34371861e-01   1.39297344e-13   1.31454141e-02   6.52482725e-01]\n",
      "limit\t: [  1.48369838e-01   3.77274578e-14   2.53559701e-01   5.98070461e-01]\n",
      "13\t: [  8.31079646e-01   1.39934915e-14   1.18108193e-01   5.08121606e-02]\n",
      "capability\t: [  9.82775873e-01   4.00873223e-28   1.68973745e-20   1.72241269e-02]\n",
      "schedule\t: [  9.93031638e-01   2.05745492e-28   4.08574735e-03   2.88261513e-03]\n",
      "01\t: [  9.69680594e-01   5.43567744e-15   3.03194051e-02   8.68327615e-10]\n",
      "appropriate\t: [  9.49847444e-01   8.15815173e-15   3.97810866e-20   5.01525557e-02]\n",
      "copy\t: [  9.99999999e-01   4.25847931e-15   2.26935100e-20   6.90973021e-10]\n",
      "change\t: [  7.93435608e-01   5.74562412e-15   3.99193071e-02   1.66645085e-01]\n",
      "development\t: [  9.88546613e-01   2.49915565e-16   1.93614035e-20   1.14533868e-02]\n",
      "profit\t: [  8.86931046e-01   3.40071582e-28   2.11396713e-20   1.13068954e-01]\n",
      "final\t: [  9.32405044e-01   1.44344080e-15   6.55028840e-02   2.09207156e-03]\n",
      "al\t: [  3.14774912e-01   2.87698092e-15   3.79986055e-01   3.05239033e-01]\n",
      "available\t: [  9.30036211e-01   4.00419007e-15   1.36999732e-03   6.85937912e-02]\n",
      "decide\t: [  5.14708387e-01   1.31031460e-14   2.37354328e-01   2.47937285e-01]\n",
      "experience\t: [  6.09254767e-02   1.36795012e-13   3.26094865e-19   9.39074523e-01]\n",
      "liquid\t: [  9.91134728e-01   1.70654984e-16   1.64301699e-20   8.86527238e-03]\n",
      "usa\t: [  2.12121680e-01   1.45780510e-15   7.77568308e-02   7.10121489e-01]\n",
      "year\t: [  7.86110420e-01   4.92024448e-15   6.37572138e-02   1.50132366e-01]\n",
      "people\t: [  7.24052536e-01   2.93636097e-14   8.23120219e-03   2.67716262e-01]\n",
      "early\t: [  6.60397734e-01   2.02612389e-14   1.50086168e-02   3.24593649e-01]\n",
      "lunar\t: [  9.66487042e-01   3.35482982e-28   9.59188370e-04   3.25537693e-02]\n",
      "second\t: [  6.41298676e-01   1.53263242e-14   1.58267075e-01   2.00434249e-01]\n",
      "know\t: [  6.90730823e-01   2.02594206e-14   2.98675745e-02   2.79401602e-01]\n",
      "history\t: [  8.90848321e-01   1.66751964e-14   4.93482358e-02   5.98034434e-02]\n",
      "mission\t: [  9.89177554e-01   1.45883552e-15   2.55325193e-03   8.26919398e-03]\n",
      "san\t: [  8.46086148e-01   2.57644056e-15   5.57220224e-02   9.81918297e-02]\n",
      "international\t: [  9.77982028e-01   2.00757625e-15   1.99554958e-03   2.00224222e-02]\n",
      "11\t: [  8.76700366e-01   1.00852613e-14   8.71208550e-02   3.61787792e-02]\n",
      "need\t: [  7.48550966e-01   1.29473526e-14   4.74289253e-02   2.04020109e-01]\n",
      "coverage\t: [  9.47293565e-01   2.06031278e-16   3.15763300e-02   2.11301045e-02]\n",
      "activity\t: [  9.67381009e-01   2.99154258e-15   1.92610935e-20   3.26189913e-02]\n",
      "louis\t: [  2.71351306e-01   5.67096522e-14   1.47470550e-01   5.81178143e-01]\n",
      "way\t: [  4.45284615e-01   3.64080851e-14   1.16732811e-01   4.37982575e-01]\n",
      "david\t: [  8.09784958e-01   1.67059513e-16   7.12786729e-03   1.83087175e-01]\n",
      "quite\t: [  5.14989602e-02   5.82897648e-14   2.79879986e-02   9.20513041e-01]\n",
      "saturn\t: [  9.18693024e-01   4.21096477e-15   9.17016365e-04   8.03899596e-02]\n",
      "hand\t: [  5.90505841e-01   5.16936238e-14   2.36162439e-01   1.73331721e-01]\n",
      "aerospace\t: [  9.86962130e-01   2.19545471e-28   8.85676098e-21   1.30378696e-02]\n",
      "kind\t: [  6.84054217e-01   2.20381428e-14   2.57892920e-02   2.90156491e-01]\n",
      "lot\t: [  2.95858483e-01   4.42501706e-14   1.02977656e-01   6.01163862e-01]\n",
      "guy\t: [  6.10138822e-18   3.69419950e-27   5.66308715e-01   4.33691285e-01]\n",
      "base\t: [  9.32571320e-01   4.81652402e-15   1.05508783e-02   5.68778017e-02]\n",
      "community\t: [  9.45013266e-01   3.55115839e-15   6.27453281e-03   4.87122016e-02]\n",
      "country\t: [  9.25783514e-01   3.71520586e-15   1.44039371e-02   5.98125491e-02]\n",
      "worth\t: [  5.30833672e-02   1.44220462e-14   2.73501010e-01   6.73415623e-01]\n",
      "90\t: [  7.02561992e-01   1.03267328e-27   1.75279953e-01   1.22158055e-01]\n",
      "start\t: [  8.04186169e-01   6.18949796e-15   6.04860839e-02   1.35327747e-01]\n",
      "________________________________\n",
      "point\t: [  8.80374363e-19   2.53935628e-13   1.37301973e-01   8.62698027e-01]\n",
      "right\t: [  1.96452263e-18   2.39974967e-13   1.48960930e-01   8.51039070e-01]\n",
      "add\t: [  1.49935665e-18   5.32882105e-14   5.62347126e-02   9.43765287e-01]\n",
      "similar\t: [  6.25558355e-18   6.17539449e-13   5.49157598e-02   9.45084240e-01]\n",
      "christ\t: [  1.12602519e-35   8.17363142e-13   2.06034055e-19   1.00000000e+00]\n",
      "return\t: [  1.24681994e-17   3.11764757e-13   3.04737736e-01   6.95262264e-01]\n",
      "cc\t: [  1.36394487e-19   1.59323273e-14   2.46143473e-02   9.75385653e-01]\n",
      "vehicle\t: [  2.32651387e-18   1.68337278e-14   1.80934123e-03   9.98190659e-01]\n",
      "exactly\t: [  3.58443569e-19   2.97093056e-13   9.70004222e-20   1.00000000e+00]\n",
      "think\t: [  5.22240513e-19   1.54083844e-13   5.65819342e-02   9.43418066e-01]\n",
      "happen\t: [  1.38597927e-18   6.30984996e-13   1.62938002e-01   8.37061998e-01]\n",
      "90\t: [  2.36131460e-18   1.72688967e-26   2.95184446e-01   7.04815554e-01]\n",
      "service\t: [  5.28957257e-18   1.53732481e-13   1.46185237e-02   9.85381476e-01]\n",
      "want\t: [  2.49288537e-18   2.47157322e-13   1.33394395e-01   8.66605605e-01]\n",
      "agree\t: [  6.89744495e-36   4.13057047e-13   3.45718500e-02   9.65428150e-01]\n",
      "distribution\t: [  4.02751641e-18   2.34732272e-15   4.65242721e-02   9.53475728e-01]\n",
      "little\t: [  1.86894767e-19   8.31465676e-14   1.86335868e-01   8.13664132e-01]\n",
      "dealer\t: [  3.51587156e-36   1.08974729e-26   6.26534004e-20   1.00000000e+00]\n",
      "start\t: [  3.06217835e-18   1.17263204e-13   1.15404135e-01   8.84595865e-01]\n",
      "pick\t: [  1.31195334e-19   7.92050688e-14   3.64455273e-01   6.35544727e-01]\n",
      "local\t: [  3.81984304e-18   1.11249312e-14   9.27913844e-02   9.07208616e-01]\n",
      "plus\t: [  3.84615185e-18   1.85879000e-15   1.36831387e-01   8.63168613e-01]\n",
      "tell\t: [  2.18260813e-18   5.31625262e-13   5.12670146e-02   9.48732985e-01]\n",
      "number\t: [  2.38055732e-18   1.08311856e-13   1.61708152e-01   8.38291848e-01]\n",
      "experience\t: [  3.77932905e-20   4.22200246e-13   1.01356534e-19   1.00000000e+00]\n",
      "steve\t: [  1.18773474e-19   1.57563199e-13   3.96114615e-01   6.03885385e-01]\n",
      "guy\t: [  5.93372246e-36   1.78752188e-26   2.75958538e-01   7.24041462e-01]\n",
      "hour\t: [  3.48646221e-18   1.02941656e-26   1.97507172e-02   9.80249283e-01]\n",
      "final\t: [  2.56067842e-17   1.97234032e-13   9.01369523e-01   9.86304773e-02]\n",
      "buy\t: [  1.83287582e-20   1.15271400e-14   1.12282095e-02   9.88771790e-01]\n",
      "usa\t: [  1.68618473e-19   5.76569956e-15   3.09706746e-02   9.69029325e-01]\n",
      "price\t: [  2.76401535e-18   1.20543167e-26   1.52806647e-03   9.98471934e-01]\n",
      "check\t: [  1.02932679e-19   2.98224943e-14   1.07123335e-01   8.92876665e-01]\n",
      "ask\t: [  6.60579217e-18   6.01129635e-13   8.59771305e-02   9.14022870e-01]\n",
      "come\t: [  5.73584398e-19   2.09060516e-13   6.79370209e-02   9.32062979e-01]\n",
      "18\t: [  1.08226152e-17   2.74139964e-13   5.12109012e-01   4.87890988e-01]\n",
      "new\t: [  5.34788582e-18   2.51759850e-13   3.30916098e-02   9.66908390e-01]\n",
      "time\t: [  1.41485994e-18   2.01350797e-13   5.82901191e-02   9.41709881e-01]\n",
      "story\t: [  6.84711474e-36   4.83184219e-13   1.56438219e-19   1.00000000e+00]\n",
      "car\t: [  5.67332365e-21   1.53703046e-16   1.78406272e-03   9.98215937e-01]\n",
      "________________________________\n",
      "sun\t: [  8.97311860e-01   6.99593693e-14   1.35158697e-18   1.02688140e-01]\n",
      "man\t: [  7.83826975e-01   6.68680288e-13   2.49164359e-18   2.16173025e-01]\n",
      "world\t: [  9.09392823e-01   2.95827591e-13   2.38502994e-19   9.06071771e-02]\n",
      "problem\t: [  3.25519719e-01   1.17795546e-12   9.51184057e-18   6.74480281e-01]\n",
      "14\t: [  9.60267423e-01   2.35297744e-13   3.62294055e-18   3.97325767e-02]\n",
      "thing\t: [  5.98964773e-01   1.38676655e-12   5.81420499e-18   4.01035227e-01]\n",
      "nntp\t: [  8.65263371e-01   4.08879380e-14   1.98243625e-18   1.34736629e-01]\n",
      "space\t: [  9.61614338e-01   1.70053291e-14   1.38445198e-20   3.83856623e-02]\n",
      "center\t: [  9.57204047e-01   6.33540319e-14   3.63415805e-18   4.27959526e-02]\n",
      "distribution\t: [  9.14320476e-01   2.76648591e-15   8.33745007e-19   8.56795235e-02]\n",
      "send\t: [  9.44111646e-01   2.18079303e-13   2.29675504e-18   5.58883541e-02]\n",
      "night\t: [  7.50311516e-01   4.81981372e-13   1.34989426e-17   2.49688484e-01]\n",
      "launch\t: [  9.86077690e-01   2.03453616e-15   7.54467305e-37   1.39223101e-02]\n",
      "research\t: [  9.66024827e-01   9.17761599e-14   5.06367060e-20   3.39751733e-02]\n",
      "way\t: [  5.99391496e-01   1.26589507e-12   6.21513236e-18   4.00608504e-01]\n",
      "american\t: [  9.41272852e-01   2.37709592e-26   5.39933499e-18   5.87271481e-02]\n",
      "post\t: [  8.57241904e-01   1.43341165e-13   1.72328845e-18   1.42758096e-01]\n",
      "surface\t: [  9.75343155e-01   7.63180039e-14   8.27339533e-37   2.46568451e-02]\n",
      "gov\t: [  9.79575466e-01   1.26963551e-13   6.42331028e-20   2.04245336e-02]\n",
      "nasa\t: [  9.89382859e-01   3.97576565e-15   1.87662238e-20   1.06171406e-02]\n",
      "want\t: [  8.79041881e-01   4.52454875e-13   3.71310621e-18   1.20958119e-01]\n",
      "reply\t: [  4.92474840e-01   6.92837315e-13   1.61400267e-18   5.07525160e-01]\n",
      "________________________________\n"
     ]
    }
   ],
   "source": [
    "#14-24 gives a good mix, but try whatever you like\n",
    "dStart = 14 \n",
    "dEnd = 24 \n",
    "\n",
    "\n",
    "def getWordsFromMatrix(WdTest):\n",
    "    originalWords  = np.array(vectorizer.get_feature_names())[WdTest] \n",
    "    return originalWords\n",
    "\n",
    "for dk in range(dStart,dEnd):\n",
    "    \n",
    "    origWords = getWordsFromMatrix(WdTest[dk])\n",
    "    wordMixtures = [origWords[n] + \"\\t: \" + str(phi[dk][n]) for n in range(len(phi[dk]))]\n",
    "    for wm in set(wordMixtures):\n",
    "        print(wm)\n",
    "    print(\"________________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Bonus Objectives\n",
    "\n",
    "Well done! You have now implemented LDA, approximated the necessary variational parameters, and examined the results to infer information about topics in documents. If you feel like you would like to experiment some more, there some variants that you could try:\n",
    "\n",
    "1. Load the provided dataset from the Associated Press docs dataset. This has random news articles from an undisclosed number of topics. Replace the dataset code in the beginning with what is provided in the next cell and redo your tests. What kind of topics does your result have? How many topics did you assume there were? (Some interesting cases I got were general topics like Crime and Economics and then one focusing solely on foreign affairs with President Bush)\n",
    "\n",
    "2. Run the tests using the MoodyLyrics dataset instead. This dataset includes the lyrics from songs in many different genres (I've included has slightly less than 200 / 50 documents and V=500). Run the tests again and see what kind of sense LDA tries to make out of these song lyrics. The dataset also provides an annotation as to what emotion (\"Angry\", \"Sad\", \"Happy\", \"Relaxed\") the song exhibits. Can you find a resemblence in your topics to these emotions? (<i>Disclaimer: The lyrics provided are not censored and some are not exactly \"PG-13\")</i>\n",
    "\n",
    "<b>It is possible to start to see results after ~50-60 iterations so if you would like to try out these bonus exercises you need not wait overnight</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading the AP docs dataset instead:\n",
    "#(everything else should work like before)\n",
    "vectorizer = pickle.load(open(\"vectorizerAP.p\", \"rb\"), encoding='latin1')\n",
    "trainDocs = pickle.load(open(\"trainDocsAP.p\", \"rb\"), encoding='latin1')\n",
    "testDocs = pickle.load(open(\"testDocsAP.p\", \"rb\"), encoding='latin1')\n",
    "\n",
    "\n",
    "#loading the moodyLyrics dataset instead:\n",
    "vectorizer = pickle.load(open(\"vectorizerMoodyLyrics.p\", \"rb\"), encoding='latin1')\n",
    "trainLyricsFile = pickle.load(open(\"trainDocsMoodyLyrics.p\", \"rb\"), encoding='latin1')\n",
    "testLyricsFile = pickle.load(open(\"testDocsMoodyLyrics.p\", \"rb\"), encoding='latin1')\n",
    "\n",
    "trainDocs = trainLyricsFile['lyrics']\n",
    "testDocs = testLyricsFile['lyrics']\n",
    "#original moods can be seen with: trainGT = trainLyricsFile['groundTruth'] but the labeling is not perfect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      "angry\n",
      "relaxed\n",
      "happy\n",
      "relaxed\n",
      "happy\n",
      "angry\n",
      "sad\n",
      "angry\n",
      "angry\n"
     ]
    }
   ],
   "source": [
    "#loading a larger vocabulary: \n",
    "\n",
    "for dk in range(10,20):\n",
    "    print(testLyricsFile['groundTruth'][dk])\n",
    "\n",
    "#for dk in range(0,len(trainLyrics['lyrics'])):\n",
    "#    print(trainLyrics['groundTruth'][dk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a matter of interest does anyone know why autos are so popular in the US while \n",
      "here in Europe they are rare??? Just wondering.....\n",
      "-- \n",
      "___________________________________________________________________ ____/|\n",
      "John Kissane                           | Motorola Ireland Ltd.,   | \\'o.O'\n",
      "UUCP    : ..uunet!motcid!glas!kissanej | Mahon Industrial Estate, | =() ()=\n",
      "Internet: kissanej@glas.rtsg.mot.com   | Blackrock, Cork, Ireland |    U\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories=['rec.autos', 'soc.religion.christian', 'sci.space', 'rec.sport.hockey']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "print(\"\\n\".join(twenty_train.data[4].split(\"\\n\\n\")[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: revdak@netcom.com (D. Andrew Kille)\n",
      "Subject: Re: Easter: what's in a name? (was Re: New Testament Double Stan\n",
      "Organization: NETCOM On-line Communication Services (408 241-9760 guest)\n",
      "Lines: 40\n",
      "\n",
      "Daniel Segard (dsegard@nyx.cs.du.edu) wrote:\n",
      "\n",
      "[a lot of stuff deleted]\n",
      "\n",
      ":      For that matter, stay Biblical and call it Omar Rasheet (The Feast of\n",
      ": First Fruits).  Torah commands that this be observed on the day following\n",
      ": the Sabbath of Passover week.  (Sunday by any other name in modern\n",
      ": parlance.)  Why is there so much objection to observing the Resurrection\n",
      ": on the 1st day of the week on which it actually occured?  Why jump it all\n",
      ": over the calendar the way Easter does?  Why not just go with the Sunday\n",
      ": following Passover the way the Bible has it?  Why seek after unbiblical\n",
      ": methods?\n",
      ":  \n",
      "In fact, that is the reason Easter \"jumps all over the calendar\"- Passsover\n",
      "itself is a lunar holiday, not a solar one, and thus falls over a wide\n",
      "possible span of times.  The few times that Easter does not fall during or\n",
      "after Passover are because Easter is further linked to the Vernal Equinox-\n",
      "the beginning of spring.\n",
      "\n",
      "[more deletions]\n",
      ":  \n",
      ":       So what does this question have to do with Easter (the whore\n",
      ": goddess)?  I am all for celebrating the Resurrection.  Just keep that\n",
      ": whore out of the discussion.\n",
      ":  \n",
      "Your obsession with the term \"whore\" clouds your argument.  \"Whore\" is\n",
      "a value judgement, not a descriptive term.\n",
      "\n",
      "[more deletions]\n",
      "\n",
      "Overall, this argument is an illustration of the \"etymological fallacy\"\n",
      "(see J.P. Louw: _Semantics of NT Greek_).  That is the idea that the true\n",
      "meaning of a word lies in its origins and linguistic form.  In fact, our\n",
      "own experience demonstrates that the meaning of a word is bound up with\n",
      "how it is _used_, not where it came from.  Very few modern people would\n",
      "make any connection whatsoever between \"Easter\" and \"Ishtar.\"  If Daniel\n",
      "Seagard does, then for him it has that meaning.  But that is a highly\n",
      "idiosyncratic \"meaning,\" and not one that needs much refutation.\n",
      "\n",
      "revdak@netcom.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(\"\\n\".join(twenty_train.data[113].split(\"\\n\")))\n",
    "print(twenty_train.data[68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-15157b01e256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soc.religion.christian\n",
      "rec.sport.hockey\n",
      "rec.autos\n",
      "rec.autos\n",
      "soc.religion.christian\n",
      "rec.autos\n",
      "rec.sport.hockey\n",
      "rec.autos\n",
      "rec.sport.hockey\n",
      "sci.space\n",
      "rec.autos\n",
      "sci.space\n",
      "rec.sport.hockey\n",
      "rec.autos\n",
      "rec.autos\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "rec.sport.hockey\n",
      "rec.autos\n",
      "rec.sport.hockey\n",
      "soc.religion.christian\n",
      "sci.space\n",
      "rec.autos\n",
      "sci.space\n",
      "sci.space\n",
      "rec.sport.hockey\n",
      "soc.religion.christian\n",
      "sci.space\n",
      "sci.space\n",
      "rec.autos\n",
      "soc.religion.christian\n",
      "rec.sport.hockey\n",
      "soc.religion.christian\n",
      "rec.sport.hockey\n",
      "rec.sport.hockey\n",
      "rec.autos\n",
      "sci.space\n",
      "rec.autos\n",
      "sci.space\n",
      "rec.autos\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.space\n",
      "rec.sport.hockey\n",
      "soc.religion.christian\n",
      "rec.sport.hockey\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "rec.autos\n",
      "rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "#subTrainTops = twenty_train.ca\n",
    "for t in twenty_train.target[250:300]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 750)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsTrain = []\n",
    "for newsD in range(200):\n",
    "    newsTrain.append(''.join([i for i in twenty_train.data[newsD] if not i.isdigit()]))\n",
    "  \n",
    "newsTest = []\n",
    "for newsD in range(250,300):\n",
    "    newsTest.append(''.join([i for i in twenty_train.data[newsD] if not i.isdigit()]))\n",
    "#newsTrain = twenty_train.data[:200]\n",
    "#newsTest = twenty_train.data[200:250]\n",
    "count_vect = CountVectorizer(max_df=0.5, min_df=0.01, stop_words='english', max_features=750)\n",
    "X_train_counts = count_vect.fit_transform(newsTrain + newsTest)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(count_vect.get_feature_names())\n",
    "pickle.dump(newsTrain, open(\"newsTrainDocs.p\", \"wb\"))\n",
    "pickle.dump(newsTest, open(\"newsTestDocs.p\", \"wb\"))\n",
    "pickle.dump(count_vect, open(\"newsVectorizer2.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: darling@cellar.org (Thomas Darling)\n",
      "Subject: Re: WHERE ARE THE DOUBTERS NOW?  HMM?\n",
      "Organization: The Cellar BBS and public access system\n",
      "Lines: \n",
      "\n",
      "jason@studsys.mscs.mu.edu (Jason Hanson) writes:\n",
      "\n",
      "> In article <Apr..@ramsey.cs.laurentian.ca> maynard@ramsey.cs.\n",
      "> >\n",
      "> >And after the Leafs make cream cheese of the Philadelphia side tomorrow\n",
      "> >night the Leafs will be without equal.\n",
      "> \n",
      "> Then again, maybe not.\n",
      "\n",
      "To put it mildly.  As I watched the Flyers demolish Toronto last night, -,\n",
      "I realized that no matter how good the Leafs' # line may be, they'll need\n",
      "one or two more decent lines to go far in the playoffs.  And, of course, a\n",
      "healthy Felix Potvin.\n",
      "\n",
      "^~^~^~^~^~^~^~^~^\\\\\\^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^\n",
      "Thomas A. Darling \\\\\\ The Cellar BBS & Public Access System: ..\n",
      "darling@cellar.org \\\\\\ GEnie: T.DARLING \\\\ FactHQ \"Truth Thru Technology\"\n",
      "v~v~v~v~v~v~v~v~v~v~\\\\\\~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((newsTrain[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__', '___', 'able', 'absolute', 'absolutes', 'ac', 'accept', 'access', 'according', 'acs', 'activities', 'acts', 'actually', 'ad', 'added', 'address', 'adirondack', 'aerospace', 'age', 'ago', 'agree', 'ahl', 'air', 'al', 'alaska', 'alchemy', 'alive', 'amateur', 'american', 'andrew', 'angeles', 'answer', 'anybody', 'applications', 'appropriate', 'apr', 'april', 'area', 'areas', 'aren', 'argument', 'ariane', 'ask', 'asked', 'assist', 'astronomy', 'atheist', 'atheists', 'athos', 'atmosphere', 'att', 'au', 'australia', 'authority', 'auto', 'automotive', 'autos', 'available', 'away', 'baalke', 'bad', 'baltimore', 'base', 'based', 'belief', 'beliefs', 'believe', 'believing', 'best', 'better', 'bible', 'big', 'biggest', 'billion', 'bit', 'bitnet', 'black', 'blue', 'bob', 'bobby', 'body', 'book', 'books', 'boston', 'box', 'boyle', 'brian', 'brown', 'budget', 'buffalo', 'built', 'buy', 'ca', 'calgary', 'called', 'callison', 'came', 'canada', 'car', 'cars', 'case', 'cb', 'cc', 'center', 'central', 'certainly', 'champs', 'chance', 'change', 'check', 'chicago', 'christ', 'christian', 'christianity', 'christians', 'church', 'city', 'claim', 'clarkson', 'clear', 'clh', 'club', 'cmu', 'coach', 'college', 'colorado', 'columbia', 'com', 'come', 'comes', 'commercial', 'committee', 'communications', 'community', 'computer', 'concept', 'conference', 'consider', 'contact', 'control', 'convertible', 'corporation', 'correct', 'cost', 'costs', 'couple', 'course', 'coverage', 'craig', 'created', 'cs', 'cso', 'cup', 'current', 'cwru', 'dallas', 'dan', 'data', 'dave', 'david', 'day', 'days', 'dc', 'deal', 'dealer', 'death', 'decided', 'defense', 'defensive', 'degrees', 'deleted', 'department', 'dept', 'described', 'design', 'details', 'detroit', 'development', 'did', 'didn', 'die', 'difference', 'different', 'digex', 'disciples', 'discuss', 'discussion', 'discussions', 'distribution', 'division', 'does', 'doesn', 'doing', 'don', 'doubt', 'doug', 'draft', 'drag', 'dseg', 'early', 'earth', 'ecn', 'effort', 'email', 'end', 'energy', 'eng', 'engine', 'engineering', 'entire', 'eos', 'eric', 'especially', 'eternal', 'european', 'event', 'evidence', 'evil', 'exactly', 'example', 'excellent', 'exist', 'existence', 'exists', 'experience', 'exploration', 'fact', 'faith', 'false', 'family', 'fans', 'faq', 'far', 'fast', 'father', 'fax', 'feel', 'fi', 'final', 'flight', 'flyers', 'fnal', 'follow', 'following', 'forget', 'forward', 'foundation', 'free', 'freenet', 'friend', 'fuel', 'funding', 'future', 'game', 'games', 'gary', 'gas', 'gave', 'gene', 'general', 'generally', 'geneva', 'georgia', 'gerald', 'gets', 'getting', 'gibbons', 'gilmour', 'given', 'giving', 'gm', 'goal', 'goalie', 'goals', 'god', 'going', 'good', 'got', 'gov', 'gravity', 'great', 'group', 'groups', 'guess', 'guy', 'half', 'hand', 'happened', 'hard', 'having', 'head', 'heard', 'heaven', 'hell', 'help', 'henry', 'hi', 'high', 'history', 'hit', 'hockey', 'holy', 'home', 'hope', 'host', 'human', 'ice', 'idea', 'ideas', 'images', 'important', 'include', 'including', 'indiana', 'inflatable', 'info', 'information', 'instead', 'institute', 'interested', 'international', 'internet', 'interpretation', 'involved', 'isn', 'issue', 'isu', 'james', 'jason', 'jeff', 'jesus', 'jets', 'jewish', 'jews', 'jim', 'joe', 'john', 'johnson', 'joseph', 'jpl', 'jr', 'jupiter', 'just', 'key', 'keywords', 'killing', 'kind', 'km', 'knew', 'know', 'knowledge', 'known', 'knows', 'laboratory', 'language', 'large', 'later', 'launch', 'law', 'leafs', 'league', 'leave', 'left', 'lemieux', 'let', 'life', 'light', 'like', 'likely', 'lindros', 'line', 'liquid', 'list', 'little', 'live', 'lived', 'living', 'll', 'local', 'long', 'look', 'looking', 'looks', 'lord', 'los', 'lot', 'louis', 'love', 'low', 'lunar', 'mail', 'mailing', 'major', 'make', 'makes', 'making', 'man', 'mark', 'mass', 'math', 'matter', 'matthew', 'maybe', 'mcgill', 'mean', 'means', 'member', 'members', 'men', 'mentioned', 'message', 'mi', 'michael', 'mid', 'mike', 'mind', 'minnesota', 'mission', 'missions', 'model', 'models', 'moncton', 'money', 'montreal', 'moon', 'mot', 'murray', 'nasa', 'national', 'nature', 'nd', 'near', 'necessarily', 'necessary', 'need', 'needs', 'net', 'network', 'new', 'news', 'newsgroup', 'nhl', 'night', 'nntp', 'non', 'north', 'note', 'nuclear', 'number', 'ny', 'oakland', 'offensive', 'office', 'oh', 'ohio', 'oil', 'ok', 'oklahoma', 'old', 'open', 'opinion', 'opinions', 'option', 'orbit', 'orbiter', 'order', 'org', 'original', 'outside', 'particular', 'parts', 'past', 'pat', 'paul', 'peace', 'people', 'performance', 'period', 'person', 'peter', 'philadelphia', 'phoenix', 'phone', 'physics', 'pittsburgh', 'place', 'planetary', 'play', 'played', 'player', 'players', 'playoffs', 'plus', 'po', 'point', 'points', 'position', 'possible', 'post', 'posting', 'power', 'pp', 'prb', 'president', 'pressure', 'previous', 'price', 'pro', 'probably', 'probe', 'problem', 'problems', 'process', 'profit', 'program', 'programs', 'project', 'prophecy', 'propulsion', 'provide', 'pts', 'public', 'puck', 'purpose', 'quebec', 'question', 'questions', 'quick', 'quite', 'radio', 'ramsey', 'rangers', 'ray', 'rd', 'read', 'reading', 'real', 'reality', 'really', 'reason', 'rec', 'record', 'red', 'redesign', 'reference', 'references', 'regular', 'religion', 'religions', 'religious', 'remember', 'remote', 'reply', 'request', 'research', 'reserve', 'result', 'rick', 'right', 'road', 'robert', 'rochester', 'rocket', 'rockets', 'romans', 'ron', 'running', 'russian', 'rutgers', 'said', 'salvation', 'san', 'satellite', 'satellites', 'saturn', 'save', 'saved', 'saw', 'say', 'saying', 'says', 'scale', 'school', 'sci', 'science', 'score', 'scoring', 'scott', 'scripture', 'season', 'second', 'seen', 'send', 'sender', 'sense', 'seriously', 'set', 'shall', 'short', 'shuttle', 'similar', 'simply', 'sin', 'single', 'sky', 'small', 'smith', 'society', 'software', 'solar', 'son', 'sort', 'sounds', 'source', 'south', 'space', 'spacecraft', 'speak', 'special', 'speed', 'spencer', 'spirit', 'spiritual', 'st', 'stage', 'standard', 'star', 'stars', 'start', 'state', 'statement', 'station', 'steve', 'stop', 'strong', 'student', 'studies', 'study', 'stupid', 'summary', 'sun', 'sunday', 'support', 'sure', 'surface', 'systems', 'taken', 'taking', 'talk', 'talking', 'talks', 'teachings', 'team', 'teams', 'technical', 'technology', 'tell', 'term', 'terms', 'testament', 'th', 'thanks', 'thing', 'things', 'think', 'thomas', 'thought', 'ti', 'tie', 'time', 'times', 'titan', 'tm', 'today', 'told', 'tom', 'tommy', 'took', 'toronto', 'total', 'trade', 'tried', 'trouble', 'true', 'truth', 'try', 'trying', 'turn', 'tv', 'tx', 'type', 'types', 'ucs', 'uiuc', 'uk', 'unc', 'understand', 'understanding', 'universe', 'university', 'unix', 'unless', 'uokmax', 'uoknor', 'upenn', 'usa', 'use', 'used', 'useful', 'using', 'usually', 'utoronto', 'uucp', 'uxa', 'various', 've', 'vehicle', 'version', 'view', 'vs', 'want', 'warning', 'washington', 'wasn', 'watch', 'way', 'week', 'went', 'western', 'white', 'wife', 'win', 'wings', 'winnipeg', 'won', 'word', 'words', 'work', 'works', 'world', 'worth', 'wouldn', 'wrong', 'wrote', 'yamauchi', 'year', 'years', 'yes', 'york', 'young']\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5,  1.5,  1.5,  1.5])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((4))*3 - np.ones((4))*1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darling cellar thomas darling doubter hmm organization cellar bb public access system 18 jason studsys msc mu jason hanson write article 1993apr4 051942 27095 ramsey c laurentian ca maynard ramsey c leaf make cream cheese philadelphia side tomorrow night leaf without equal maybe put mildly watch flyer demolish toronto last night realize matter good leaf line may need one two decent go far playoff course healthy felix potvin thomas darling cellar bb public access system 215 539 3043 darling cellar genie darling facthq truth thru technology\n"
     ]
    }
   ],
   "source": [
    "pp = NLTKPreprocessor()\n",
    "trainDocsFiles = twenty_train.data[:200]\n",
    "ppDoc = []\n",
    "trainDocs = []\n",
    "for dd in range(len(trainDocsFiles)):\n",
    "    derp = pp.tokenize(trainDocsFiles[dd])\n",
    "    ppDoc = []\n",
    "    for wor in derp:\n",
    "        ppDoc.append(wor)\n",
    "    trainDocs.append(\" \".join(ppDoc))\n",
    "print(trainDocs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(trainDocsFiles, open(\"trainDocsNews2Orig.p\", \"wb\"))\n",
    "pickle.dump(testDocsFiles, open(\"testDocsNews2Orig.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rick_granberry pt mot rick granberry help reply rick_granberry pt mot rick granberry organization motorola paging telepoint system group 46 article apr 21 03 26 51 1993 1379 geneva rutgers lmvec westminster ac uk william hargreaves write hi everyone commited christian battle problem know romans talk save faith deed yet hebrew james say faith without deed useless say fool still think believing enough someone fully believe life totally lead god accord roman person still save faith 02 yes believe scenario possible either believe living least part lead god else believing intellectually wait enough especially important remember one judge whether committed judge someone else guess close come know someone situation listen statement fallible sense communion one another bit say god prefer someone cold know condemn lukewarm christian someone know believe god make attempt live bible regard passage need remember letter church laodicea people body christ rev 14 16 talk work translation could say say lack concern make sick point throw opinion save faith alone taught roman square mind teaching james conjunction lukewarm christian spat right save faith alone except faith come alone catch two meaning offer explanation jesus would either fire cold know thus could make aware separation admonishment child eternal damnation answer fool accord folly l thou also like unto answer fool accord folly l wise conceit proverbs 26\n"
     ]
    }
   ],
   "source": [
    "testDocsFiles = twenty_train.data[250:300]\n",
    "ppDoc = []\n",
    "testDocs = []\n",
    "for dd in range(len(testDocsFiles)):\n",
    "    derp = pp.tokenize(testDocsFiles[dd])\n",
    "    ppDoc = []\n",
    "    for wor in derp:\n",
    "        ppDoc.append(wor)\n",
    "    testDocs.append(\" \".join(ppDoc))\n",
    "print(testDocs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle as pickle\n",
    "count_vect = CountVectorizer(max_df=0.5, min_df=0.01, stop_words='english', max_features=750)\n",
    "X_train_counts = count_vect.fit_transform(trainDocs + testDocs)\n",
    "X_train_counts.shape\n",
    "\n",
    "\n",
    "pickle.dump(trainDocs, open(\"trainDocsNews2.p\", \"wb\"))\n",
    "pickle.dump(testDocs, open(\"testDocsNews2.p\", \"wb\"))\n",
    "pickle.dump(count_vect, open(\"vecNews2.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                #no 1 letter words\n",
    "                if len(token)<2:\n",
    "                    continue\n",
    "                    \n",
    "                if token=='org' or token=='edu' or token=='lines' or token=='university' or token=='subject' or token=='posted' or token=='hosted' or token=='host' or token=='com':\n",
    "                    continue\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ipykernel_py3]",
   "language": "python",
   "name": "conda-env-ipykernel_py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
