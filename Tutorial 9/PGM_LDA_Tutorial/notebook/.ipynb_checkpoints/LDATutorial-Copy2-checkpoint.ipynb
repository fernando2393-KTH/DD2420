{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Allocation in Song Lyrics Using LDA\n",
    "\n",
    "The purpose of this tutorial is to give you a basic understanding of Latent Dirichlet Allocation (LDA) from http://ai.stanford.edu/~ang/papers/jair03-lda.pdf and use it to implement a simplified part of the work done by Cai, Rui, et al. ACM, 2007. (http://dl.acm.org/citation.cfm?id=1291369). \n",
    "\n",
    "Section 1 will give some background to the mechanics and theory behind LDA. Section 2 will then tackle the task of implementing LDA to infer emotions in songs based on their lyrics. This section will provide you with skeleton code already written in Python 3 using the numpy, scipy, and scikit-learn libraries. \n",
    "\n",
    "If you do not have jupyter notebook installed then you probably aren't reading this, but see http://jupyter.readthedocs.io/en/latest/install.html\n",
    "\n",
    "If you do not have a python 3 kernel installed for jupyter notebook see https://ipython.readthedocs.io/en/latest/install/kernel_install.html or https://stackoverflow.com/questions/28831854/how-do-i-add-python3-kernel-to-jupyter-ipython\n",
    "\n",
    "If you do not have some of the libraries installed for your python 3 kernel, use the \"Kernel -> Conda packages\" dropdown menu in Jupyter if you used anaconda for your python 3 kernel, if not use the normal pip install commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Theory\n",
    "\n",
    "### Background and terminology\n",
    "\n",
    "Since we will be working in the setting of text corpora, we should clarify some of the terminology used in this setting:\n",
    "<ul>\n",
    "<li>A word is the basic unit of discrete data, defined to be an item from a vocabulary indexed by {1, . . . ,V}. \n",
    "<li> A document is a sequence of <i>N</i> words denoted by <b>w</b>=(<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>, . . . ,<i>w</i><sub>N</sub>), where <i>w</i><sub>n</sub> is the <i>n</i>th word in the sequence.</li>\n",
    "<li>A corpus is a collection of <i>M</i> documents denoted by $D$ ={<b>w</b><sub>1</sub>,<b>w</b><sub>2</sub>, . . . ,<b>w</b><sub>M</sub>}</li>\n",
    "</ul>\n",
    "\n",
    "It is important to note that LDA works in other domains besides text collections, but this is the setting in which we will use it.\n",
    "\n",
    "LDA is a generative probabilistic model that is used for modeling collections of discrete data. In our application we will be using it to model text corpora, or more specifically song lyrics. The purpose of the model is to give us compact representations of the data in these collections, allowing us to process large collections while still retaining sufficient information to be able to perform for example classification and relevance measures. \n",
    "\n",
    "There have been several solutions for this type of information retrieval problem, such as the tf-idf (term frequency - inverse document frequency) scheme by Salton and McGill, 1983. This approach produces a term-by-document matrix X whose columns contain the tf-idf values for each of the documents in the corpus. This representation however did not provide significantly shortened representation of the corpora, or represent the inter- or intra- document statistics in a intuitive way. A step forward from this was given by LSI (latent semantic indexing) where singular value decomposition was used on the matrix X to offer a more compact representation. The authors of the method also argued that since the LSI features are linnear combinations of the basic tf-idf features, they incorporate some linguistical notions such as synonomy and polysemy.\n",
    "The first step to providing a generative model was the <i>probabilistic</i> LSI (pLSI), which uses mixture models to model each word in a document. The mixture components are the \"topics\" and represented as multinomial random variables, allowing different words in the document to be genereated by different topics. The compact representation for each document is then the list of numbers representing the mixing proportions for the fixed set of topics. The method however gives no generative probabilistic model for getting these numbers, causing the number of parameters in the model to grow linearly with the corpus size. Also, since there is no probabilistic model for the mixture components that represent a document, there is no clear way of assigning a probability to a document that is outside the training set.\n",
    "\n",
    "Both LSI and pLSI use the \"bag-of-words\" approach which assumes exchangeability within the words of the document as well as the documents themselves, meaning their order is of no importance. A theorem due to de Finetti (1990) states that any collection of exchangeable random variables has a representation as a mixture distribution—in general an infinite mixture. This means we must consider mixture models that capture the exchangeability of both documents and words if we wish to achieve exchangeable representations for them. It is this line of thinking that leads to LDA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Theory Behind LDA\n",
    "\n",
    "As mentioned earlier, LDA is a generative probabilistic model for a corpus. It can be seen as a hierarchical Bayesian model with three levels: each document in a corpus is modeled as a finite random mixture over a latent set of topics, and each of these topics are characterized by a distribution of words. A graphical model for LDA using plate notation can be seen below:\n",
    "![title](imgs/LDAPlateGM.png)\n",
    "From here we can see the three levels of the model. $\\alpha$ and $\\beta$ and corpus level parameters, $\\theta$ is a document level parameter for the M documents in the corpus, and $z$ and $w$ are word level parameters for the N words in a document.\n",
    "\n",
    "The generative process according to LDA for each document <b>w</b> is then:\n",
    "<ol>\n",
    "<li>Choose N ∼Poisson(ξ)</li>\n",
    "<li>Choose $\\theta$∼Dir($\\alpha$)</li>\n",
    "<li>For each of the N words w<sub>n</sub>:\n",
    "<ol type=\"a\">\n",
    "    <li>Choose a topic z<sub>n</sub> ∼Multinomial($\\theta$).</li>\n",
    "    <li>Choose a word w<sub>n</sub> from p(w<sub>n</sub> |z<sub>n</sub>,$\\beta$), a multinomial probability conditioned on the topic z<sub>n</sub>.</li>\n",
    "    </ol></li>\n",
    "</ol>\n",
    "\n",
    "There are however some simplifications to these steps that we will utilize. First, we assume that the dimensionality of the Dirichlet distribution, and therefore the dimensionality for the topic variable $z$ is known and fixed, meaning we assume a fixed known number of topics, $k$. Furthermore, the probabilities for words ($w$) are parameterized by a $k \\times V$ matrix $\\beta$ which defines $p(w^j = 1| z^i = 1) = \\beta_{i,j}$, that we will estimate later and keep fixed. We also note that $N$ is independant of the other data generating variables $\\theta$ and <b>z</b> so we will ignore the Poisson assumption and set it to a known fixed value (the length of the document).  \n",
    "\n",
    "#### Dirichlet Distribution in LDA\n",
    "\n",
    "The probability distribution for a $k$-dimensional Dirichlet random variable $\\theta$ is defined as follows: \n",
    "\n",
    "<b>Eq. 1:</b>\n",
    "![Eq 1](imgs/LDAEq1.png \"Eq 1\")\n",
    "\n",
    "\n",
    "where $\\alpha$ is $k$-dimensional with all elements larger than 0 and $\\Gamma(x)$ is the Gamma function. The Dirichlet distribution has some advantegous advantageous qualities; it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution. These properties help us in running variational inference for the parameters later.\n",
    "\n",
    "We can now express the joint distribution of a topic mixture $\\theta$, a set of $N$ topics <b>z</b>, and\n",
    "a set of $N$ words <b>w</b> given the corpus level parameters $\\alpha,\\beta$ as:\n",
    "\n",
    "<b>Eq. 2:</b>\n",
    "![Eq. 2](imgs/LDAEq2.png)\n",
    "where the probability $p(z_n |\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z^i_n=1$. We can then obtain the marginal distribution over a document by integrating over $\\theta$ and summing over $z$:\n",
    "\n",
    "<b>Eq. 3:</b>\n",
    "![Eq. 3](imgs/LDAEq3.png)\n",
    "\n",
    "\n",
    "#### Comparison to other Latent Variable Models\n",
    "In order to get feeling for how LDA works and what highlights its strengths, it can be helpful to relate it to other related models:\n",
    "\n",
    "  a) Unigram Model\n",
    "\n",
    "  b) Mixture of Unigrams Model\n",
    "\n",
    "  c) pLSI Model\n",
    "  \n",
    "\n",
    "\n",
    "We will begin by examing the absolute simplest model, the unigram model: \n",
    "\n",
    "![Eq. 3](imgs/UniGramMdl.png)\n",
    "\n",
    "This method has no latent variables and instead states that each word in a document is independantly drawn from a single multinomial distribution as seen here:\n",
    "\n",
    "![Eq. 3](imgs/UniGramEq.png)\n",
    "\n",
    "\n",
    "A slighly more complex model is the mixture of unigrams:\n",
    "\n",
    "![Eq. 3](imgs/MixUniGramMdl.png)\n",
    "\n",
    "This model incorporates a discrete latent topic variable, $z$. Here, each document <b>w</b> is generated by first sampling the topic variable $z$, and then generating all words from a conditional probability on that choice:\n",
    "\n",
    "![Eq. 3](imgs/MixUniGramEq.png)\n",
    "\n",
    "This effectively limits the modeling of words in a document to only being representative of one topic. The LDA model on the other hand allows for documents to exhibit multiple topics with different mixtures.\n",
    "\n",
    "Finally we have the pLSI model which we mentioned earlier. It was a relatively popular model around the time that LDA was proposed, and is the model with highest generative capabilities of these three mentioned. \n",
    "\n",
    "![Eq. 3](imgs/PISLMdl.png)\n",
    "\n",
    "pLSI proposes that each word is conditionally independant a \"document label\", $d$, given an unobserved topic $z$:\n",
    "\n",
    "![Eq. 3](imgs/PISLEq.png)\n",
    "\n",
    "This proposal aims to soften the constraint of having each document modeled as being generated from only one topic, as it is in the mixture of unigrams approach. It does so by incorporating the probability, $p(z | d)$ for a certain document $d$ as the mixture of topics that document. A true generative model cannot be created for this mixture however; as d is only a dummy index to the documents pISL was trained with, meaning it is a multinomial random variable with the same amount of possible values as training documents. This leads to the method only learning the topic mixtures, $p(z | d)$, for documents it has already seen \n",
    "<span style=\"color:red\">and there is no natural way to assign probability to an unseen document with it.\n",
    "Another problem is that to model $k$ topics with pLSI you need K multinomial distributions with vocabulary size $V$ and $M$ mixtures over the hidden topics $k$ for each training document, resulting in $kV + kM$ parameters.\n",
    "\n",
    "LDA however treats the topic mixtures as a $k$-parameter hidden variable, meaning the amount of parameters does not scale with the number training documents, and the generative model can still be used even with unseen documents.\n",
    "\n",
    "We can see these differences geometrically as well if we examine the distribution over words as a $V$-1 dimensional on a vocabulary of size $V$ with another $k$-1 dimensional simplex spanning $k$ topics. We can set $V$ and $k$ to 3 for simplicity (3 words gives a two-dimensional triangle):\n",
    "\n",
    "![title](imgs/UnigramSampling.png)\n",
    "\n",
    "Each of these methods provide a different distribution over this simplex. How this distribution is spread out and how it uses the topics distribution differs among the methods. The mixture of unigrams method pics a random point on the word simplex that corresponds to one of the topic simplex vertices k, and draws all the words for a document from the distribution corresponding to that point. pLSI assumes that all words in training documents belong to a single randomly chosen topic. The topics are drawn from a document-specific distribution, meaning each document has a topic distribution that sits on the topic simplex. The training documents then give an empirical distribution over the topic simplex. LDA instead models that <b>each word</b> in a document is drawn from a randomly chosen topic that is sampled from a distribution governed by a random parameter. Since this parameter is sampled once per document, it gives a smooth probability distribution over the topic simplex. \n",
    "</span>\n",
    "\n",
    "Now that we know how LDA compares with other methods, lets take a look at how to do inference in LDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and Parameter Estimation with LDA\n",
    "\n",
    "The main inference problem we will be interested in solving is the posterior distribution of the latent variables given a document, which would allow us to infer the topics associated with the document. This is given by the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "p( \\theta, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta) = \\frac{p( \\theta, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)}{p(\\mathbf{w} \\mid \\alpha, \\beta)} \n",
    "\\end{equation}\n",
    "\n",
    "However, to compute the normalizing denominator we would rewrite equation 3 using equation 1 and $p(z_n \\mid\\theta)=\\theta_i$ and then integrate, resulting in:\n",
    "\n",
    "\\begin{equation}\n",
    "p( \\mathbf{w} \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\sum_i\\alpha_i)}{\\prod_i\\Gamma(\\alpha_i)}\\int\\Bigg(\\prod_{i=1}^k\\theta_i^{\\alpha_i-1}\\Bigg)\\Bigg(\\prod_{n=1}^N\\sum_{i=1}^k\\prod_{j=1}^V(\\theta_i\\beta_{ij})^{w_n^j}\\Bigg)d\\theta\n",
    "\\end{equation}\n",
    "\n",
    "Unfortunately, this expression is intractable due to the coupling of $\\theta$ and $\\beta$ in the summation over topics. We can however solve this problem approximately using variational inference methods. \n",
    "\n",
    "#### Variational Inference for LDA\n",
    "\n",
    "It is possible to use several VI methods for LDA, including La Place approximation, variational approximation, and MCMC methods. In our case, we will be using the convexity-based variational approximation <span style=\"color:red\">that was mentioned in  Olga's tutorial.</span> From there we learned that in this VI we attempt to reformulate/simplify the original graphical model by removing some dependencies and introducing free variational parameters. This leads to a family of distributions dependant on these variational parameters which form a lower bound on the log likelyhood. We then aim to find the parameter values that give the tightest lower bound. \n",
    "\n",
    "In our case, the problematic dependancy is between $\\theta$ and $\\beta$ which is introduced by the edges between $\\theta, \\mathbf{z}$ and $\\mathbf{w}$ (remember w is a 'collision' node and is observed). If we simplify our model by removing these edges along with the <b>w</b> node, and introduce two variational parameters $\\gamma$ and $\\phi$ which give a family of distributions over the remaining latent variables, we are left with the graphical model shown on the right in the figure below:\n",
    "\n",
    "![LDA VI](imgs/LDAVIGM.png \"GM for the VI used for our LDA\")\n",
    "\n",
    "This results in the following distribution over the latent variables:\n",
    "\n",
    "\\begin{equation}\n",
    "p( \\theta, \\mathbf{z} \\mid \\gamma, \\phi) = q(\\theta\\mid\\gamma)\\prod_{n=1}^Nq(z_n\\mid\\phi_n)\n",
    "\\end{equation}\n",
    "\n",
    "where the new Dirichlet parameters $\\gamma$ and the multinomial parameters $\\phi$ are the free variational parameters. Now having simplified our graphical model, we need to find the optimal values for the variational parameters ($\\gamma^*, \\phi^*$). <span style=\"color:red\">From Olga's tutorial we know that this is equivalent to finding the values which minimize the KL divergence between the simplified distribution and the true posterior distribution:</span>\n",
    "\n",
    "\\begin{equation}\n",
    "( \\gamma^*, \\phi^*) = \\arg\\!\\min_{(\\gamma,\\phi)}D\\big(q( \\theta, \\mathbf{z} \\mid \\gamma, \\phi) \\mid\\mid p( \\theta, \\mathbf{z} \\mid \\mathbf{w},\\alpha, \\beta\\big)\n",
    "\\end{equation}\n",
    "\n",
    "<span style=\"color:red\">As discussed in the VI tutorial</span>, by setting the derivatives of the KL divergence to zero w.r.t $\\gamma$ and $\\phi$ we get the following update equations for the parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_{ni} \\propto \\beta_{iw_n}\\mathrm{exp}\\big\\lbrace\\mathrm{E}_q\\big[log(\\theta_i)\\mid\\gamma\\big]\\big\\rbrace = \\beta_{iw_n}\\mathrm{exp}\\big\\lbrace\\Psi(\\gamma_i)\\big\\rbrace\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\gamma_i = \\alpha_i + \\sum_{n=1}^N\\phi_{ni}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Psi$ is the digamma function (first derivative of log $\\Gamma$). It is important to note that these update equations derived from the KL divergence are dependant on a certain choice of <b>w</b>. This means that the shown approximation for the variational parameters is only valid for one set of words, and must therefore be calculated for each document when we use them later on. \n",
    "\n",
    "We must also find a way of estimating the $\\beta$ matrix, as it is used in the approximations for the variational parameters. The log likelihood of the data given $\\beta$ and $\\alpha$ is intractable as we saw at the end of the previous section. However, it is possible to implement a variational EM procedure that gives us an approximation of the best value for $\\beta$ by first maximizing a lower bound for the optimal variational parameters $\\gamma^*,\\phi^*$, then maximizing the lower bound w.r.t $\\beta$ with the previously acquired variational parameters. Essentially we will iterate the following steps until a sufficient level of convergence:\n",
    "<ol>\n",
    "<li>(E-step) Find the optimizing values of the variational parameters {$\\gamma^∗\n",
    "_d,\\phi^∗_d : d\\in D$},for each document as described earlier.</li>\n",
    "<li>(M-step) Maximize the resulting lower bound on the log likelihood w.r.t $\\beta$ using:\n",
    "\\begin{equation}\n",
    "\\beta_{ij}\\propto\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\phi^*_{dni}w^j_{dn}\n",
    "\\end{equation}\n",
    "as well as maximize the resulting lower bound on the log likelihood w.r.t $\\alpha$ (this will be given to you).\n",
    "</ol>\n",
    "\n",
    "In laymans terms, what we are essentially doing in the E-step is finding out \"How prevalent are topics in the document across its words?\". In the M-step we then ask \"How prevalent are specific words across topics?\". By using the answer from one question as a starting point for the other, we iteratively gain the answer to both. \n",
    "\n",
    "<span style=\"color:red\">For proof for the update equations, see appendix of http://ai.stanford.edu/~ang/papers/jair03-lda.pdf</span>. This appendix also includes the derivation of the Newton-Raphson based method for updating $\\alpha$.  \n",
    "\n",
    "We have now seen the basic intuition behind LDA, and gone through methods for running inference based on the LDA model. In the next section we will put this knowledge in to practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Implementation\n",
    "\n",
    "The goal of this task is to use LDA to create topics for song lyrics based on emotions, and infer the topic (emotion) that new lyrics would belong to. In this setting, our document corpus is the MoodyLyrics dataset of song lyrics, a document is a certain song lyric, and a topic is an emotion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "First we will load the dataset we will use for training and testing. We will simplify the approach of the original paper by only focusing on the four emotions available in the MoodyLyrics dataset; relaxed, happy, sad and Angry. I have already preprocessed the song lyrics, as well as built the vocabulary dataset both as pickle files. Due to the computational complexity of LDA, the resulting vocabulary is notibly reduced for the purposes of this exercise.  Run the code in the cell below and double check that you get the output \"found 191 training and 48 test lyrics, with a vocabulary of 750 words\". Do not worry if you get a warning regarding the version of CountVectorizer which is used for handling the vocabulary of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  191  and  48  test lyrics, with a vocabulary of  750  words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manttari/anaconda2/envs/ipykernel_py3/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.18.1 when using version 0.18.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle as pickle\n",
    "\n",
    "#load the pre-configured vocabulary handler and lyrics\n",
    "vectorizer = pickle.load(open(\"vectorizerSuperSmall3.p\", \"rb\"), encoding='latin1')\n",
    "trainLyricsFile = pickle.load(open(\"trainLyricsSuperSmall2.p\", \"rb\"), encoding='latin1')\n",
    "testLyricsFile = pickle.load(open(\"testLyricsSuperSmall2.p\", \"rb\"), encoding='latin1')\n",
    "\n",
    "trainLyrics = trainLyricsFile['lyrics']\n",
    "testLyrics = testLyricsFile['lyrics']\n",
    "\n",
    "print(\"Found \", len(trainLyrics), \" and \", len(testLyrics), \" test lyrics, with a vocabulary of \", len(vectorizer.get_feature_names()), \" words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  200 training and  50  test lyrics, with a vocabulary of  750  words.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle as pickle\n",
    "\n",
    "#vectorizer = pickle.load(open(\"vectorizerAPSuperSmall2.p\", \"rb\"), encoding='latin1')\n",
    "#trainLyrics = pickle.load(open(\"trainApDocsSuperSmall2.p\", \"rb\"), encoding='latin1')\n",
    "#testLyrics = pickle.load(open(\"testApDocsSuperSmall2.p\", \"rb\"), encoding='latin1')\n",
    "vectorizer = pickle.load(open(\"newsVectorizer2.p\", \"rb\"))\n",
    "trainLyrics = pickle.load(open(\"newsTrainDocs.p\", \"rb\"))\n",
    "testLyrics = pickle.load(open(\"newsTestDocs.p\", \"rb\"))\n",
    "\n",
    "print(\"Found \", len(trainLyrics), \"training and \", len(testLyrics), \" test lyrics, with a vocabulary of \", len(vectorizer.get_feature_names()), \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation\n",
    "\n",
    "We must now find the optimal values for the variational parameters, as well as the values for $\\alpha$ and the $\\beta$ matrix that were introduced in the variational inference section. In order to follow the instructions given in the VI section we will need to do some setup first, so run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import scipy.special as special\n",
    "import scipy.optimize \n",
    "import time\n",
    "\n",
    "#diGamma func from scipy, use this in your code!\n",
    "diGamma = special.digamma\n",
    "\n",
    "#Function definitions for maximizing the VI parameters. This will later be completed by you.\n",
    "def maxVIParam(phi, gamma, B, alpha, M, k, Wd, eta):\n",
    "    \n",
    "    for d in range(M):\n",
    "        N = len(Wd[d])\n",
    "        #Initialization of vars, as shown in E-step. \n",
    "        phi[d] = np.ones((N,k))*1.0/k\n",
    "        gamma[d] = np.ones(k)*(N/k) + alpha\n",
    "        converged = False\n",
    "\n",
    "        #-----Complete the method here by implementing the pseudo code for the E-Step----\n",
    "        while(not converged):\n",
    "            for n in range(N):\n",
    "                for i in range(k):\n",
    "                    #print(\"old,\", phi[n,i])\n",
    "                    phi[d][n,i] = B[i,Wd[d][n]]*np.exp(diGamma(gamma[d][i]))\n",
    "                    #print(\"new:\", phi[n,i])\n",
    "                if(np.sum(phi[d][n])==0):\n",
    "                    print(\"crap, phi sum is: \", np.sum(phi[d][n]), \"for doc \",d)\n",
    "                    phi[d][n] = np.zeros(k)\n",
    "                else:\n",
    "                    phi[d][n] = phi[d][n]/np.sum(phi[d][n])\n",
    "\n",
    "            updateError = 0\n",
    "            for i in range(k):\n",
    "                gammaOld = gamma[d][i]\n",
    "                #print(\"old:,\", gammaOld)\n",
    "                gamma[d][i] = alpha[i] + np.sum(phi[d][:,i])\n",
    "                #print(\"new: \", gamma[d][i], \"der: \",np.exp(diGamma(gammaOld)))\n",
    "                updateError += abs(gammaOld - gamma[d][i]) \n",
    "                #print(\"u e: \", updateError)\n",
    "            if(updateError < eta):\n",
    "                converged = True\n",
    "    \n",
    "    return gamma, phi\n",
    "\n",
    "#Function definitions for maximizing the B parameter. This will later be completed by you.\n",
    "def MaxB(B, phi, k, V, M, Wd):\n",
    "    \n",
    "    #YOUR CODE FOR THE M-STEP HERE\n",
    "    for i in range(k):\n",
    "        for j in range(V):\n",
    "            B[i,j] = 0\n",
    "            for d in range(M):    \n",
    "\n",
    "                if(j in Wd[d]):\n",
    "                    Wj = 1\n",
    "                else:\n",
    "                    Wj = 0\n",
    "\n",
    "                for n in range(len(phi[d])):\n",
    "                    B[i,j] += phi[d][n,i]*Wj\n",
    "    return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Here are the functions needed for updating the alpha parameter, as shown in the start of appendix A.4.2.\n",
    "These are all provided for you as it is just plugging in the definition for the gradient and hessian into the \n",
    "Newton-Raphson based method to find a stationary point using SciPy. Feel free to take a look at the appendix to\n",
    "see where these values come from.'''\n",
    "\n",
    "#value of Likelihood(gamma,phi,alpha,beta) function w.r.t. alpha terms (see start of appendix A.4.2) \n",
    "def L_alpha_val(a):\n",
    "    val = 0\n",
    "    M = len(gamma)\n",
    "    k = len(a)\n",
    "    for d in range(M):\n",
    "        val += (np.log(scipy.special.gamma(np.sum(a))) - np.sum([np.log(scipy.special.gamma(a[i])) for i in range(k)]) + np.sum([((a[i] -1)*(diGamma(gamma[d][i]) - diGamma(np.sum(gamma[d])))) for i in range(k)]))\n",
    "\n",
    "    return -val\n",
    "\n",
    "#value of the derivative of above func w.r.t. alpha_i (2nd eq of appendix A.4.2) \n",
    "def L_alpha_der(a):\n",
    "    M = len(gamma)\n",
    "    k = len(a)\n",
    "    der = np.array(\n",
    "    [(M*(diGamma(np.sum(a)) - diGamma(a[i])) + np.sum([diGamma(gamma[d][i]) - diGamma(np.sum(gamma[d])) for d in range(M)])) for i in range(k)]\n",
    "    )\n",
    "    return -der\n",
    "\n",
    "def L_alpha_hess(a):\n",
    "    hess = np.zeros((len(a),len(a)))\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(a)):\n",
    "            k_delta = 1 if i == j else 0\n",
    "            hess[i,j] = k_delta*M*scipy.special.polygamma(1,a[i]) - scipy.special.polygamma(1,np.sum(a))\n",
    "    return -hess\n",
    "\n",
    "def MaxA(a):\n",
    "    res = scipy.optimize.minimize(L_alpha_val, a, method='Newton-CG',\n",
    "        jac=L_alpha_der, hess=L_alpha_hess,\n",
    "        options={'xtol': 1e-8, 'disp': False})\n",
    "    print(res.x)\n",
    "    \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, we can now initialize the required parameters and define the skeleton of our loop for the parameter estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on iter:  0\n",
      "[ 1.81106657  1.49653745  2.04145055  1.73366009]\n",
      "iter took:  76.02988290786743\n",
      "new alpha:  [ 1.81106657  1.49653745  2.04145055  1.73366009]\n",
      "on iter:  1\n",
      "[ 1.24366948  1.32166426  1.766854    1.42392032]\n",
      "iter took:  95.628098487854\n",
      "new alpha:  [ 1.24366948  1.32166426  1.766854    1.42392032]\n",
      "on iter:  2\n",
      "[ 0.57798235  0.8112265   0.89213983  0.68236058]\n",
      "iter took:  59.33216595649719\n",
      "new alpha:  [ 0.57798235  0.8112265   0.89213983  0.68236058]\n",
      "on iter:  3\n",
      "[ 0.29631869  0.51814511  0.44083318  0.35016575]\n",
      "iter took:  67.64705634117126\n",
      "new alpha:  [ 0.29631869  0.51814511  0.44083318  0.35016575]\n",
      "on iter:  4\n",
      "[ 0.16886734  0.35228358  0.24864678  0.19643493]\n",
      "iter took:  54.40777087211609\n",
      "new alpha:  [ 0.16886734  0.35228358  0.24864678  0.19643493]\n",
      "on iter:  5\n",
      "[ 0.11211254  0.2522805   0.15874707  0.12789902]\n",
      "iter took:  45.72015881538391\n",
      "new alpha:  [ 0.11211254  0.2522805   0.15874707  0.12789902]\n",
      "on iter:  6\n",
      "[ 0.08365279  0.19082223  0.11657595  0.09496649]\n",
      "iter took:  42.25055384635925\n",
      "new alpha:  [ 0.08365279  0.19082223  0.11657595  0.09496649]\n",
      "on iter:  7\n",
      "[ 0.06723838  0.1532557   0.09231145  0.07594297]\n",
      "iter took:  40.883816719055176\n",
      "new alpha:  [ 0.06723838  0.1532557   0.09231145  0.07594297]\n",
      "on iter:  8\n",
      "[ 0.05656609  0.1322063   0.07789942  0.0641197 ]\n",
      "iter took:  41.161500692367554\n",
      "new alpha:  [ 0.05656609  0.1322063   0.07789942  0.0641197 ]\n",
      "on iter:  9\n",
      "[ 0.04914827  0.11887212  0.06885838  0.05621813]\n",
      "iter took:  40.58208417892456\n",
      "new alpha:  [ 0.04914827  0.11887212  0.06885838  0.05621813]\n",
      "on iter:  10\n",
      "[ 0.04390426  0.10895275  0.06248665  0.0506163 ]\n",
      "iter took:  39.39052939414978\n",
      "new alpha:  [ 0.04390426  0.10895275  0.06248665  0.0506163 ]\n",
      "on iter:  11\n",
      "[ 0.04025392  0.10198402  0.05783375  0.04650736]\n",
      "iter took:  38.62171792984009\n",
      "new alpha:  [ 0.04025392  0.10198402  0.05783375  0.04650736]\n",
      "on iter:  12\n",
      "[ 0.03746449  0.09680834  0.05432975  0.04340378]\n",
      "iter took:  38.5281400680542\n",
      "new alpha:  [ 0.03746449  0.09680834  0.05432975  0.04340378]\n",
      "on iter:  13\n",
      "[ 0.03528631  0.09281688  0.05162577  0.0410042 ]\n",
      "iter took:  38.55471658706665\n",
      "new alpha:  [ 0.03528631  0.09281688  0.05162577  0.0410042 ]\n",
      "on iter:  14\n",
      "[ 0.03355602  0.08965753  0.04949768  0.03911373]\n",
      "iter took:  39.28280067443848\n",
      "new alpha:  [ 0.03355602  0.08965753  0.04949768  0.03911373]\n",
      "on iter:  15\n",
      "[ 0.03216226  0.08711008  0.04779573  0.03760147]\n",
      "iter took:  39.32506608963013\n",
      "new alpha:  [ 0.03216226  0.08711008  0.04779573  0.03760147]\n",
      "on iter:  16\n",
      "[ 0.03102664  0.08502791  0.04641645  0.03637651]\n",
      "iter took:  39.0546190738678\n",
      "new alpha:  [ 0.03102664  0.08502791  0.04641645  0.03637651]\n",
      "on iter:  17\n",
      "[ 0.03009251  0.08330852  0.04528607  0.03537396]\n",
      "iter took:  40.05936598777771\n",
      "new alpha:  [ 0.03009251  0.08330852  0.04528607  0.03537396]\n",
      "on iter:  18\n",
      "[ 0.02916385  0.08183976  0.04433777  0.03453903]\n",
      "iter took:  40.625025033950806\n",
      "new alpha:  [ 0.02916385  0.08183976  0.04433777  0.03453903]\n",
      "on iter:  19\n",
      "[ 0.0283789   0.07994198  0.04324559  0.03380545]\n",
      "iter took:  39.44118022918701\n",
      "new alpha:  [ 0.0283789   0.07994198  0.04324559  0.03380545]\n",
      "on iter:  20\n",
      "[ 0.02771597  0.07848941  0.04235067  0.0331687 ]\n",
      "iter took:  39.78821635246277\n",
      "new alpha:  [ 0.02771597  0.07848941  0.04235067  0.0331687 ]\n",
      "on iter:  21\n",
      "[ 0.02715512  0.07733086  0.04161279  0.03261928]\n",
      "iter took:  38.0749146938324\n",
      "new alpha:  [ 0.02715512  0.07733086  0.04161279  0.03261928]\n",
      "on iter:  22\n",
      "[ 0.02667943  0.07638139  0.04100014  0.03214646]\n",
      "iter took:  38.59035634994507\n",
      "new alpha:  [ 0.02667943  0.07638139  0.04100014  0.03214646]\n",
      "on iter:  23\n",
      "[ 0.02627486  0.0755894   0.04048802  0.03173994]\n",
      "iter took:  39.0961012840271\n",
      "new alpha:  [ 0.02627486  0.0755894   0.04048802  0.03173994]\n",
      "on iter:  24\n",
      "[ 0.02592984  0.07492118  0.04005731  0.03139039]\n",
      "iter took:  38.65637707710266\n",
      "new alpha:  [ 0.02592984  0.07492118  0.04005731  0.03139039]\n",
      "on iter:  25\n",
      "[ 0.02563487  0.07435311  0.03969313  0.03108964]\n",
      "iter took:  38.58091449737549\n",
      "new alpha:  [ 0.02563487  0.07435311  0.03969313  0.03108964]\n",
      "on iter:  26\n",
      "[ 0.0253821   0.07386767  0.03938377  0.03083067]\n",
      "iter took:  38.82390308380127\n",
      "new alpha:  [ 0.0253821   0.07386767  0.03938377  0.03083067]\n",
      "on iter:  27\n",
      "[ 0.02516506  0.07345128  0.03911995  0.03060746]\n",
      "iter took:  38.74169206619263\n",
      "new alpha:  [ 0.02516506  0.07345128  0.03911995  0.03060746]\n",
      "on iter:  28\n",
      "[ 0.02497835  0.07309315  0.03889421  0.03041489]\n",
      "iter took:  38.65331554412842\n",
      "new alpha:  [ 0.02497835  0.07309315  0.03889421  0.03041489]\n",
      "on iter:  29\n",
      "[ 0.02481748  0.07278446  0.03870052  0.03024859]\n",
      "iter took:  39.64489984512329\n",
      "new alpha:  [ 0.02481748  0.07278446  0.03870052  0.03024859]\n",
      "on iter:  30\n",
      "[ 0.02467868  0.07251793  0.03853394  0.03010484]\n",
      "iter took:  39.53165817260742\n",
      "new alpha:  [ 0.02467868  0.07251793  0.03853394  0.03010484]\n",
      "on iter:  31\n",
      "[ 0.02455878  0.07228747  0.03839039  0.0299805 ]\n",
      "iter took:  39.50899386405945\n",
      "new alpha:  [ 0.02455878  0.07228747  0.03839039  0.0299805 ]\n",
      "on iter:  32\n",
      "[ 0.02445508  0.07208803  0.03826649  0.02987284]\n",
      "iter took:  38.32273983955383\n",
      "new alpha:  [ 0.02445508  0.07208803  0.03826649  0.02987284]\n",
      "on iter:  33\n",
      "[ 0.02436531  0.07191518  0.03815939  0.02977958]\n",
      "iter took:  40.05676817893982\n",
      "new alpha:  [ 0.02436531  0.07191518  0.03815939  0.02977958]\n",
      "on iter:  34\n",
      "[ 0.02428754  0.07176524  0.0380667   0.02969872]\n",
      "iter took:  38.393627405166626\n",
      "new alpha:  [ 0.02428754  0.07176524  0.0380667   0.02969872]\n",
      "on iter:  35\n",
      "[ 0.02422011  0.07163507  0.0379864   0.02962858]\n",
      "iter took:  38.56832981109619\n",
      "new alpha:  [ 0.02422011  0.07163507  0.0379864   0.02962858]\n",
      "on iter:  36\n",
      "[ 0.02416161  0.07152195  0.03791677  0.0295677 ]\n",
      "iter took:  38.538785457611084\n",
      "new alpha:  [ 0.02416161  0.07152195  0.03791677  0.0295677 ]\n",
      "on iter:  37\n",
      "[ 0.02411083  0.07142355  0.03785634  0.02951483]\n",
      "iter took:  39.144387006759644\n",
      "new alpha:  [ 0.02411083  0.07142355  0.03785634  0.02951483]\n",
      "on iter:  38\n",
      "[ 0.02406671  0.07133779  0.03780385  0.02946889]\n",
      "iter took:  39.504741191864014\n",
      "new alpha:  [ 0.02406671  0.07133779  0.03780385  0.02946889]\n",
      "on iter:  39\n",
      "[ 0.02402837  0.07126272  0.03775821  0.02942894]\n",
      "iter took:  39.49947237968445\n",
      "new alpha:  [ 0.02402837  0.07126272  0.03775821  0.02942894]\n",
      "on iter:  40\n",
      "[ 0.02398263  0.07070681  0.03768761  0.02937539]\n",
      "iter took:  40.12379336357117\n",
      "new alpha:  [ 0.02398263  0.07070681  0.03768761  0.02937539]\n",
      "on iter:  41\n",
      "[ 0.02393614  0.07037974  0.03761297  0.0293196 ]\n",
      "iter took:  38.981799602508545\n",
      "new alpha:  [ 0.02393614  0.07037974  0.03761297  0.0293196 ]\n",
      "on iter:  42\n",
      "[ 0.02389194  0.07017122  0.03754283  0.02926647]\n",
      "iter took:  38.54072165489197\n",
      "new alpha:  [ 0.02389194  0.07017122  0.03754283  0.02926647]\n",
      "on iter:  43\n",
      "[ 0.0238513   0.07002713  0.03748008  0.02921798]\n",
      "iter took:  38.410593032836914\n",
      "new alpha:  [ 0.0238513   0.07002713  0.03748008  0.02921798]\n",
      "on iter:  44\n",
      "[ 0.02381461  0.06992029  0.03742522  0.02917468]\n",
      "iter took:  38.03009080886841\n",
      "new alpha:  [ 0.02381461  0.06992029  0.03742522  0.02917468]\n",
      "on iter:  45\n",
      "[ 0.02378184  0.06983651  0.03737776  0.02913646]\n",
      "iter took:  39.17038345336914\n",
      "new alpha:  [ 0.02378184  0.06983651  0.03737776  0.02913646]\n",
      "on iter:  46\n",
      "[ 0.02375276  0.06976822  0.03733686  0.02910295]\n",
      "iter took:  39.5168023109436\n",
      "new alpha:  [ 0.02375276  0.06976822  0.03733686  0.02910295]\n",
      "on iter:  47\n",
      "[ 0.02372708  0.06971114  0.03730165  0.02907368]\n",
      "iter took:  38.544798612594604\n",
      "new alpha:  [ 0.02372708  0.06971114  0.03730165  0.02907368]\n",
      "on iter:  48\n",
      "[ 0.02370445  0.06966266  0.03727132  0.02904816]\n",
      "iter took:  39.26296639442444\n",
      "new alpha:  [ 0.02370445  0.06966266  0.03727132  0.02904816]\n",
      "on iter:  49\n",
      "[ 0.02368457  0.06962109  0.03724516  0.02902593]\n",
      "iter took:  38.952823638916016\n",
      "new alpha:  [ 0.02368457  0.06962109  0.03724516  0.02902593]\n",
      "on iter:  50\n",
      "[ 0.02366712  0.06958523  0.03722257  0.02900659]\n",
      "iter took:  39.940380811691284\n",
      "new alpha:  [ 0.02366712  0.06958523  0.03722257  0.02900659]\n",
      "on iter:  51\n",
      "[ 0.02365182  0.06955419  0.03720304  0.02898977]\n",
      "iter took:  39.588597536087036\n",
      "new alpha:  [ 0.02365182  0.06955419  0.03720304  0.02898977]\n",
      "on iter:  52\n",
      "[ 0.02363842  0.0695273   0.03718612  0.02897513]\n",
      "iter took:  38.64675450325012\n",
      "new alpha:  [ 0.02363842  0.0695273   0.03718612  0.02897513]\n",
      "on iter:  53\n",
      "[ 0.02362669  0.06950392  0.03717146  0.02896239]\n",
      "iter took: "
     ]
    }
   ],
   "source": [
    "eta = 10e-5 #threshold for convergence\n",
    "\n",
    "#hyperparamater init.\n",
    "V = len(vectorizer.get_feature_names()) #vocab. cardinality\n",
    "M = int(len(trainLyrics)) #number of documents\n",
    "k = 4 #amount of emotions\n",
    "\n",
    "nIter=100 # number of iterations to run until parameter estimation is considered converged\n",
    "\n",
    "#initialize B matrix as random valid distr (most common according to https://profs.sci.univr.it/~bicego/papers/2015_SIMBAD.pdf)\n",
    "B = np.random.rand(k,V)\n",
    "Bderp=np.random.rand(nIter,k,V)\n",
    "\n",
    "#normalize B\n",
    "for i in range(k):\n",
    "    B[i,:] = B[i]/np.sum(B[i])\n",
    "    \n",
    "alpha = np.ones(k)\n",
    "#variational params (one for each doc)\n",
    "phi = [None]*M\n",
    "gamma = [None]*M\n",
    "Wd = [None]*M\n",
    "\n",
    "'''Since scikit gives a matrix of counts of all words, and we want a list of words,\n",
    "we do some quick transformations here. This gives us a representation of the song lyrics\n",
    "as a list of numbers, where each number is the vocabulary index of a word. This way, to access\n",
    "B_ij where i is the ith topic and j is the nth word in the document d, you can simply write B[i][W[d][n]]. '''\n",
    "\n",
    "for d in range(M):\n",
    "    Wmat = vectorizer.transform([trainLyrics[d]]).toarray()[0] #get vocabulary matrix for document\n",
    "    WVidxs = np.where(Wmat!=0)[0]\n",
    "    WVcounts = Wmat[WVidxs]\n",
    "    N = np.sum(WVcounts)\n",
    "    W = np.zeros((N)).astype(int)\n",
    "\n",
    "    i = 0\n",
    "    for WVidx, WV in enumerate(WVidxs):\n",
    "        for wordCount in range(WVcounts[WVidx]):\n",
    "            W[i] = WV\n",
    "            i+=1\n",
    "    Wd[d] = W #We save the list of words for the document for analysis later\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#start of parameter estimation loop\n",
    "for j in range(nIter):\n",
    "    print(\"on iter: \", j)\n",
    "    #Variational EM for gamma and phi (E-step from VI section)\n",
    "    start = time.time()\n",
    "    gamma, phi = maxVIParam(phi, gamma, B, alpha, M, k, Wd, eta)\n",
    "    end = time.time()\n",
    "    #print(\" VI took: \", end-start)\n",
    "    Bold = B\n",
    "    B = MaxB(B,phi,k,V,M,Wd) #first half of M-step from VI section \n",
    "    #print(\"B max diff: \", np.amax(abs(B-Bold)))\n",
    "    #renormalize B\n",
    "    for i in range(k):\n",
    "        B[i,:] = B[i]/np.sum(B[i])\n",
    "    \n",
    "    Bderp[j]=B\n",
    "    end = time.time()\n",
    "    #print(\"B took: \", end-start)\n",
    "    \n",
    "    alpha = MaxA(alpha) #second half of M-step from VI section \n",
    "    end = time.time()\n",
    "    print(\"iter took: \", end-start)\n",
    "    #print(\"B col 1 new: \", B[:,0:3])\n",
    "    print(\"new alpha: \", alpha)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"took: \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI Parameter Estimation\n",
    "We can now begin with implementing the \"E step\" in the previous section which updates the variational parameters. The pseudo code for this is the following (remember these have to be calculated separately for each document):\n",
    "![VIPseudo](imgs/VIPseudo.png)\n",
    "\n",
    "Since we are working with four topics (emotions), k will be set to 4 and N will be the amount of words in the current document. Regarding the \"until convergence\" condition, it is sufficient to check if the largest difference between the previous and new gamma is less than $10^{-5}$. Now, use the pseudo code to fill in the missing code in the \"MaxVIParam\" function defined earlier and remember to use the provided diGamma function. To see that your implementation seems to be working, set nIter to 2 and the optional param debug to true, and check that the output conveys that your phi values are being updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beta Matrix Estimation\n",
    "After you have implemented the MaxVIParam function, it's time to update the Beta matrix. Recall that the update function for Beta was:\n",
    "\\begin{equation}\n",
    "\\beta_{ij}\\propto\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\phi^*_{dni}w^j_{dn}\n",
    "\\end{equation}\n",
    "Implement this in the definition for MaxB above. To verify your code, you may set nIter to something low such as 5 and just verify that the largest update to an element in B is decreasing. After you have verified this, set nIter to around 300-500 and let it run overnight as it might take a couple hours. You can use the code in the following cell to save the parameter values you calculated for later so you don't have to re-run everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(alpha, open(\"myAlphaAP.p\", \"wb\"))\n",
    "pickle.dump(B, open(\"myBetaAP.p\", \"wb\"))\n",
    "pickle.dump(Bderp, open(\"myBetaDAP.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification\n",
    "Let's take a look at what we've done so far. We can get an idea of what our implementation has done up to this point by inspecting the B matrix. As you may remember, B$_{ij}$ holds the probability of a vocabulary word j being representative of a certain topic i. Using the code in the following cell we can see the most representative words for our 4 topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for topic  0 : \n",
      "['edmonton' 'played' 'team' 'ca' 'toronto' 'university' 'games' 'st' 'new'\n",
      " 'nhl']\n",
      "top words for topic  1 : \n",
      "['christ' 'time' 'does' 'don' 'like' 'think' 'just' 'say' 'people' 'god']\n",
      "top words for topic  2 : \n",
      "['just' 'play' 'ca' 'hockey' 'team' 'don' 'article' 'game' 'university'\n",
      " 'writes']\n",
      "top words for topic  3 : \n",
      "['think' 'university' 'just' 'com' 'like' 'nntp' 'host' 'posting' 'writes'\n",
      " 'article']\n"
     ]
    }
   ],
   "source": [
    "#representation of top words for each topic:\n",
    "\n",
    "nTop = 10\n",
    "for i in range(k):\n",
    "    topVocabs = np.argsort(B[i])[-nTop:]\n",
    "    topWords = np.array(vectorizer.get_feature_names())[topVocabs]\n",
    "    print(\"top words for topic \",i,\": \")\n",
    "    print(topWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no guarantees regarding the order of the topics or what your initial B matrix values were, it is difficult to say exactly what results you should be seeing. The topics generated by LDA are also not supervised in any way, meaning you will have to decide which topic seems to represent the moods happy, sad, angry and relaxed the best according to your own opinion. For example, in my results I could see one topic including the words \"burn\", \"pain\" and \"fight\" in one topic, making it a clear candidate for the \"angry\" emotion. Another topic had high ranking words such as \"good\", \"home\", \"heart\", and \"right\" which I would say corresponds to a happy or relaxed topic. You can also load the $\\alpha$ and $\\beta$ values in the cell below which are pre-calculated for 300 iterations and compare your topic results to those available there. Hopefully you have similar words in the same topic, even if the topic numbers do not coincide.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for topic  0 : \n",
      "['let' 'morning' 'till' 'thing' 'wanna' 'day' 'repeat' 'tell' 'need'\n",
      " 'change' 'think' 'people' 'make' 'night' 'everybody' 'right' 'turn'\n",
      " 'tonight' 'good' 'ain']\n",
      "top words for topic  1 : \n",
      "['sad' 'heart' 'life' 'think' 'really' 'won' 'wanna' 'day' 'home' 'try'\n",
      " 'world' 'long' 'need' 'little' 'let' 'feel' 'night' 'hey' 'say' 'right']\n",
      "top words for topic  2 : \n",
      "['face' 'hold' 'somebody' 'burn' 'think' 'day' 'won' 'things' 'good'\n",
      " 'heart' 'need' 'eyes' 'pain' 'make' 'makes' 'feel' 'let' 'right' 'life'\n",
      " 'say']\n",
      "top words for topic  3 : \n",
      "['look' 'gotta' 'day' 'things' 'little' 'heart' 'night' 'man' 'right'\n",
      " 'tell' 'gonna' 'good' 'life' 'world' 'ooh' 'girl' 'ain' 'need' 'say'\n",
      " 'feel']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manttari/anaconda2/envs/ipykernel_py3/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.18.1 when using version 0.18.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "alphaTest = pickle.load(open(\"500itsLyricsAlpha.p\", \"rb\"))\n",
    "BTest = pickle.load(open(\"500itsLyricsB.p\", \"rb\"))\n",
    "vecTest = pickle.load(open(\"vectorizerSuperSmall.p\", \"rb\"), encoding='latin1')\n",
    "nTop = 20\n",
    "for i in range(k):\n",
    "    topVocabs = np.argsort(BTest[i])[-nTop:]\n",
    "    topWords = np.array(vecTest.get_feature_names())[topVocabs]\n",
    "    print(\"top words for topic \",i,\": \")\n",
    "    print(topWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the emotion (topic) of a test document\n",
    "\n",
    "In this section we will be using our estimated parameter values to infer the emotion (topic) of some test lyrics. In order to do this, we will have to calculate the phi and gamma values for each new document we would like to do inference on. This is rather straight forward, and you should be able to reuse your code from the previous sections together with the test documents as lyrics instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  7\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  8\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n",
      "crap, phi sum is:  0.0 for doc  29\n"
     ]
    }
   ],
   "source": [
    "eta = 10e-5 #threshold for convergence\n",
    "\n",
    "#we are not re-initializing beta and alpha, we calculated them using the training docs.\n",
    "\n",
    "V = len(vectorizer.get_feature_names()) #vocab. cardinality\n",
    "M = int(len(testLyrics)) #number of documents\n",
    "k = 4 #amount of emotions\n",
    "\n",
    "#variational params (one for each doc)\n",
    "phi = [None]*M\n",
    "gamma = [None]*M\n",
    "WdTest = [None]*M\n",
    "\n",
    "'''Since scikit gives a matrix of counts of all words, and we want a list of words,\n",
    "we do some quick transformations here. This gives us a representation of the song lyrics\n",
    "as a list of numbers, where each number is the vocabulary index of a word. This way, to access\n",
    "B_ij where i is the ith topic and j is the nth word in the document d, you can simply write B[i][W[d][n]]. '''\n",
    "\n",
    "for d in range(M):\n",
    "    Wmat = vectorizer.transform([testLyrics[d]]).toarray()[0] #get vocabulary matrix for document\n",
    "    WVidxs = np.where(Wmat!=0)[0]\n",
    "    WVcounts = Wmat[WVidxs]\n",
    "    N = np.sum(WVcounts)\n",
    "    W = np.zeros((N)).astype(int)\n",
    "\n",
    "    i = 0\n",
    "    for WVidx, WV in enumerate(WVidxs):\n",
    "        for wordCount in range(WVcounts[WVidx]):\n",
    "            W[i] = WV\n",
    "            i+=1\n",
    "    WdTest[d] = W #We save the list of words for the document for analysis later\n",
    "\n",
    "'''Now that you have your variables initialized for the test documents, you should be able to use your function for \n",
    "maximizing the VI parameters with those variables instead. Remember, we're just calculating the variational parameters\n",
    "gamma and phi for each test document so there is no iteration between maximizing Beta and maximizing gamma and phi.'''\n",
    "\n",
    "#Run the gamma/phi maximization here.\n",
    "gamma, phi = maxVIParam(phi, gamma, B, alpha, M, k, WdTest, eta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now calculated the variational parameters for our test documents, so let us see what information we can infer from them. If you take a look at the pseudo code we used for the MaxVIParam method, you can see that the posterior gamma parameter $\\gamma_i $we are calculating is approximately the prior Dichlet parameter $\\alpha_i$ added to the expected number of words that were generated by that $i^{th}$ topic for a certain document. Looking at the values for the different $\\gamma_i$ over all words for a test document tells us what mixture of topics form such a document. Let us now take a look at the mixtures for some of our test documents by running the code in the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated mixture for document  40  is: \n",
      "_______________________\n",
      "topic  0 :  0.012528008964\n",
      "topic  1 :  0.0226782202354\n",
      "topic  2 :  0.0157206975471\n",
      "topic  3 :  62.0492330255\n",
      "_______________________\n",
      "For the following lyrics:\n",
      " \n",
      "From: hhm@cbnewsd.cb.att.com (herschel.h.mayo)\n",
      "Subject: Re: BRAINDEAD Drivers Who Don't Look Ahead--\n",
      "Organization: Chicago Home for the Morally Challenged\n",
      "Distribution: usa\n",
      "Keywords: bad drivers\n",
      "Lines: \n",
      "\n",
      "\n",
      "> I agree that if traffic is all blocked up and you want to pass, you might\n",
      "> not feel like moving over for someone behind you because you don't want to\n",
      "> give them that one car-length, when they should just wait like you are.\n",
      "> BUT, if you're one of those people that just sit's behind the person, and\n",
      "> doesn't flash them with the high beams, or pull left and flash them, or\n",
      "> ride their bumper, or otherwise tell them that you *do* in fact want to \n",
      "> go by, and you're not just drafting them, then get the hell out of the \n",
      "> way of someone who will!  I especially hate it when you flash someone at\n",
      "> the back of a line and they don't 'pass it on'.  \n",
      "> And there's also the issue of some cars being more intimidating to get \n",
      "> people out of the way than others...  (For instance '-' GTs look \n",
      "> pretty mean in a rearview mirror at night with the foglights on...  :^)\n",
      "> There have been plenty of times when I've broken up a pack that a \n",
      "> second-in-line hyundai has been behind for miles...  You just need to\n",
      "> know how to get their attention...\n",
      "\n",
      "\n",
      "I'd like to see you use this method on a couple of semi drivers. If they see you,\n",
      "they usually acknowledge by sticking their hand out the window with their middle\n",
      "finger extended. Because it is also obvious to them that there is no clear lane\n",
      "ahead.  \n",
      "\n",
      "\n",
      "\n",
      "                                                      H.H.M.\n",
      "\n",
      "\n",
      "__________________________________________\n",
      "Estimated mixture for document  41  is: \n",
      "_______________________\n",
      "topic  0 :  24.4109999044\n",
      "topic  1 :  0.0226782202354\n",
      "topic  2 :  43.8110717701\n",
      "topic  3 :  6.85541005746\n",
      "_______________________\n",
      "For the following lyrics:\n",
      " \n",
      "From: dwk@cci.cci.com (Dave Kehrer)\n",
      "Subject: Individual Winners (WAS: Re: WHERE ARE THE DOUBTERS NOW? HMM?)\n",
      "Summary: my picks\n",
      "Organization: Northern Telecom, Inc. - Network Application Systems\n",
      "Lines: \n",
      "\n",
      "Well, since you mentioned it...\n",
      "\n",
      "In article <Apr..@jarvis.csri.toronto.edu>, migod@turing.toronto.edu (Mike Godfrey) writes:\n",
      "  \n",
      "> Lemieux is clearly the MVP\n",
      "\n",
      "No question here.  Chip in the Masterson as well...\n",
      "\n",
      "> Selanne wins the Calder\n",
      "\n",
      "Yep. \n",
      "\n",
      "> Chelios the Norris,\n",
      "\n",
      "If you asked me  days ago, I'd agree with you.  I now give the nod to\n",
      "Raymond Bourque; his play took off the same time the B's did.  Chelios\n",
      "gets a close second...\n",
      "\n",
      "> dunno who wins the Vezina, but I suspect not Potvin.  \n",
      "  \n",
      "Barrasso finally gets his due, in a close one over Eddie the Eagle...\n",
      "\n",
      "> Coach of the year is tricky:  Burns did the most with the least raw talent,\n",
      "> King did a good job but the Flames clearly underachieved last year, Brian\n",
      "> Sutter has done exceptionally well in his first year with a new team, ditto\n",
      "> Demers, Page has been blessed by the ripening and acquisition of young\n",
      "> talent, Darryl Sutter is having a good year for a rookie coach, Berry made\n",
      "> the best of a bad situation, Terry Crisp worked minor miracles, and Bowman\n",
      "> was Bowman.  I'd pick Burns, but I'm mildly biased.\n",
      "\n",
      "In *your* case, that bias is acceptable :-)... Mine shows with the Norris pick,\n",
      "so we're even...\n",
      "\n",
      "I'm impressed with what all the coaches you mentioned did, but my pick would be \n",
      "Al Arbour.  Not too many folks thought the Isles would be in the playoffs, let \n",
      "alone contend for rd in their division... Granted that they *did* have a little\n",
      "help from their cousins on Broadway... :-)\n",
      "\n",
      "And I like the Islanders about as much as I like mowing my lawn...\n",
      "\n",
      "> Mike Godfrey\n",
      "--------\n",
      "David Kehrer (dwk@sunsrvr.cci.com)-Northern Telecom NAS-Rochester, New York \n",
      "\"It's nothing for me to eat six or seven pieces of pizza, then go out to\n",
      "dinner with my wife and not remember I had the pizza.\" - Jacques Demers\n",
      "\n",
      "__________________________________________\n",
      "Estimated mixture for document  42  is: \n",
      "_______________________\n",
      "topic  0 :  0.012528008964\n",
      "topic  1 :  188.436119688\n",
      "topic  2 :  3.06469630122\n",
      "topic  3 :  3.58681595369\n",
      "_______________________\n",
      "For the following lyrics:\n",
      " \n",
      "From: REXLEX@fnal.fnal.gov\n",
      "Subject: Re: Certainty and Arrogance\n",
      "Organization: FNAL/AD/Net\n",
      "Lines: \n",
      "\n",
      "In article <Apr......@geneva.rutgers.edu>\n",
      "kilroy@gboro.rowan.edu (Dr Nancy's Sweetie) writes:\n",
      "\n",
      ">Someone called `REXLEX' has claimed that there IS a way out of the loop, but\n",
      ">he did not bother to explain what it was, preferring instead to paraphrase\n",
      ">Sartre, ramble about Wittgenstein, and say that the conclusion of my argument\n",
      ">leads to relativism.\n",
      "\n",
      "I will answer this as I find time.\n",
      "\n",
      ">\n",
      ">`REXLEX' suggested that people read _He is There and He is Not Silent_, by\n",
      ">Francis Schaeffer.  I didn't think very highly of it, but I think that\n",
      ">Mr Schaeffer is grossly overrated by many Evangelical Christians.  Somebody\n",
      ">else might like it, though, so don't let my opinion stop you from reading it.\n",
      ">\n",
      ">If someone is interested in my opinion, I'd suggest _On Certainty_, by\n",
      ">Ludwig Wittgenstein.\n",
      ">\n",
      ">\n",
      ">Darren F Provine / kilroy@gboro.rowan.edu\n",
      ">\"If any substantial number of  [ talk.religion.misc ]  readers read some\n",
      "> Wittgenstein, % of the postings would disappear.  (If they *understood*\n",
      "> some Wittgenstein, % would disappear. :-))\" -- Michael L Siemon\n",
      ">\n",
      "\n",
      "Notice what I said about this book.  I called it \"Easy reading.\"  The reason I\n",
      "dropped philosphy as my major was because I ran into too many pharisaical\n",
      "Simon's.  I don't know how many walking encyclopedia's I ran across in\n",
      "philosphy classes.  The problem isn't in knowing sooooo much more than your\n",
      "average lay person, the problem comes when you become puffed up about it. \n",
      "Schaeffer is just fine for the average lay person.  That was who he was\n",
      "writting to.  I suppose that you would have criticised John that his gospel was\n",
      "to simple.  I've talked with Schaeffer one on one.  I've been in lectures with\n",
      "the man when he was being drilled by philosphy students and prof's from secular\n",
      "as well as Christian universities. (ND alone would fill both those catagories) \n",
      "His answers were enough that the prof's themselves often were taken back and\n",
      "caused to re-think what their question was.  I saw this time and time again at\n",
      "different open forums.  So yes, Schaeffers books are by in large, well,\n",
      "simplistic.  It certainly isn't grad level reading.  But we must get off our\n",
      "high horses when it comes to recommended reading.  Do you seriously think most\n",
      "people would get through the first chapter of Wittgenstein?  I may have more to\n",
      "say about this secular scientist at another time.\n",
      "\n",
      "Also, one must finally get beyond the doubt caused by *insistent*\n",
      "inquisitiveness.  One cannot live his life constantly from a cartisian doubt\n",
      "base.  \n",
      "\n",
      "Look, the Christian wholeheartedly supports genuine rationality.  But we must\n",
      "add a qualification to give this balance.  Christianity is second to none in\n",
      "keeping reason in its place.  We never know the value of a thing until we know\n",
      "its limits.  Put unlimited value on something and in the end you will exhaust\n",
      "it of all value!\n",
      "\n",
      "THis is why Xianity is thoroughly rational but not the least bit rationalistic.\n",
      " It also explains the curious fact that it is rationalism, and not Christian\n",
      "faith, which leads to irrationality.  If we forget the limits of a thing, we\n",
      "fly in the face of reality and condem ourselves to learn the simple ironic\n",
      "lesson of life: \n",
      "\n",
      "\"More without limits is less;  less with limits is more.\"  \n",
      "\n",
      "Or as I have so often stated it, freedom without form soon becomes form w/o\n",
      "freedom.\n",
      "\n",
      "Let's put it another way.  The rationality of faith is implacably opposed to\n",
      "absurdity but has no quarrel with mystery. Think about that.  It can tell the\n",
      "difference between the two if you will let it.  Christianity's contention with\n",
      "rationalism is not that it has too much reason in it, but that it has very\n",
      "little else.  When a Christian comes to faith his understanding and his trust\n",
      "go hand in hand, but as he continues in faith his trust may sometimes be called\n",
      "to go on by itself without his understanding.\n",
      "\n",
      "This is where the principle of suspended judgment applies.  At such time if the\n",
      "Christian faith is to be itself and let God be God, it must suspend judgment\n",
      "and say, \"Father I do not understand you but I trust you.\"  Now don't read all\n",
      "your objections of me into that statement.  I wasn't saying I do not understand\n",
      "you at all, but I trust you anyway.\" It means that \"I do not understand You *in\n",
      "this situation* but I do understand *why I trust You* anyway\"  Therefore I can\n",
      "trust that you understand even though I do not. The former is a mystery\n",
      "unrelieved by rationality and indistinguishable from absurdity.  The latter is\n",
      "a statement of rationality of faith walking hand in hand with the mystery of\n",
      "Faith.  So.... the principle of suspended judgment is not irrational.  It is\n",
      "not a leap of faith but a walk of faith.  As believers we cannot always know\n",
      "why, but we canalways know why we trust God who knows why and this makes all\n",
      "the difference.\n",
      "\n",
      "Now, there is one obvious snag to all this and this is where I have parted\n",
      "company with philosophy- what is eminently reasonable in theory is a rather bit\n",
      "more difficult in practice.  In practice the pressure of mystery acts on faith\n",
      "like the insistent \"whying\" of a  year old.  It isn't just that we would like\n",
      "to know what we do not know but that we feel we *must* know what we cannot\n",
      "know.  The one produces frustration because curiosity is denied; the other\n",
      "leads to genuine anguish.  More specifically the poorer our understanding is in\n",
      "coming to faith the more necessary it will be to understand everything after\n",
      "coming to faith.  If we do not know why we trust God, then we will always need\n",
      "to know exactly what God is doing in order to trust him.  Failing to grasp\n",
      "that, we may not be able to trust him, for anything we do not understand may\n",
      "count decisevely against what we are able to trust.  \n",
      "\n",
      "If, on the other hand, we do know why we trust God, we will be able to trust\n",
      "him in situations where we do not understand what He is doing. (Too many Xian\n",
      "leaders teach as if the Christian had a window in the back of his head which\n",
      "allows for understanding at every foot fall)  For what God is doing may be\n",
      "ambiguous, but it will not be inherently contradictory!  It may be mystery to\n",
      "us, but mystery is only inscrutable; what would be insufferable is absurdity.\n",
      "And that my friend, was the conclusion of Nietzche both in theory and in\n",
      "practice.   \n",
      "\n",
      "--Rex\n",
      "\n",
      "__________________________________________\n",
      "Estimated mixture for document  43  is: \n",
      "_______________________\n",
      "topic  0 :  0.012528008964\n",
      "topic  1 :  2.07988134973\n",
      "topic  2 :  0.0157206975471\n",
      "topic  3 :  10.992029896\n",
      "_______________________\n",
      "For the following lyrics:\n",
      " \n",
      "From: Stefan.M.Gorsch@dartmouth.edu (Stefan M. Gorsch)\n",
      "Subject: Importing Volvo?\n",
      "X-Posted-From: InterNews.b@newshost.dartmouth.edu\n",
      "Organization: Dartmouth College, Hanover, NH\n",
      "Lines: \n",
      "\n",
      "Well, I'm afraid the time has come; my rice-burner has finally died.\n",
      "I'd always promised my wife that we would do a Scandanavian tour when\n",
      "my car died and pick up a Volvo in Sweden, drive it around and then\n",
      "import it home. \n",
      "\n",
      "Can anyone give me ) advice on feasibility and relative costs )\n",
      "references where I might learn more ) Personal experience?\n",
      "\n",
      "Please email\n",
      "\n",
      "Thanks\n",
      "\n",
      "__________________________________________\n",
      "Estimated mixture for document  44  is: \n",
      "_______________________\n",
      "topic  0 :  0.012528008964\n",
      "topic  1 :  3.80093545767\n",
      "topic  2 :  4.03355119865\n",
      "topic  3 :  36.253145287\n",
      "_______________________\n",
      "For the following lyrics:\n",
      " \n",
      "From: ch@cleveland.Freenet.Edu (James K. Black)\n",
      "Subject: NEEDED: algorithms for -d & -d object recognition\n",
      "Organization: Case Western Reserve University, Cleveland, OH (USA)\n",
      "Lines: \n",
      "Reply-To: ch@cleveland.Freenet.Edu (James K. Black)\n",
      "NNTP-Posting-Host: hela.ins.cwru.edu\n",
      "\n",
      "\n",
      "Hi,\n",
      "         I have a friend who is working on -d and -d object recognition. He is looking\n",
      "for references describing algorithms on the following subject areas:\n",
      "\n",
      "Thresholding\n",
      "Edge Segmentation\n",
      "Marr-Hildreth\n",
      "Sobel Operator\n",
      "Chain Codes\n",
      "Thinning - Skeletonising\n",
      "\n",
      "If anybody is willing to post an algorithm that they have implemented which demonstrates\n",
      "any of the above topics, it would be much appreciated.\n",
      "\n",
      "Please post all replies to my e-mail address. If requested I will post a summary to the\n",
      "newsgroup in a couple of weeks.\n",
      "\n",
      "\n",
      "Thanks in advance for all replies\n",
      "\n",
      "James\n",
      "eb@city.ac.uk\n",
      "\n",
      "__________________________________________\n",
      "Estimated mixture for document  45  is: \n",
      "_______________________\n",
      "topic  0 :  0.012528008964\n",
      "topic  1 :  19.7247040485\n",
      "topic  2 :  0.0157206975471\n",
      "topic  3 :  28.3472071972\n",
      "_______________________\n",
      "For the following lyrics:\n",
      " \n",
      "From: tbrent@florin.ecn.purdue.edu (Timothy J Brent)\n",
      "Subject: Re: Pantheism & Environmentalism\n",
      "Organization: Purdue University Engineering Computer Network\n",
      "Lines: \n",
      "\n",
      "In article <Apr......@athos.rutgers.edu> jono@mac-ak-.rtsg.mot.com (Jon Ogden) writes:\n",
      "\n",
      ">So we see that we are masters of this planet.  It IS ours to care for and\n",
      ">ours to look after.  We will be judged on how well we do this task.\n",
      ">C.)  We are not to be in the business of spreading lies.  What we tell\n",
      ">others we must be sure is true.  We should check out the information,\n",
      ">verify it with scientific fact and go from there.\n",
      "\t\t\t   ^^^^\n",
      "\n",
      "Just what are these \"scientific facts\"?  I have never heard of such a thing.\n",
      "Science never proves or disproves any theory - history does.\n",
      "\n",
      "-Tim\n",
      "\n",
      " ______________________________________________________________________________\n",
      "|\t\t\t\t|\t\t\t\t       \t       |\n",
      "|       Timothy J. Brent        |   A man will come to know true happiness,    |\n",
      "|   BRENT@bank.ecn.purdue.edu   |   only when he accepts that he is but a      |\n",
      "|=========$$$$==================|   small part of an infinite universe.\t       |\n",
      "|       PURDUE UNIVERSITY       |\t\t\t  \t   -Spinoza    |\n",
      "| MATERIALS SCIENCE ENGINEERING |\t\t\t \t [paraphrased] |\n",
      "|_______________________________|______________________________________________|\n",
      "\n",
      "[I hope we don't get embroiled in a discussion over words here.  When\n",
      "somebody says \"get the facts\", I'm not sure we need to get into\n",
      "arguments over the philosophy of science.  --clh]\n",
      "\n",
      "__________________________________________\n",
      "Estimated mixture for document  46  is: \n",
      "_______________________\n",
      "topic  0 :  0.012528008964\n",
      "topic  1 :  9.13460445748\n",
      "topic  2 :  64.9037944593\n",
      "topic  3 :  0.0492330264676\n",
      "_______________________\n",
      "For the following lyrics:\n",
      " \n",
      "From: maynard@ramsey.cs.laurentian.ca (Roger Maynard)\n",
      "Subject: Re: If You Were Pat Burns ...\n",
      "Keywords: Leaf Wings\n",
      "Organization: Dept. of Computer Science, Laurentian University, Sudbury, ON\n",
      "Lines: \n",
      "\n",
      "In <Apr..@alchemy.chem.utoronto.ca> golchowy@alchemy.chem.utoronto.ca (Gerald Olchowy) writes:\n",
      "\n",
      ">Pray for the Wings to become lazy and overconfident...the Wings\n",
      ">can only lose the series...Toronto cannot win it.  Take away\n",
      ">Doug Gilmour and the Leafs are an old Tampa Bay.\n",
      "\n",
      "Right Gerald.  And take away Bob Probert and the Wings are dead Octopuses.\n",
      "\n",
      ">The Leafs deserve a lot of credit for their diligent effort\n",
      ">during the regular season...but if Detroit puts in a reasonable\n",
      ">effort, this is not a contest.\n",
      "\n",
      "Let's wait for the body to get cold before we start in with the eulogies\n",
      "hm?  They have only lost ONE game.  The game was in Detroit after all and\n",
      "Potvin did not have his best evening.  Nobody that I saw thought that the\n",
      "Leafs would sweep the Wings.  It looks like it might go six.  The Leafs\n",
      "will take the Wings home advantage away in the next game.\n",
      "\n",
      "\n",
      "-- \n",
      "\n",
      "cordially, as always,                      maynard@ramsey.cs.laurentian.ca \n",
      "                                           \"So many morons...\n",
      "rm                                                   ...and so little time.\" \n",
      "\n",
      "__________________________________________\n",
      "Estimated mixture for document  47  is: \n",
      "_______________________\n",
      "topic  0 :  0.012528008964\n",
      "topic  1 :  1.88531490235\n",
      "topic  2 :  0.0157206975471\n",
      "topic  3 :  49.1865963434\n",
      "_______________________\n",
      "For the following lyrics:\n",
      " \n",
      "From: eliot@stalfos.engr.washington.edu (eliot)\n",
      "Subject: Re: Ultimate AWD vehicles\n",
      "Organization: clearer than blir\n",
      "Lines: \n",
      "Distribution: usa\n",
      "NNTP-Posting-Host: ...\n",
      "\n",
      "In article <CLz.E@cbnewsj.cb.att.com> rdb@cbnewsj.cb.att.com (ronald.j.deblock..jr) writes:\n",
      ">Yes, I saw a  Turbo Quattro wagon on I- in NJ on Monday.  I thought\n",
      ">Audi stopped selling wagons in the US after the .  This is exactly the\n",
      ">type of vehicle I would like to own.  I bet its price is - times my\n",
      ">car budget.\n",
      "\n",
      "think again!!  thanks to  minutes (tick tick tick), used \n",
      "quattros are bargains.. 's go for about $K, 's go for perhaps \n",
      "or K more, the  valve 's are quite a bit more because of an\n",
      "enormous hp and torque gain.. i think they go for about $ to $K if\n",
      "you can find one.  i have seen quite a lot of '-'  quattros (not\n",
      "that many wagons though) at the dealer lot.. they use very high\n",
      "quality paint and the entire car is zinc galvanized, so it will never\n",
      "rust.\n",
      "\n",
      "in short, typically a  yr old  looks no more older than a  year\n",
      "old and the  bangers are bullet proof engines.  K out of one is\n",
      "not rare, even for a turbo, which is watercooled for the s.  then\n",
      "there are aftermarket chips that you can buy to bump up turbo boost...\n",
      "\n",
      "if you are into luxo-gizmos.. the cars are loaded with just about\n",
      "everything too..\n",
      "\n",
      "the price of parts is a different story though...\n",
      "\n",
      "\n",
      "eliot\n",
      "\n",
      "__________________________________________\n",
      "Estimated mixture for document  48  is: \n",
      "_______________________\n",
      "topic  0 :  0.012528008964\n",
      "topic  1 :  0.0226782202354\n",
      "topic  2 :  0.0157206975471\n",
      "topic  3 :  20.0492330255\n",
      "_______________________\n",
      "For the following lyrics:\n",
      " \n",
      "From: rdb@cbnewsj.cb.att.com (ronald.j.deblock..jr)\n",
      "Subject: Re: male/female mystery [ Re: Dumbest automotive concepts of all time ]\n",
      "Article-I.D.: cbnewsj.Apr..\n",
      "Distribution: na\n",
      "Organization: AT&T\n",
      "Lines: \n",
      "\n",
      "In article <pqtkINNbek@chester.ksu.ksu.edu> bets@chester.ksu.ksu.edu (Beth Schwindt) writes:\n",
      ">\n",
      ">Besides which, where would men put all their crap if their wives\n",
      ">didn't carry purses? :-)\n",
      ">\n",
      ">\n",
      ">Beth\n",
      ">\n",
      "\n",
      "My wife rarely carries a purse, so all of her crap ends up in my pockets!\n",
      "\n",
      "\n",
      "-- \n",
      "Ron DeBlock  rdb@homxb.att.com  (that's a number  in rdb, not letter l)\n",
      "AT&T Bell Labs   Somerset, NJ  USA\n",
      "\n",
      "__________________________________________\n",
      "Estimated mixture for document  49  is: \n",
      "_______________________\n",
      "topic  0 :  6.84077798197\n",
      "topic  1 :  7.41095725006\n",
      "topic  2 :  15.799191694\n",
      "topic  3 :  0.0492330262354\n",
      "_______________________\n",
      "For the following lyrics:\n",
      " \n",
      "Subject: Paul Kuryia and Canadian World Team\n",
      "From: apland@mala.bc.ca\n",
      "Organization: Malaspina College\n",
      "Lines: \n",
      "\n",
      "Heard last night that Paul Kuryia will be playing for the Canadian World\n",
      "Hockey team this year.  He was on a local radio station when a friend of\n",
      "the familty called to congratulate him on the invitation.  Meekly Paul told\n",
      "the host that he didn't think they wanted it out yet.  This morning I heard\n",
      "that he is destined to play on a line with Lindros and Recci{unsure of this\n",
      "one}.  If he plays well in this arena, he could go # or  in the draft.\n",
      "\n",
      "__________________________________________\n"
     ]
    }
   ],
   "source": [
    "#take a look at some example test documents (29-39, and 9-19 has a good spread of different types)\n",
    "dStart = 40\n",
    "dEnd = 50\n",
    "for d in range(dStart,dEnd):\n",
    "    print(\"Estimated mixture for document \", d,\" is: \")\n",
    "    print(\"_______________________\")\n",
    "    for i in range(len(gamma[d])):\n",
    "        print(\"topic \", i,\": \", gamma[d][i])\n",
    "    print(\"_______________________\")\n",
    "    print(\"For the following lyrics:\")\n",
    "    print(\" \")\n",
    "    print(testLyrics[d])\n",
    "    print(\"__________________________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisit the cell that presented the top words for your topics. Do the presented mixtures make sense if you look at the lyrics? Recall which emotions you (to your best ability) assigned to which topics. Do the song lyrics present a mixture of those emotions for you? \n",
    "\n",
    "LDA can experience some issues in this setting, as for example many words that would be present in a happy song could also be present in a sad song ('love', 'hold', 'forever') but in different order or with certain \"negating\" words between them. It is possible to alleviate this problem by using a vocabulary of n-grams, however this increases the total size of the vocabulary (and therefore the run time as well) substantially. \n",
    "\n",
    "It is also possible to gain some more insight by examining the $\\phi$ values for the documents. Recall that the $\\phi$ values for the document approximate $p(z_n | \\mathbf{w})$, showing how the topics are mixed for individual words in the lyrics. The cell below provides a method for converting the vocabulary matrix of a document to their original words. Using this, write code for looping over the words of a test document and printing the phi values for each word. Apart from just examining how the topics are mixed for specific words, take a look at the topic mixtures for the same word that appears in several different documents. As stated in the theory section, in LDA the distribution of topic mixtures that are assigned to each word is document specific. This means that hopefully it should be apparent from your results how the topic mixture for the same word can be differ in different test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gerald\t: [  5.87712136e-39   2.01493902e-23   1.00000000e+00   1.14148813e-22]\n",
      "asked\t: [  6.76332466e-37   1.00000000e+00   3.33788463e-29   1.66974469e-11]\n",
      "think\t: [  4.65142464e-38   1.00840393e-01   8.99159607e-01   2.16189862e-11]\n",
      "time\t: [  1.99371151e-37   1.22500146e-01   8.77499854e-01   1.99652579e-11]\n",
      "goals\t: [  1.25493715e-37   1.30910368e-23   1.00000000e+00   6.65112177e-13]\n",
      "given\t: [  3.39952794e-37   3.12003083e-01   6.87996917e-01   3.95436352e-11]\n",
      "points\t: [  3.68031629e-37   4.63088492e-02   9.53691151e-01   6.54607402e-12]\n",
      "based\t: [  4.58452401e-74   1.02251717e-01   8.97748283e-01   9.50163935e-12]\n",
      "writes\t: [  5.34852671e-38   4.10860552e-02   9.58913945e-01   1.71431443e-11]\n",
      "team\t: [  2.21366067e-37   2.40326847e-23   1.00000000e+00   2.73442891e-12]\n",
      "teams\t: [  3.06823292e-37   4.36226661e-23   1.00000000e+00   2.15152460e-12]\n",
      "ca\t: [  2.40448428e-37   3.23487195e-02   9.67651280e-01   1.02396370e-11]\n",
      "player\t: [  3.20335991e-37   2.12269253e-23   1.00000000e+00   1.25420585e-22]\n",
      "league\t: [  1.53168560e-37   2.13441829e-23   1.00000000e+00   4.34323080e-13]\n",
      "different\t: [  2.94610360e-37   4.16048574e-01   5.83951426e-01   1.03927604e-10]\n",
      "article\t: [  4.38610009e-38   5.21393630e-02   9.47860637e-01   2.41997853e-11]\n",
      "apr\t: [  1.43656574e-37   3.53361020e-02   9.64663898e-01   1.76192738e-11]\n",
      "information\t: [  1.16168661e-73   2.06843850e-01   7.93156150e-01   3.85487451e-11]\n",
      "stats\t: [  1.09629508e-37   1.47910767e-23   1.00000000e+00   1.38600238e-12]\n",
      "selanne\t: [  8.43693648e-37   1.35402830e-23   1.00000000e+00   1.08629802e-22]\n",
      "involved\t: [  1.20325595e-37   3.02932695e-01   6.97067305e-01   1.00181501e-11]\n",
      "game\t: [  1.00637809e-37   2.22522807e-23   1.00000000e+00   9.66036479e-13]\n",
      "just\t: [  7.46868165e-38   9.29924518e-02   9.07007548e-01   2.01897322e-11]\n",
      "lindros\t: [  7.51164969e-36   1.69178891e-22   1.00000000e+00   8.83749152e-22]\n",
      "chem\t: [  5.87712136e-39   2.01493902e-23   1.00000000e+00   1.14148813e-22]\n",
      "alchemy\t: [  5.87712136e-39   2.01493902e-23   1.00000000e+00   1.14148813e-22]\n",
      "words\t: [  3.91610713e-37   1.00000000e+00   1.30211654e-29   8.47570443e-11]\n",
      "looking\t: [  8.47334004e-73   1.00000000e+00   8.30938607e-29   4.33417337e-10]\n",
      "plus\t: [  4.19332774e-38   3.68989331e-02   9.63101067e-01   1.69367134e-11]\n",
      "college\t: [  4.82325135e-74   2.10418463e-02   9.78958154e-01   6.92819674e-12]\n",
      "utoronto\t: [  5.87712136e-39   2.01493902e-23   1.00000000e+00   1.14148813e-22]\n",
      "playing\t: [  7.04090601e-38   1.37700264e-23   1.00000000e+00   7.59300176e-13]\n",
      "players\t: [  1.93498864e-37   2.81397235e-23   1.00000000e+00   1.27920511e-12]\n",
      "________________________________\n",
      "example\t: [  1.37611946e-03   8.96913791e-01   1.80269642e-31   1.01710089e-01]\n",
      "catholic\t: [  4.32935808e-03   9.91049609e-01   1.52719097e-31   4.62103330e-03]\n",
      "modern\t: [  1.52853291e-38   9.05910478e-01   2.34742015e-31   9.40895220e-02]\n",
      "understanding\t: [  1.70710557e-38   7.98471836e-01   2.98502835e-31   2.01528164e-01]\n",
      "wouldn\t: [  2.67646575e-38   7.33974143e-01   1.02863311e-01   1.63162546e-01]\n",
      "note\t: [  8.87810764e-02   8.61898563e-01   1.26246662e-31   4.93203608e-02]\n",
      "ask\t: [ 0.00326999  0.76634951  0.05399995  0.17638055]\n",
      "work\t: [ 0.00919032  0.81144231  0.02653351  0.15283385]\n",
      "grace\t: [  8.27700777e-40   1.00000000e+00   2.70712104e-32   2.13223666e-13]\n",
      "point\t: [ 0.0456171   0.90534379  0.01911229  0.02992682]\n",
      "mean\t: [  1.08087600e-38   9.62659981e-01   1.52817612e-02   2.20582573e-02]\n",
      "think\t: [ 0.01052895  0.8518928   0.03885616  0.09872209]\n",
      "thing\t: [  8.84960334e-39   9.11841864e-01   2.48166310e-02   6.33415052e-02]\n",
      "problem\t: [ 0.03574156  0.85751798  0.001162    0.10557846]\n",
      "church\t: [  3.00715773e-04   9.95724671e-01   8.27835582e-32   3.97461334e-03]\n",
      "jesus\t: [ 0.00108153  0.99155334  0.00526398  0.00210115]\n",
      "instead\t: [ 0.05081674  0.77443277  0.01895613  0.15579435]\n",
      "especially\t: [  1.37904721e-38   9.68411141e-01   1.74038187e-02   1.41850403e-02]\n",
      "believe\t: [ 0.02001294  0.91681301  0.02883216  0.03434189]\n",
      "got\t: [ 0.05123909  0.6914185   0.11905023  0.13829217]\n",
      "matter\t: [  2.83942549e-02   9.70516099e-01   1.07676779e-31   1.08964658e-03]\n",
      "st\t: [ 0.17496003  0.77970224  0.02945441  0.01588333]\n",
      "terms\t: [ 0.03908647  0.92544779  0.00572671  0.02973904]\n",
      "body\t: [  9.82963834e-39   9.54150029e-01   1.76204146e-31   4.58499713e-02]\n",
      "holy\t: [  4.75891725e-39   9.86289855e-01   1.16201835e-02   2.08996200e-03]\n",
      "soul\t: [  6.97864725e-39   9.95026793e-01   1.11650136e-31   4.97320740e-03]\n",
      "city\t: [ 0.24280596  0.50969297  0.18762833  0.05987274]\n",
      "doing\t: [  2.52938157e-02   9.22166520e-01   1.50595166e-31   5.25396639e-02]\n",
      "new\t: [ 0.16247968  0.71901907  0.04134055  0.0771607 ]\n",
      "thinking\t: [ 0.02443732  0.8825483   0.03755312  0.05546126]\n",
      "meaning\t: [ 0.00219814  0.98571427  0.01085511  0.00123249]\n",
      "does\t: [ 0.00523629  0.89794146  0.02860481  0.06821744]\n",
      "exist\t: [  4.61225480e-39   9.92006736e-01   1.15040238e-31   7.99326370e-03]\n",
      "god\t: [  1.30524441e-04   9.92267537e-01   1.12844342e-31   7.60193845e-03]\n",
      "host\t: [ 0.14665917  0.07633344  0.20619611  0.57081128]\n",
      "issue\t: [  6.04583382e-39   9.24338978e-01   3.27743688e-02   4.28866530e-02]\n",
      "murray\t: [  8.38262043e-01   8.15241713e-02   8.02137853e-02   2.86406762e-12]\n",
      "away\t: [  3.13096269e-02   9.09458717e-01   1.48795261e-31   5.92316558e-02]\n",
      "fact\t: [ 0.01592437  0.89835277  0.02982338  0.05589948]\n",
      "reply\t: [ 0.06979091  0.81262322  0.01746884  0.10011703]\n",
      "start\t: [  1.04478716e-38   9.54490816e-01   1.89091114e-02   2.66000721e-02]\n",
      "joseph\t: [ 0.50506369  0.44092028  0.02589893  0.02811709]\n",
      "kind\t: [  1.26994199e-38   8.66266842e-01   1.90999695e-31   1.33733158e-01]\n",
      "shift\t: [  1.65927380e-37   5.78117758e-21   4.90194291e-01   5.09805709e-01]\n",
      "simply\t: [  2.65209591e-02   9.65395932e-01   1.05428148e-31   8.08310850e-03]\n",
      "try\t: [  9.89522504e-03   6.70249144e-01   7.12242482e-31   3.19855631e-01]\n",
      "real\t: [ 0.00114985  0.89772208  0.01397395  0.08715412]\n",
      "experience\t: [  8.11244096e-02   5.97364953e-01   4.78673610e-31   3.21510637e-01]\n",
      "easter\t: [  7.74156599e-02   8.29127454e-01   4.35708391e-31   9.34568862e-02]\n",
      "time\t: [ 0.03732515  0.85590839  0.03136248  0.07540397]\n",
      "way\t: [ 0.02785745  0.7938602   0.05258521  0.12569714]\n",
      "remember\t: [  1.90913492e-01   5.93308027e-01   6.13584140e-31   2.15778480e-01]\n",
      "problems\t: [ 0.11907061  0.79369739  0.01267918  0.07455282]\n",
      "paul\t: [  4.95158653e-39   9.97759770e-01   8.74471247e-32   2.24023035e-03]\n",
      "dare\t: [  1.03027563e-37   2.10103602e-21   1.00000000e+00   7.14265707e-12]\n",
      "likely\t: [  7.70148619e-02   8.40226064e-01   8.27590738e-02   1.90073427e-12]\n",
      "help\t: [ 0.0244665   0.54599399  0.11556647  0.31397304]\n",
      "means\t: [  8.33560329e-03   9.13915884e-01   1.45731385e-31   7.77485124e-02]\n",
      "words\t: [  9.93373858e-03   9.46693812e-01   6.30568216e-32   4.33724498e-02]\n",
      "better\t: [ 0.04785379  0.70449523  0.05105362  0.19659737]\n",
      "christ\t: [  6.72036660e-39   9.89792688e-01   4.69109180e-03   5.51622033e-03]\n",
      "doesn\t: [ 0.00892049  0.88390989  0.01618004  0.09098959]\n",
      "come\t: [ 0.02404109  0.8902213   0.01109724  0.07464036]\n",
      "heart\t: [  3.44364372e-39   1.00000000e+00   7.63588503e-32   6.07798089e-13]\n",
      "site\t: [  2.03381465e-01   3.25651530e-21   5.33857858e-02   7.43232749e-01]\n",
      "old\t: [  1.90181976e-38   8.44905237e-01   1.24444685e-02   1.42650294e-01]\n",
      "days\t: [ 0.00704811  0.94725168  0.00389771  0.0418025 ]\n",
      "________________________________\n",
      "nd\t: [  2.48557531e-01   5.51066914e-24   7.51442469e-01   1.74694346e-13]\n",
      "jim\t: [ 0.33200372  0.00410263  0.54890821  0.11498544]\n",
      "maple\t: [  9.56176122e-01   8.37787395e-24   1.43940571e-30   4.38238781e-02]\n",
      "nhl\t: [  5.11600774e-01   2.34991854e-04   4.86676175e-01   1.48805871e-03]\n",
      "run\t: [  5.90535324e-01   8.89094966e-02   1.04653962e-29   3.20555179e-01]\n",
      "steve\t: [  1.17929135e-37   1.91193773e-03   9.48643629e-01   4.94444335e-02]\n",
      "year\t: [ 0.59329843  0.00993675  0.34449207  0.05227276]\n",
      "got\t: [ 0.16059553  0.01914613  0.76936616  0.05089218]\n",
      "team\t: [  3.56192336e-01   1.27507352e-23   6.33385892e-01   1.04217726e-02]\n",
      "guess\t: [  2.16795241e-37   7.85766615e-02   7.33658530e-01   1.87764808e-01]\n",
      "works\t: [  5.05755006e-02   5.08554674e-02   3.01295144e-29   8.98569032e-01]\n",
      "player\t: [  4.48667416e-01   9.80314220e-24   5.51332584e-01   4.16091699e-13]\n",
      "left\t: [ 0.07758874  0.01211947  0.83979405  0.07049774]\n",
      "like\t: [ 0.24674151  0.06035614  0.58391126  0.10899109]\n",
      "real\t: [ 0.02389178  0.16479979  0.59868257  0.21262585]\n",
      "terrible\t: [  4.05630830e-38   3.05959593e-02   9.69404041e-01   5.56777976e-13]\n",
      "biggest\t: [ 0.74185757  0.00895045  0.23276063  0.01643134]\n",
      "host\t: [ 0.22932911  0.00105456  0.66481568  0.10480065]\n",
      "thinking\t: [ 0.21034384  0.06711548  0.66648916  0.05605152]\n",
      "better\t: [ 0.26231369  0.03411856  0.57703476  0.12653299]\n",
      "place\t: [  4.24825463e-37   1.60104761e-01   6.74390540e-01   1.65504699e-01]\n",
      "sanderson\t: [  8.63817186e-01   3.46787441e-24   1.36182814e-01   1.52313154e-13]\n",
      "live\t: [ 0.42887221  0.2497034   0.18886689  0.13255749]\n",
      "right\t: [ 0.0163335   0.08474285  0.73364884  0.16527482]\n",
      "number\t: [ 0.23913852  0.09647813  0.59023039  0.07415296]\n",
      "round\t: [  8.88338053e-02   3.59125515e-04   9.09317032e-01   1.49003734e-03]\n",
      "work\t: [ 0.10324561  0.08053904  0.61461942  0.20159593]\n",
      "eric\t: [ 0.07932911  0.23862126  0.64419545  0.03785418]\n",
      "hartford\t: [  7.73429606e-01   6.04367084e-24   2.26570394e-01   2.79132196e-13]\n",
      "goals\t: [  2.41007232e-01   8.28975233e-24   7.55967217e-01   3.02555066e-03]\n",
      "ca\t: [ 0.36638567  0.01625298  0.58040385  0.0369575 ]\n",
      "mention\t: [  2.22934915e-37   1.84370310e-01   6.51533200e-01   1.64096490e-01]\n",
      "fans\t: [  3.77796606e-01   1.40714824e-23   6.17751411e-01   4.45198314e-03]\n",
      "nntp\t: [ 0.20779132  0.0010863   0.68481965  0.10630273]\n",
      "quite\t: [ 0.15580402  0.11537806  0.66841938  0.06039854]\n",
      "season\t: [  4.41806069e-01   1.13179864e-23   5.58193931e-01   5.41972189e-13]\n",
      "writes\t: [ 0.11025575  0.02792678  0.77811101  0.08370646]\n",
      "years\t: [ 0.39437661  0.04110437  0.50679956  0.05771946]\n",
      "powerplay\t: [  2.84563834e-01   3.57333889e-24   7.15436166e-01   1.46159925e-13]\n",
      "start\t: [  2.06703649e-37   1.66839974e-01   7.71369117e-01   6.17909085e-02]\n",
      "played\t: [  5.60691840e-01   2.31486592e-03   4.36993294e-01   2.62587221e-13]\n",
      "posting\t: [ 0.20237736  0.0218646   0.66697681  0.10878123]\n",
      "extended\t: [  8.81821049e-01   4.12960159e-02   4.54594757e-30   7.68829346e-02]\n",
      "keywords\t: [  3.90834145e-01   3.00828443e-04   5.26858103e-01   8.20069238e-02]\n",
      "new\t: [ 0.61748183  0.02414202  0.32394569  0.03443045]\n",
      "uwaterloo\t: [  1.94731702e-01   1.56490501e-23   7.92745242e-01   1.25230558e-02]\n",
      "teams\t: [  4.34867657e-01   2.03863935e-23   5.57909360e-01   7.22298201e-03]\n",
      "article\t: [ 0.08924162  0.03497953  0.75915112  0.11662773]\n",
      "close\t: [ 0.15597252  0.04270479  0.70823938  0.09308332]\n",
      "john\t: [ 0.22965881  0.04046476  0.70728738  0.02258905]\n",
      "leafs\t: [  3.99270890e-01   9.68451398e-24   6.00729110e-01   3.96503004e-13]\n",
      "pick\t: [  1.55962114e-01   1.49299236e-23   8.28226442e-01   1.58114434e-02]\n",
      "ago\t: [ 0.37301879  0.04140656  0.55433643  0.03123822]\n",
      "couple\t: [  6.44075277e-01   1.96275988e-01   1.03313399e-29   1.59648735e-01]\n",
      "goal\t: [  1.23546529e-01   7.21879973e-24   8.76453471e-01   2.91807832e-13]\n",
      "probably\t: [ 0.26311438  0.0444957   0.60651636  0.08587356]\n",
      "way\t: [ 0.17625036  0.04437524  0.68599834  0.09337606]\n",
      "make\t: [ 0.22551695  0.07743628  0.54982405  0.14722272]\n",
      "points\t: [ 0.47536183  0.01972253  0.48488838  0.02002726]\n",
      "went\t: [ 0.06279406  0.02140074  0.8835143   0.0322909 ]\n",
      "mvp\t: [  4.26646972e-01   2.50638687e-23   5.73353028e-01   1.59451995e-12]\n",
      "net\t: [ 0.33671075  0.03626419  0.59322253  0.03380253]\n",
      "point\t: [ 0.47253198  0.08285621  0.40821313  0.03639868]\n",
      "mark\t: [ 0.06462049  0.08647931  0.84072728  0.00817292]\n",
      "power\t: [  7.54155971e-01   8.41772343e-02   3.44791432e-30   1.61666795e-01]\n",
      "appropriate\t: [  1.05241662e-01   8.10521725e-01   1.66750480e-29   8.42366127e-02]\n",
      "play\t: [ 0.32053282  0.00308937  0.65705041  0.01932739]\n",
      "andrew\t: [ 0.59732549  0.08104605  0.16325768  0.15837078]\n",
      "far\t: [ 0.18997139  0.05588815  0.71986986  0.0342706 ]\n",
      "hard\t: [ 0.28741083  0.02063608  0.48337694  0.20857615]\n",
      "just\t: [ 0.14638658  0.06009861  0.6997825   0.09373231]\n",
      "score\t: [  3.58346507e-38   8.69679312e-24   1.00000000e+00   3.24494790e-13]\n",
      "center\t: [ 0.65312513  0.02094308  0.07307544  0.25285635]\n",
      "coming\t: [  6.50336411e-37   5.58011514e-01   2.05799723e-29   4.41988486e-01]\n",
      "hockey\t: [  3.64685964e-01   1.28337615e-23   6.32885628e-01   2.42840809e-03]\n",
      "com\t: [ 0.29543051  0.04614483  0.5219764   0.13644826]\n",
      "period\t: [ 0.48430655  0.01715828  0.47818017  0.020355  ]\n",
      "games\t: [  4.90178046e-01   7.12414180e-24   5.08553609e-01   1.26834511e-03]\n",
      "opinion\t: [ 0.25318154  0.09487632  0.63419252  0.01774963]\n",
      "________________________________\n",
      "christ\t: [  7.80835828e-75   1.00000000e+00   8.78648930e-32   3.17735001e-13]\n",
      "possible\t: [  1.46384206e-38   1.00000000e+00   5.09460129e-60   8.01072354e-12]\n",
      "posting\t: [  9.40458221e-38   1.00000000e+00   2.42321636e-30   2.13435571e-11]\n",
      "response\t: [  8.69421086e-75   1.00000000e+00   2.83513102e-32   1.88081734e-12]\n",
      "son\t: [  6.35705670e-75   1.00000000e+00   3.59141225e-31   3.31641122e-23]\n",
      "holy\t: [  5.54899836e-75   1.00000000e+00   2.18420860e-31   1.20809621e-13]\n",
      "john\t: [  5.76667139e-38   1.00000000e+00   1.38848723e-30   2.39483388e-12]\n",
      "th\t: [  2.61896895e-37   1.00000000e+00   4.61584352e-30   5.16041028e-13]\n",
      "time\t: [  5.01516635e-38   1.00000000e+00   6.79311272e-31   5.02266975e-12]\n",
      "god\t: [  1.51277406e-40   1.00000000e+00   2.10832082e-60   4.36780520e-13]\n",
      "came\t: [  1.01532745e-39   1.00000000e+00   7.30024175e-31   1.98108092e-12]\n",
      "reference\t: [  2.03184685e-74   1.00000000e+00   8.19709298e-60   1.18300000e-11]\n",
      "line\t: [  4.40125843e-38   1.00000000e+00   4.79728796e-30   3.37388954e-11]\n",
      "times\t: [  6.84101074e-38   1.00000000e+00   6.87514170e-31   7.74686567e-12]\n",
      "provide\t: [  2.94540417e-74   1.00000000e+00   1.39693413e-31   1.59042691e-11]\n",
      "left\t: [  6.50480537e-38   1.00000000e+00   5.50442874e-30   2.49543376e-11]\n",
      "christian\t: [  3.68748064e-38   1.00000000e+00   8.70495077e-32   9.96878416e-14]\n",
      "human\t: [  3.29727205e-75   1.00000000e+00   1.45808000e-60   1.34880901e-12]\n",
      "lord\t: [  2.24800774e-40   1.00000000e+00   1.48581993e-60   4.05132425e-23]\n",
      "heart\t: [  3.96031036e-75   1.00000000e+00   1.41561473e-60   3.46518979e-23]\n",
      "history\t: [  8.03692107e-38   1.00000000e+00   4.62744714e-31   3.17907274e-12]\n",
      "read\t: [  1.26875595e-74   1.00000000e+00   2.73805028e-31   4.37921146e-12]\n",
      "peter\t: [  9.69964919e-38   1.00000000e+00   8.38160478e-32   3.92751074e-12]\n",
      "discussion\t: [  8.21955427e-75   1.00000000e+00   3.34613561e-32   2.21328489e-12]\n",
      "quite\t: [  1.37206435e-38   1.00000000e+00   4.60202231e-31   2.24572963e-12]\n",
      "note\t: [  1.18460963e-37   1.00000000e+00   2.71549770e-60   3.26240432e-12]\n",
      "sure\t: [  3.22210067e-38   1.00000000e+00   1.04001213e-30   8.10380458e-12]\n",
      "true\t: [  1.44287717e-38   1.00000000e+00   1.61648058e-31   4.13455490e-12]\n",
      "especially\t: [  1.63768498e-74   1.00000000e+00   3.33173512e-31   8.35100016e-13]\n",
      "catholic\t: [  5.02387844e-39   1.00000000e+00   2.85682505e-60   2.65834534e-13]\n",
      "reason\t: [  4.08979900e-39   1.00000000e+00   4.95932331e-31   3.05502043e-12]\n",
      "sin\t: [  5.42504704e-75   1.00000000e+00   1.24763506e-60   2.06599748e-13]\n",
      "parents\t: [  1.56304256e-38   1.00000000e+00   1.11272216e-59   1.09792019e-11]\n",
      "want\t: [  4.96081437e-38   1.00000000e+00   2.30635921e-31   4.42883544e-12]\n",
      "jesus\t: [  1.25439226e-39   1.00000000e+00   9.84200818e-32   1.20811786e-13]\n",
      "religion\t: [  1.86630586e-39   1.00000000e+00   2.51501471e-60   7.00046043e-13]\n",
      "end\t: [  6.09413569e-39   1.00000000e+00   1.24421459e-30   2.70377624e-12]\n",
      "grace\t: [  9.51884756e-76   1.00000000e+00   5.01872464e-61   1.21563473e-23]\n",
      "given\t: [  3.35752584e-38   1.00000000e+00   2.09115384e-31   3.90583607e-12]\n",
      "says\t: [  6.75331120e-75   1.00000000e+00   1.93564894e-60   1.97427807e-12]\n",
      "joseph\t: [  1.31733757e-36   1.00000000e+00   1.08894888e-30   3.63561625e-12]\n",
      "women\t: [  2.09749972e-74   1.00000000e+00   1.05156985e-30   1.54911446e-12]\n",
      "day\t: [  5.72413570e-38   1.00000000e+00   4.13578602e-31   2.50482291e-12]\n",
      "heaven\t: [  8.47071822e-75   1.00000000e+00   2.29209971e-60   2.55604744e-13]\n",
      "com\t: [  6.50506115e-38   1.00000000e+00   8.98567656e-31   1.26852576e-11]\n",
      "thanks\t: [  7.42950000e-38   1.00000000e+00   4.22627727e-30   2.53913580e-11]\n",
      "physical\t: [  2.54089350e-74   1.00000000e+00   6.02276535e-60   1.38126194e-11]\n",
      "man\t: [  1.62586582e-74   1.00000000e+00   4.04580836e-31   7.07290870e-12]\n",
      "books\t: [  1.93054497e-74   1.00000000e+00   4.49498739e-60   9.09023436e-12]\n",
      "book\t: [  9.63049451e-75   1.00000000e+00   2.76076811e-60   2.76168436e-12]\n",
      "thought\t: [  2.50137428e-39   1.00000000e+00   1.09502772e-30   3.23663051e-12]\n",
      "goes\t: [  2.00135557e-74   1.00000000e+00   9.67232828e-31   3.83742159e-12]\n",
      "power\t: [  9.10302154e-38   1.00000000e+00   3.25375418e-60   8.23911309e-12]\n",
      "title\t: [  7.55509817e-37   1.00000000e+00   9.35226715e-31   2.95838681e-11]\n",
      "just\t: [  2.47488991e-38   1.00000000e+00   9.24957078e-31   6.69081725e-12]\n",
      "thing\t: [  1.11613130e-74   1.00000000e+00   5.04555551e-31   3.96037716e-12]\n",
      "matter\t: [  3.36464107e-38   1.00000000e+00   2.05686129e-60   6.40104105e-14]\n",
      "born\t: [  4.05294842e-38   1.00000000e+00   2.79212353e-60   1.49659608e-12]\n",
      "far\t: [  3.45372328e-38   1.00000000e+00   1.02319216e-30   2.63060884e-12]\n",
      "city\t: [  5.47850065e-37   1.00000000e+00   6.82457283e-30   6.69712148e-12]\n",
      "knowledge\t: [  2.50347665e-74   1.00000000e+00   3.50309223e-31   9.61918583e-12]\n",
      "said\t: [  2.73458184e-38   1.00000000e+00   5.51367341e-31   1.56564010e-12]\n",
      "think\t: [  1.42138293e-38   1.00000000e+00   8.45591556e-31   6.60689039e-12]\n",
      "coming\t: [  1.18416974e-74   1.00000000e+00   2.92970946e-60   3.39798991e-12]\n",
      "faith\t: [  7.30427197e-75   1.00000000e+00   3.74732944e-32   3.81744748e-13]\n",
      "light\t: [  1.29153846e-74   1.00000000e+00   8.92889981e-31   9.53917179e-13]\n",
      "ad\t: [  9.58267163e-75   1.00000000e+00   2.12040004e-60   1.32639668e-12]\n",
      "world\t: [  3.16174552e-38   1.00000000e+00   8.59128091e-31   5.32575263e-12]\n",
      "body\t: [  1.18476409e-74   1.00000000e+00   3.42361680e-60   2.73961853e-12]\n",
      "choose\t: [  8.72295527e-75   1.00000000e+00   2.85896090e-60   2.83184889e-12]\n",
      "hope\t: [  2.73416063e-38   1.00000000e+00   1.22364354e-30   3.76356862e-12]\n",
      "use\t: [  4.98956755e-39   1.00000000e+00   2.84064386e-31   4.89484925e-12]\n",
      "mention\t: [  1.22858768e-74   1.00000000e+00   2.80716857e-31   3.81823449e-12]\n",
      "going\t: [  4.51708169e-38   1.00000000e+00   7.84278668e-31   9.00183850e-12]\n",
      "original\t: [  1.18769463e-74   1.00000000e+00   4.06300120e-60   2.16160146e-12]\n",
      "really\t: [  9.92772515e-40   1.00000000e+00   5.70577830e-31   6.93866091e-12]\n",
      "mary\t: [  1.65865261e-74   1.00000000e+00   2.49920311e-60   4.97376686e-23]\n",
      "st\t: [  2.58060220e-37   1.00000000e+00   7.00337248e-31   1.16139621e-12]\n",
      "let\t: [  5.24549161e-38   1.00000000e+00   1.58692619e-30   5.39644308e-12]\n",
      "control\t: [  3.25323508e-39   1.00000000e+00   5.94966974e-31   4.44783225e-13]\n",
      "david\t: [  4.01567226e-39   1.00000000e+00   7.57756627e-32   1.68840440e-12]\n",
      "hold\t: [  8.36144387e-75   1.00000000e+00   7.07470327e-31   2.11071539e-13]\n",
      "non\t: [  1.65842125e-38   1.00000000e+00   2.50280145e-32   4.34451274e-12]\n",
      "question\t: [  2.36265397e-38   1.00000000e+00   3.31955234e-31   4.26720357e-12]\n",
      "bit\t: [  1.53898138e-74   1.00000000e+00   5.07158925e-60   8.99850993e-12]\n",
      "example\t: [  1.76447879e-39   1.00000000e+00   3.72612644e-60   6.46518609e-12]\n",
      "scripture\t: [  9.00101302e-40   1.00000000e+00   1.22094235e-60   4.23781059e-23]\n",
      "soul\t: [  8.06580058e-75   1.00000000e+00   2.08022434e-60   2.84950545e-13]\n",
      "life\t: [  1.03871063e-74   1.00000000e+00   9.61020659e-32   5.70640241e-12]\n",
      "actually\t: [  1.73193640e-39   1.00000000e+00   1.19952692e-30   5.70038482e-12]\n",
      "clh\t: [  5.33204940e-75   1.00000000e+00   1.41171734e-60   6.16494876e-13]\n",
      "________________________________\n",
      "pick\t: [  3.00131079e-38   7.58968803e-44   9.72488647e-01   2.75113535e-02]\n",
      "rangers\t: [  2.90888642e-37   1.17443644e-43   1.00000000e+00   2.10358030e-12]\n",
      "stanley\t: [  2.16854384e-74   1.47506685e-23   9.84012945e-01   1.59870548e-02]\n",
      "nntp\t: [  4.04290939e-38   5.58327374e-24   8.12992255e-01   1.87007745e-01]\n",
      "let\t: [  3.87388521e-38   1.98223018e-22   9.14654329e-01   8.53456707e-02]\n",
      "posting\t: [  4.00494401e-38   1.14301117e-22   8.05357763e-01   1.94642237e-01]\n",
      "st\t: [  4.51594396e-37   4.69700491e-22   9.56476759e-01   4.35232413e-02]\n",
      "playoff\t: [  1.78452871e-37   9.17584464e-44   1.00000000e+00   1.43935819e-12]\n",
      "points\t: [  1.51404844e-37   1.65940546e-22   9.42325272e-01   5.76747281e-02]\n",
      "series\t: [  8.90960605e-75   8.71512841e-44   8.16649954e-01   1.83350046e-01]\n",
      "nd\t: [  5.42110222e-38   3.17496901e-44   1.00000000e+00   3.44499110e-13]\n",
      "probably\t: [  5.87681516e-38   2.62536680e-22   8.26577479e-01   1.73422521e-01]\n",
      "scoring\t: [  8.86762958e-38   2.36231657e-44   1.00000000e+00   3.24263750e-13]\n",
      "email\t: [  1.15871849e-73   1.81454504e-22   2.00867510e-29   1.00000000e+00]\n",
      "hockey\t: [  9.39048122e-38   8.72965834e-44   9.94346218e-01   5.65378195e-03]\n",
      "goes\t: [  2.39097722e-74   3.20659949e-22   9.01824291e-01   9.81757087e-02]\n",
      "happen\t: [  1.30225392e-37   2.93471353e-22   7.53816199e-01   2.46183801e-01]\n",
      "university\t: [  8.75969193e-38   1.07234817e-22   8.73747317e-01   1.26252683e-01]\n",
      "game\t: [  4.15429872e-38   8.00101303e-44   9.91459563e-01   8.54043695e-03]\n",
      "opinion\t: [  6.28230493e-38   6.21898169e-22   9.60177846e-01   3.98221537e-02]\n",
      "season\t: [  1.29718815e-37   8.77839153e-44   1.00000000e+00   1.43878825e-12]\n",
      "round\t: [  1.59722488e-38   1.70572316e-24   9.97577669e-01   2.42233127e-03]\n",
      "course\t: [  6.39274853e-38   9.69816101e-22   3.13202655e-01   6.86797345e-01]\n",
      "conference\t: [  5.88276561e-39   1.58011496e-43   7.94867553e-01   2.05132447e-01]\n",
      "post\t: [  2.64022800e-74   5.77130159e-22   7.33889612e-01   2.66110388e-01]\n",
      "games\t: [  1.57387931e-37   6.04261061e-44   9.96317832e-01   3.68216793e-03]\n",
      "number\t: [  5.59805415e-38   5.96610509e-22   8.43048797e-01   1.56951203e-01]\n",
      "ll\t: [  1.80196928e-74   1.36310302e-22   9.01989423e-01   9.80105772e-02]\n",
      "want\t: [  1.80498230e-37   9.76592768e-22   6.54917561e-01   3.45082439e-01]\n",
      "format\t: [  1.31320879e-73   1.18062435e-42   1.63865387e-29   1.00000000e+00]\n",
      "cup\t: [  3.21393099e-38   7.21000578e-44   9.94866722e-01   5.13327811e-03]\n",
      "like\t: [  5.42497311e-38   3.50551060e-22   7.83331910e-01   2.16668090e-01]\n",
      "ot\t: [  8.88517059e-37   1.21346348e-21   9.75123503e-01   2.48764971e-02]\n",
      "mail\t: [  1.74384468e-38   3.16690700e-22   1.69044879e-29   1.00000000e+00]\n",
      "form\t: [  6.10498151e-37   7.68626850e-22   1.56442130e-29   1.00000000e+00]\n",
      "league\t: [  6.35261428e-38   7.71074410e-44   9.96142146e-01   3.85785395e-03]\n",
      "long\t: [  2.17480513e-38   6.43937054e-22   7.07440024e-01   2.92559976e-01]\n",
      "won\t: [  2.79490318e-38   1.39851228e-22   9.52943759e-01   4.70562414e-02]\n",
      "deal\t: [  8.67965141e-38   2.36446151e-22   7.37235671e-01   2.62764329e-01]\n",
      "real\t: [  4.28520425e-39   7.80825728e-22   6.55183834e-01   3.44816166e-01]\n",
      "school\t: [  5.38692290e-37   2.01677375e-21   2.03945280e-29   1.00000000e+00]\n",
      "way\t: [  3.50401008e-38   2.33050788e-22   8.32150729e-01   1.67849271e-01]\n",
      "getting\t: [  4.61156474e-38   1.14912695e-43   9.17260451e-01   8.27395494e-02]\n",
      "early\t: [  2.07928309e-37   1.05710673e-21   1.74724833e-29   1.00000000e+00]\n",
      "think\t: [  1.77358324e-38   3.34914549e-22   8.23456827e-01   1.76543173e-01]\n",
      "important\t: [  7.18646112e-38   1.44792922e-21   2.55613017e-01   7.44386983e-01]\n",
      "host\t: [  4.58290609e-38   5.56710132e-24   8.10637354e-01   1.89362646e-01]\n",
      "________________________________\n",
      "ll\t: [  1.06872767e-37   4.31339204e-22   6.87940922e-01   3.12059078e-01]\n",
      "home\t: [  2.49065984e-01   2.15038833e-22   5.62231351e-01   1.88702665e-01]\n",
      "article\t: [  7.13357545e-02   3.94093185e-22   4.76147708e-01   4.52516538e-01]\n",
      "distribution\t: [  3.21711258e-01   6.97117323e-24   2.87537472e-01   3.90751270e-01]\n",
      "happened\t: [  2.66156683e-01   1.39707552e-21   4.18842580e-01   3.15000737e-01]\n",
      "aren\t: [  2.28762745e-01   2.81817665e-43   2.18749967e-01   5.52487288e-01]\n",
      "agree\t: [  2.39539675e-02   8.87127687e-22   6.87409477e-01   2.88636555e-01]\n",
      "quebec\t: [  9.79191926e-01   2.58869847e-23   1.25593323e-30   2.08080738e-02]\n",
      "school\t: [  5.00861485e-01   1.00047132e-21   2.43848768e-30   4.99138515e-01]\n",
      "read\t: [  1.63028221e-37   1.84013131e-21   3.53098492e-01   6.46901508e-01]\n",
      "think\t: [  8.12061627e-02   8.18167098e-22   4.84850979e-01   4.33942858e-01]\n",
      "stupid\t: [  5.29334824e-38   1.05093312e-22   6.64652249e-01   3.35347751e-01]\n",
      "leafs\t: [  4.58599742e-01   1.56779421e-43   5.41400258e-01   2.21057589e-12]\n",
      "league\t: [  3.27965152e-01   2.12393924e-43   6.61342694e-01   1.06921539e-02]\n",
      "time\t: [  2.84838024e-01   8.13348627e-22   3.87214122e-01   3.27947854e-01]\n",
      "best\t: [  7.63236992e-02   4.39116842e-22   7.11314631e-01   2.12361669e-01]\n",
      "lose\t: [  2.94753232e-37   2.85664826e-21   9.52856405e-01   4.71435952e-02]\n",
      "playing\t: [  1.81464882e-01   1.64931383e-43   7.96035675e-01   2.24994429e-02]\n",
      "university\t: [  3.27177226e-01   2.13698085e-22   4.19672119e-01   2.53150655e-01]\n",
      "roger\t: [  6.37101184e-38   1.84816744e-43   1.00000000e+00   3.33888095e-12]\n",
      "patrick\t: [  8.48800864e-01   4.67243965e-23   1.45889934e-01   5.30920223e-03]\n",
      "wanted\t: [  8.87015271e-01   1.44558059e-21   3.63693663e-30   1.12984729e-01]\n",
      "hell\t: [  1.19851146e-37   6.10864344e-22   4.37303383e-01   5.62696617e-01]\n",
      "isn\t: [  2.76725650e-01   1.11972022e-21   4.22901292e-01   3.00373058e-01]\n",
      "ottawa\t: [  9.12240081e-01   1.17983597e-43   4.81141503e-02   3.96457687e-02]\n",
      "hartford\t: [  8.13103271e-01   8.95510716e-44   1.86896729e-01   1.42438580e-12]\n",
      "american\t: [  7.72434399e-01   4.42359525e-22   2.85016152e-30   2.27565601e-01]\n",
      "ve\t: [  2.32888324e-02   7.21188089e-22   3.45850734e-01   6.30860434e-01]\n",
      "washington\t: [  6.68160644e-01   2.12436734e-22   8.71960942e-02   2.44643262e-01]\n",
      "hope\t: [  1.41361581e-01   7.40408654e-22   6.34939051e-01   2.23699368e-01]\n",
      "look\t: [  3.13200826e-01   9.71216217e-22   1.03495406e-01   5.83303768e-01]\n",
      "played\t: [  6.20524851e-01   3.61082010e-23   3.79475149e-01   1.41059169e-12]\n",
      "come\t: [  2.84395262e-01   1.31135178e-21   2.12386843e-01   5.03217895e-01]\n",
      "lindros\t: [  9.60505815e-01   1.00534548e-43   3.94941854e-02   1.29923558e-12]\n",
      "host\t: [  1.82055193e-01   1.17994516e-23   4.14112897e-01   4.03831910e-01]\n",
      "posting\t: [  1.61419760e-01   2.45799677e-22   4.17425647e-01   4.21154592e-01]\n",
      "talk\t: [  4.75449328e-01   2.69035024e-22   4.97496397e-01   2.70542752e-02]\n",
      "let\t: [  1.91608282e-01   5.23109007e-22   5.81774329e-01   2.26617388e-01]\n",
      "team\t: [  3.94121680e-01   1.98850207e-43   5.49905121e-01   5.59731998e-02]\n",
      "rangers\t: [  6.93441631e-01   1.49376833e-43   3.06558369e-01   2.69207060e-12]\n",
      "talking\t: [  4.90832150e-01   3.07026289e-21   3.45209171e-30   5.09167850e-01]\n",
      "baseball\t: [  1.17802569e-37   2.78840094e-43   1.00000000e+00   4.57531093e-12]\n",
      "nntp\t: [  1.64767642e-01   1.21405238e-23   4.26083148e-01   4.09149210e-01]\n",
      "stand\t: [  5.70875915e-38   4.92296470e-22   7.75496468e-01   2.24503532e-01]\n",
      "things\t: [  2.48739599e-01   2.28410478e-21   1.45982172e-01   6.05278229e-01]\n",
      "na\t: [  1.36614353e-01   3.73258512e-43   6.77313483e-01   1.86072165e-01]\n",
      "mail\t: [  3.14615743e-02   3.04844627e-22   3.92197390e-30   9.68538426e-01]\n",
      "right\t: [  1.17151592e-02   8.56677056e-22   4.12886669e-01   5.75398172e-01]\n",
      "smith\t: [  6.30128493e-01   1.89136260e-23   3.64871727e-01   4.99977982e-03]\n",
      "told\t: [  1.75718879e-01   9.43832450e-22   5.56153563e-01   2.68127558e-01]\n",
      "like\t: [  1.99962446e-01   6.89402287e-22   3.71301496e-01   4.28736058e-01]\n",
      "head\t: [  2.22993858e-01   2.45309526e-22   6.08052923e-01   1.68953219e-01]\n",
      "pretty\t: [  8.22457402e-03   2.33411934e-22   1.55513733e-01   8.36261693e-01]\n",
      "maybe\t: [  6.71518916e-38   5.78987008e-22   6.65973074e-01   3.34026926e-01]\n",
      "________________________________\n",
      "think\t: [ 0.00953649  0.04637321  0.85335377  0.09073653]\n",
      "ago\t: [ 0.05011352  0.03784112  0.87577488  0.03627048]\n",
      "experience\t: [  1.83008070e-01   8.09910585e-02   2.61832681e-29   7.36000872e-01]\n",
      "gary\t: [ 0.01459055  0.00860967  0.94922339  0.02757638]\n",
      "jets\t: [  5.59619460e-02   6.93812982e-24   9.44038054e-01   5.13302450e-13]\n",
      "posting\t: [ 0.02215483  0.01628241  0.85864193  0.10292084]\n",
      "com\t: [ 0.03726954  0.03959974  0.77436271  0.14876801]\n",
      "news\t: [ 0.06326096  0.00952988  0.82827461  0.09893455]\n",
      "support\t: [  1.45990108e-02   1.79184396e-23   9.02180230e-01   8.32207591e-02]\n",
      "winning\t: [  2.85359578e-39   5.71318715e-24   1.00000000e+00   2.41602677e-13]\n",
      "won\t: [ 0.01436556  0.0185105   0.94400504  0.0231189 ]\n",
      "money\t: [ 0.04165312  0.00806719  0.74593551  0.20434419]\n",
      "player\t: [  6.47226676e-02   9.61983859e-24   9.35277332e-01   5.18756766e-13]\n",
      "market\t: [  6.97132707e-38   1.36189154e-02   3.01171381e-01   6.85209704e-01]\n",
      "chance\t: [ 0.00645633  0.00807459  0.95230328  0.03316579]\n",
      "nhl\t: [  8.18665020e-02   2.55798770e-04   9.15819734e-01   2.05796484e-03]\n",
      "win\t: [  7.36340447e-02   5.30292707e-03   9.21063028e-01   5.11431757e-13]\n",
      "stanley\t: [  1.13205805e-38   1.98293426e-03   9.90039632e-01   7.97743392e-03]\n",
      "smith\t: [ 0.10301984  0.00149242  0.8940323   0.00145543]\n",
      "coach\t: [  5.08783416e-39   7.34990107e-24   1.00000000e+00   5.53000127e-13]\n",
      "playoffs\t: [  5.77959317e-02   1.10553289e-23   9.42204068e-01   6.92565114e-13]\n",
      "couldn\t: [  3.87680983e-38   8.80414308e-02   8.52769680e-01   5.91888895e-02]\n",
      "dare\t: [  4.24900996e-39   5.20770609e-24   1.00000000e+00   2.98922368e-13]\n",
      "coming\t: [  8.53929724e-38   4.98422713e-01   3.17777919e-29   5.01577287e-01]\n",
      "help\t: [ 0.00769852  0.01032528  0.88172444  0.10025176]\n",
      "taken\t: [  1.22051986e-01   5.96158479e-01   2.43909795e-29   2.81789535e-01]\n",
      "power\t: [  2.76857028e-01   2.10213031e-01   1.48848929e-29   5.12929941e-01]\n",
      "watch\t: [ 0.04156438  0.01805014  0.82895442  0.11143106]\n",
      "business\t: [  4.36644520e-38   1.57274713e-01   2.15078893e-29   8.42725287e-01]\n",
      "days\t: [ 0.03508138  0.28336616  0.47041249  0.21113998]\n",
      "hockey\t: [  4.65862121e-02   1.11522608e-23   9.50732751e-01   2.68103737e-03]\n",
      "just\t: [ 0.01525734  0.0426101   0.85770007  0.0844325 ]\n",
      "exist\t: [  6.80962168e-38   8.80243767e-01   4.11836792e-29   1.19756233e-01]\n",
      "lot\t: [  8.09598578e-39   8.04037667e-03   9.19363736e-01   7.25958873e-02]\n",
      "months\t: [  3.60615807e-03   1.92984059e-01   3.09509882e-29   8.03409783e-01]\n",
      "let\t: [ 0.02002853  0.0263908   0.91140343  0.04217724]\n",
      "don\t: [ 0.00787805  0.03345045  0.89771291  0.06095859]\n",
      "host\t: [  2.55940306e-02   8.00613616e-04   8.72520452e-01   1.01084904e-01]\n",
      "club\t: [  2.07747445e-02   1.45733472e-23   9.59886707e-01   1.93385488e-02]\n",
      "season\t: [  6.30612937e-02   1.09893238e-23   9.36938706e-01   6.68576404e-13]\n",
      "deal\t: [ 0.04769745  0.03345964  0.78081924  0.13802367]\n",
      "western\t: [ 0.01090293  0.03582714  0.89388921  0.05938073]\n",
      "like\t: [ 0.02914541  0.04849757  0.81109119  0.11126583]\n",
      "years\t: [ 0.05529192  0.03920208  0.83556762  0.06993838]\n",
      "city\t: [ 0.04971763  0.00627247  0.93156922  0.01244069]\n",
      "reply\t: [ 0.10840473  0.07586077  0.65792918  0.15780532]\n",
      "left\t: [ 0.00728874  0.00774476  0.92773013  0.05723637]\n",
      "fact\t: [ 0.01873939  0.06353582  0.85097266  0.06675213]\n",
      "time\t: [ 0.04031916  0.05556685  0.82145913  0.08265486]\n",
      "rangers\t: [  1.31137406e-01   1.36340681e-23   8.68862594e-01   9.06469464e-13]\n",
      "fans\t: [  4.91870051e-02   1.24624315e-23   9.45803561e-01   5.00943428e-03]\n",
      "talk\t: [ 0.05862826  0.01601169  0.91942002  0.00594003]\n",
      "despite\t: [ 0.23130342  0.34816452  0.38846962  0.03206244]\n",
      "got\t: [ 0.01642389  0.0133197   0.92527463  0.04498178]\n",
      "maybe\t: [  6.18610492e-39   2.57426579e-02   9.19468697e-01   5.47886454e-02]\n",
      "fan\t: [  4.31935773e-02   1.76024706e-23   9.55198963e-01   1.60745955e-03]\n",
      "writes\t: [ 0.01083702  0.0186724   0.89938388  0.07110669]\n",
      "remember\t: [  4.28503437e-01   8.00343748e-02   3.33931578e-29   4.91462188e-01]\n",
      "men\t: [  6.44169345e-38   9.06507408e-01   3.05075265e-29   9.34925923e-02]\n",
      "general\t: [  3.94356710e-01   7.60381902e-02   3.91793220e-29   5.29605100e-01]\n",
      "gld\t: [  4.24900996e-39   5.20770609e-24   1.00000000e+00   2.98922368e-13]\n",
      "columbia\t: [  1.06977127e-03   1.63725600e-23   9.73477086e-01   2.54531426e-02]\n",
      "cup\t: [  1.64440761e-02   9.49958567e-24   9.81045414e-01   2.51051017e-03]\n",
      "given\t: [ 0.0675308   0.13901836  0.63264414  0.1608067 ]\n",
      "interesting\t: [ 0.02201249  0.08931208  0.8314156   0.05725984]\n",
      "come\t: [ 0.05692009  0.12667415  0.63707724  0.17932852]\n",
      "year\t: [ 0.11489626  0.01309024  0.784525    0.0874885 ]\n",
      "long\t: [ 0.01187983  0.09057937  0.74478457  0.15275623]\n",
      "mark\t: [ 0.00609041  0.05544458  0.93180773  0.00665727]\n",
      "winnipeg\t: [  1.43483478e-01   7.22539602e-24   8.56516522e-01   5.25548930e-13]\n",
      "mike\t: [  5.52271983e-03   1.26908890e-23   9.89947717e-01   4.52956296e-03]\n",
      "happen\t: [ 0.06875858  0.03990185  0.76709286  0.12424672]\n",
      "new\t: [ 0.12630621  0.03359269  0.77923351  0.06086759]\n",
      "history\t: [ 0.08825979  0.07590368  0.76437353  0.071463  ]\n",
      "known\t: [ 0.00249365  0.0616344   0.88566228  0.05020967]\n",
      "canada\t: [  3.87199609e-02   2.37231611e-23   9.17264456e-01   4.40155831e-02]\n",
      "cc\t: [ 0.02971803  0.00126568  0.91719386  0.05182243]\n",
      "york\t: [  8.19247434e-02   3.09898978e-23   8.81391284e-01   3.66839725e-02]\n",
      "keywords\t: [  5.35586990e-02   2.80431645e-04   8.49035902e-01   9.71249674e-02]\n",
      "team\t: [  4.51180906e-02   1.09868190e-23   9.43472831e-01   1.14090787e-02]\n",
      "keenan\t: [  5.52245601e-03   1.46806436e-23   9.94477544e-01   1.07166843e-12]\n",
      "sports\t: [  6.71667832e-02   2.29503954e-23   8.65039268e-01   6.77939492e-02]\n",
      "devils\t: [  4.73710008e-02   7.43769220e-04   9.47747818e-01   4.13741164e-03]\n",
      "times\t: [ 0.05142741  0.05195939  0.77740448  0.11920872]\n",
      "selanne\t: [  1.54163471e-01   5.54950776e-24   8.45836529e-01   4.06340367e-13]\n",
      "said\t: [ 0.02854915  0.07215925  0.86583345  0.03345815]\n",
      "islanders\t: [  1.99194751e-01   1.95901167e-23   7.93687386e-01   7.11786256e-03]\n",
      "chicago\t: [ 0.36842162  0.00490011  0.6237499   0.00292837]\n",
      "ve\t: [ 0.00348741  0.05212298  0.77618476  0.16820485]\n",
      "person\t: [ 0.00555496  0.33332168  0.50348276  0.1576406 ]\n",
      "nntp\t: [  2.26175852e-02   8.04335558e-04   8.76576679e-01   1.00001400e-01]\n",
      "big\t: [ 0.01545662  0.0109091   0.81180343  0.16183086]\n",
      "________________________________\n",
      "teams\t: [  3.63953524e-37   4.50716759e-43   9.32524479e-01   6.74755208e-02]\n",
      "play\t: [  2.09779488e-37   5.34115924e-23   8.58809780e-01   1.41190220e-01]\n",
      "network\t: [  6.44173717e-37   8.93002882e-23   2.70736200e-01   7.29263800e-01]\n",
      "does\t: [  2.52720884e-38   1.01145788e-21   5.68505732e-01   4.31494268e-01]\n",
      "aren\t: [  1.25194031e-37   2.89065294e-43   3.04705623e-01   6.95294377e-01]\n",
      "detroit\t: [  1.32105768e-36   3.40599591e-43   1.00000000e+00   7.44029909e-12]\n",
      "acs\t: [  6.62695854e-74   4.17849540e-22   6.71267127e-30   1.00000000e+00]\n",
      "try\t: [  2.36052897e-38   3.73166330e-22   6.99664941e-30   1.00000000e+00]\n",
      "advance\t: [  1.30795345e-73   1.74306699e-22   8.48178556e-30   1.00000000e+00]\n",
      "steve\t: [  4.82038279e-74   2.06447031e-23   7.74410588e-01   2.25589412e-01]\n",
      "wings\t: [  4.31022001e-37   2.98481285e-43   9.65591626e-01   3.44083736e-02]\n",
      "local\t: [  2.20338975e-37   4.07758583e-43   3.19555736e-01   6.80444264e-01]\n",
      "know\t: [  3.20309579e-38   5.98188147e-22   6.10816127e-01   3.89183873e-01]\n",
      "games\t: [  4.75989622e-37   1.82747173e-43   9.86252556e-01   1.37474440e-02]\n",
      "today\t: [  8.40640235e-38   8.00939375e-22   9.63610740e-30   1.00000000e+00]\n",
      "radio\t: [  2.43507842e-37   2.11007282e-23   9.09712842e-01   9.02871582e-02]\n",
      "org\t: [  2.49471112e-38   4.45787555e-22   1.71277934e-01   8.28722066e-01]\n",
      "writes\t: [  4.43092566e-38   2.96475383e-22   6.24514559e-01   3.75485441e-01]\n",
      "baseball\t: [  4.62829051e-74   2.05329092e-43   1.00000000e+00   4.13365825e-12]\n",
      "article\t: [  3.16692783e-38   3.27913464e-22   5.38030326e-01   4.61969674e-01]\n",
      "texas\t: [  6.43931890e-39   4.27283221e-43   2.84785149e-01   7.15214851e-01]\n",
      "nhl\t: [  5.17516780e-37   6.27943449e-24   9.83198262e-01   1.68017377e-02]\n",
      "pick\t: [  8.51987444e-38   2.15449827e-43   9.03589021e-01   9.64109787e-02]\n",
      "toronto\t: [  1.03389732e-36   4.65822814e-43   9.31947598e-01   6.80524024e-02]\n",
      "probably\t: [  1.21261247e-37   5.41713909e-22   5.58248734e-01   4.41751266e-01]\n",
      "used\t: [  1.30962961e-38   1.00940320e-21   4.96457365e-01   5.03542635e-01]\n",
      "pittsburgh\t: [  7.90826464e-37   7.12512962e-22   9.94004091e-01   5.99590933e-03]\n",
      "penguins\t: [  1.00626514e-37   3.43975371e-43   1.00000000e+00   5.18552577e-12]\n",
      "computer\t: [  3.78977368e-74   1.18210467e-22   4.30920252e-01   5.69079748e-01]\n",
      "don\t: [  3.40765713e-38   5.61879173e-22   6.59458482e-01   3.40541518e-01]\n",
      "check\t: [  5.54841104e-38   8.46077083e-23   6.85891903e-01   3.14108097e-01]\n",
      "playing\t: [  8.73323186e-38   1.48769916e-43   9.75099832e-01   2.49001676e-02]\n",
      "right\t: [  4.93460676e-39   6.76317940e-22   4.42659070e-01   5.57340930e-01]\n",
      "university\t: [  1.98250876e-37   2.42695708e-22   6.47256346e-01   3.52743654e-01]\n",
      "series\t: [  1.80485375e-74   1.76545765e-43   5.41481516e-01   4.58518484e-01]\n",
      "like\t: [  1.03555165e-37   6.69153048e-22   4.89422774e-01   5.10577226e-01]\n",
      "thanks\t: [  4.24334621e-38   1.53300091e-22   6.16610282e-01   3.83389718e-01]\n",
      "purdue\t: [  1.05217509e-73   6.97501486e-22   1.04339744e-29   1.00000000e+00]\n",
      "time\t: [  1.63727608e-37   8.76254186e-22   5.66512230e-01   4.33487770e-01]\n",
      "couple\t: [  3.61435040e-37   2.90961334e-21   1.15786533e-29   1.00000000e+00]\n",
      "list\t: [  5.25314879e-37   3.70934051e-22   7.78312111e-01   2.21687889e-01]\n",
      "able\t: [  2.92147906e-38   2.61066172e-22   7.96920888e-01   2.03079112e-01]\n",
      "ecn\t: [  7.43413025e-74   1.60320999e-21   4.96926063e-30   1.00000000e+00]\n",
      "engineering\t: [  5.96341957e-74   1.27400614e-21   5.76039289e-30   1.00000000e+00]\n",
      "________________________________\n",
      "com\t: [  3.78055520e-37   1.54480896e-21   5.17168458e-30   1.00000000e+00]\n",
      "division\t: [  2.67142786e-36   9.94030901e-23   3.42469310e-30   1.00000000e+00]\n",
      "place\t: [  4.48196500e-73   4.41889517e-21   5.50871515e-30   1.00000000e+00]\n",
      "hp\t: [  2.52760573e-37   1.45495976e-42   4.15099197e-59   1.00000000e+00]\n",
      "friend\t: [  2.05480519e-38   3.02947782e-21   3.99086337e-59   1.00000000e+00]\n",
      "sports\t: [  1.49511539e-36   1.96467746e-42   1.26777458e-29   1.00000000e+00]\n",
      "want\t: [  8.25784873e-37   4.42470710e-21   3.80205105e-30   1.00000000e+00]\n",
      "year\t: [  1.98182833e-36   8.68337755e-22   8.90949294e-30   1.00000000e+00]\n",
      "wrong\t: [  5.83059003e-39   3.56168545e-21   4.74821005e-59   1.00000000e+00]\n",
      "uucp\t: [  1.80782391e-73   3.46891990e-22   4.79635127e-59   1.00000000e+00]\n",
      "________________________________\n",
      "price\t: [  6.20450435e-39   3.43341584e-43   1.15608719e-59   1.00000000e+00]\n",
      "lot\t: [  6.42717323e-74   2.50454235e-22   4.85228884e-30   1.00000000e+00]\n",
      "local\t: [  2.41023341e-37   4.50680456e-43   1.36839192e-30   1.00000000e+00]\n",
      "needed\t: [  1.06524136e-73   8.85140899e-22   4.22677625e-30   1.00000000e+00]\n",
      "host\t: [  1.45919949e-37   1.79102098e-23   3.30720113e-30   1.00000000e+00]\n",
      "free\t: [  7.28967418e-74   1.97495653e-21   4.69822337e-30   1.00000000e+00]\n",
      "car\t: [  6.92631697e-39   9.87769700e-23   5.11730958e-31   1.00000000e+00]\n",
      "cause\t: [  7.17012830e-74   6.89009589e-22   6.07951942e-30   1.00000000e+00]\n",
      "doubt\t: [  3.91163389e-36   2.67531326e-20   2.41136137e-29   1.00000000e+00]\n",
      "probably\t: [  2.04317028e-37   9.22253593e-22   3.68219160e-30   1.00000000e+00]\n",
      "told\t: [  2.12123705e-37   2.15770931e-21   6.68953232e-30   1.00000000e+00]\n",
      "does\t: [  4.35939716e-38   1.76291325e-21   3.83898359e-30   1.00000000e+00]\n",
      "ve\t: [  1.19488650e-38   7.00736498e-22   1.76806365e-30   1.00000000e+00]\n",
      "didn\t: [  5.66640725e-38   1.77768310e-21   1.27378655e-29   1.00000000e+00]\n",
      "ericsson\t: [ 0.  0.  0.  0.]\n",
      "necessary\t: [  3.51086366e-37   6.49952388e-22   1.47392947e-59   1.00000000e+00]\n",
      "reply\t: [  3.95903070e-37   1.08707527e-21   1.59745564e-30   1.00000000e+00]\n",
      "year\t: [  7.56862910e-37   3.38345702e-22   3.43579411e-30   1.00000000e+00]\n",
      "hand\t: [  7.02846585e-39   1.22566660e-21   9.37723951e-31   1.00000000e+00]\n",
      "posting\t: [  1.24058746e-37   3.57749678e-22   3.19653948e-30   1.00000000e+00]\n",
      "info\t: [  7.60438310e-38   3.91086865e-23   3.67561546e-30   1.00000000e+00]\n",
      "friend\t: [  7.84732871e-39   1.18042869e-21   1.53900845e-59   1.00000000e+00]\n",
      "se\t: [  2.43272779e-74   3.14264550e-43   1.26293441e-59   1.00000000e+00]\n",
      "ll\t: [  1.10851727e-73   8.47269543e-22   7.10979057e-30   1.00000000e+00]\n",
      "nntp\t: [  1.30347420e-37   1.81884291e-23   3.35857557e-30   1.00000000e+00]\n",
      "good\t: [  3.18694066e-39   6.64801767e-22   4.95648327e-30   1.00000000e+00]\n",
      "week\t: [  9.24719020e-37   3.95626774e-21   3.77316083e-30   1.00000000e+00]\n",
      "days\t: [  9.57565421e-38   3.03487990e-21   8.53649649e-31   1.00000000e+00]\n",
      "st\t: [  6.25597982e-36   6.57454414e-21   1.69778034e-29   1.00000000e+00]\n",
      "driving\t: [  3.05521272e-74   5.41379273e-43   2.92276137e-31   1.00000000e+00]\n",
      "include\t: [  2.61021517e-74   4.06496864e-22   5.46082487e-60   1.00000000e+00]\n",
      "________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dStart = 20 #29\n",
    "dEnd = 30 #40\n",
    "for dk in range(dStart,dEnd):\n",
    "    filtWords = np.array(vectorizer.get_feature_names())[WdTest[dk]]\n",
    "\n",
    "    wordMixtures = [filtWords[n] + \"\\t: \" + str(phi[dk][n]) for n in range(len(phi[dk]))]\n",
    "    for wm in set(wordMixtures):\n",
    "        print(wm)\n",
    "    print(\"________________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Bonus Objectives\n",
    "\n",
    "Well done! You have now implemented LDA, approximated the necessary variational parameters, and examined the results to infer information about topics in documents. If you feel like you would like to experiment some more, you can examine the results of increasing the number of topics, use the less restricted and larger lyrics vocabulary, or re-run your implementation on the AP docs dataset. See the cell below to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading the AP docs dataset instead:\n",
    "#(everything else should work like before)\n",
    "vectorizer = pickle.load(open(\"vectorizerAPSuperSmall.p\", \"rb\"), encoding='latin1')\n",
    "trainLyrics = pickle.load(open(\"trainApDocsSuperSmall.p\", \"rb\"), encoding='latin1')\n",
    "testLyrics = pickle.load(open(\"testApDocsSuperSmall2.p\", \"rb\"), encoding='latin1')\n",
    "\n",
    "#loading the larger vocabulary version of moodyLyrics dataset instead:\n",
    "vectorizer = pickle.load(open(\"vectorizerMedium.p\", \"rb\"), encoding='latin1')\n",
    "trainLyrics = pickle.load(open(\"trainDocsMedium.p\", \"rb\"), encoding='latin1')\n",
    "testLyrics = pickle.load(open(\"testDocsMedium.p\", \"rb\"), encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      "angry\n",
      "relaxed\n",
      "happy\n",
      "relaxed\n",
      "happy\n",
      "angry\n",
      "sad\n",
      "angry\n",
      "angry\n"
     ]
    }
   ],
   "source": [
    "#loading a larger vocabulary: \n",
    "\n",
    "for dk in range(10,20):\n",
    "    print(testLyricsFile['groundTruth'][dk])\n",
    "\n",
    "#for dk in range(0,len(trainLyrics['lyrics'])):\n",
    "#    print(trainLyrics['groundTruth'][dk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a matter of interest does anyone know why autos are so popular in the US while \n",
      "here in Europe they are rare??? Just wondering.....\n",
      "-- \n",
      "___________________________________________________________________ ____/|\n",
      "John Kissane                           | Motorola Ireland Ltd.,   | \\'o.O'\n",
      "UUCP    : ..uunet!motcid!glas!kissanej | Mahon Industrial Estate, | =() ()=\n",
      "Internet: kissanej@glas.rtsg.mot.com   | Blackrock, Cork, Ireland |    U\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories=['rec.autos', 'soc.religion.christian', 'sci.space', 'rec.sport.hockey']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "print(\"\\n\".join(twenty_train.data[4].split(\"\\n\\n\")[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: revdak@netcom.com (D. Andrew Kille)\n",
      "Subject: Re: Easter: what's in a name? (was Re: New Testament Double Stan\n",
      "Organization: NETCOM On-line Communication Services (408 241-9760 guest)\n",
      "Lines: 40\n",
      "\n",
      "Daniel Segard (dsegard@nyx.cs.du.edu) wrote:\n",
      "\n",
      "[a lot of stuff deleted]\n",
      "\n",
      ":      For that matter, stay Biblical and call it Omar Rasheet (The Feast of\n",
      ": First Fruits).  Torah commands that this be observed on the day following\n",
      ": the Sabbath of Passover week.  (Sunday by any other name in modern\n",
      ": parlance.)  Why is there so much objection to observing the Resurrection\n",
      ": on the 1st day of the week on which it actually occured?  Why jump it all\n",
      ": over the calendar the way Easter does?  Why not just go with the Sunday\n",
      ": following Passover the way the Bible has it?  Why seek after unbiblical\n",
      ": methods?\n",
      ":  \n",
      "In fact, that is the reason Easter \"jumps all over the calendar\"- Passsover\n",
      "itself is a lunar holiday, not a solar one, and thus falls over a wide\n",
      "possible span of times.  The few times that Easter does not fall during or\n",
      "after Passover are because Easter is further linked to the Vernal Equinox-\n",
      "the beginning of spring.\n",
      "\n",
      "[more deletions]\n",
      ":  \n",
      ":       So what does this question have to do with Easter (the whore\n",
      ": goddess)?  I am all for celebrating the Resurrection.  Just keep that\n",
      ": whore out of the discussion.\n",
      ":  \n",
      "Your obsession with the term \"whore\" clouds your argument.  \"Whore\" is\n",
      "a value judgement, not a descriptive term.\n",
      "\n",
      "[more deletions]\n",
      "\n",
      "Overall, this argument is an illustration of the \"etymological fallacy\"\n",
      "(see J.P. Louw: _Semantics of NT Greek_).  That is the idea that the true\n",
      "meaning of a word lies in its origins and linguistic form.  In fact, our\n",
      "own experience demonstrates that the meaning of a word is bound up with\n",
      "how it is _used_, not where it came from.  Very few modern people would\n",
      "make any connection whatsoever between \"Easter\" and \"Ishtar.\"  If Daniel\n",
      "Seagard does, then for him it has that meaning.  But that is a highly\n",
      "idiosyncratic \"meaning,\" and not one that needs much refutation.\n",
      "\n",
      "revdak@netcom.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(\"\\n\".join(twenty_train.data[113].split(\"\\n\")))\n",
    "print(twenty_train.data[68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soc.religion.christian\n",
      "rec.sport.hockey\n",
      "rec.autos\n",
      "rec.autos\n",
      "soc.religion.christian\n",
      "rec.autos\n",
      "rec.sport.hockey\n",
      "rec.autos\n",
      "rec.sport.hockey\n",
      "sci.space\n",
      "rec.autos\n",
      "sci.space\n",
      "rec.sport.hockey\n",
      "rec.autos\n",
      "rec.autos\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "rec.sport.hockey\n",
      "rec.autos\n",
      "rec.sport.hockey\n",
      "soc.religion.christian\n",
      "sci.space\n",
      "rec.autos\n",
      "sci.space\n",
      "sci.space\n",
      "rec.sport.hockey\n",
      "soc.religion.christian\n",
      "sci.space\n",
      "sci.space\n",
      "rec.autos\n",
      "soc.religion.christian\n",
      "rec.sport.hockey\n",
      "soc.religion.christian\n",
      "rec.sport.hockey\n",
      "rec.sport.hockey\n",
      "rec.autos\n",
      "sci.space\n",
      "rec.autos\n",
      "sci.space\n",
      "rec.autos\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.space\n",
      "rec.sport.hockey\n",
      "soc.religion.christian\n",
      "rec.sport.hockey\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "rec.autos\n",
      "rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "#subTrainTops = twenty_train.ca\n",
    "for t in twenty_train.target[250:300]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 750)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsTrain = []\n",
    "for newsD in range(200):\n",
    "    newsTrain.append(''.join([i for i in twenty_train.data[newsD] if not i.isdigit()]))\n",
    "  \n",
    "newsTest = []\n",
    "for newsD in range(250,300):\n",
    "    newsTest.append(''.join([i for i in twenty_train.data[newsD] if not i.isdigit()]))\n",
    "#newsTrain = twenty_train.data[:200]\n",
    "#newsTest = twenty_train.data[200:250]\n",
    "count_vect = CountVectorizer(max_df=0.5, min_df=0.01, stop_words='english', max_features=750)\n",
    "X_train_counts = count_vect.fit_transform(newsTrain + newsTest)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(count_vect.get_feature_names())\n",
    "pickle.dump(newsTrain, open(\"newsTrainDocs.p\", \"wb\"))\n",
    "pickle.dump(newsTest, open(\"newsTestDocs.p\", \"wb\"))\n",
    "pickle.dump(count_vect, open(\"newsVectorizer2.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: darling@cellar.org (Thomas Darling)\n",
      "Subject: Re: WHERE ARE THE DOUBTERS NOW?  HMM?\n",
      "Organization: The Cellar BBS and public access system\n",
      "Lines: \n",
      "\n",
      "jason@studsys.mscs.mu.edu (Jason Hanson) writes:\n",
      "\n",
      "> In article <Apr..@ramsey.cs.laurentian.ca> maynard@ramsey.cs.\n",
      "> >\n",
      "> >And after the Leafs make cream cheese of the Philadelphia side tomorrow\n",
      "> >night the Leafs will be without equal.\n",
      "> \n",
      "> Then again, maybe not.\n",
      "\n",
      "To put it mildly.  As I watched the Flyers demolish Toronto last night, -,\n",
      "I realized that no matter how good the Leafs' # line may be, they'll need\n",
      "one or two more decent lines to go far in the playoffs.  And, of course, a\n",
      "healthy Felix Potvin.\n",
      "\n",
      "^~^~^~^~^~^~^~^~^\\\\\\^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^~^\n",
      "Thomas A. Darling \\\\\\ The Cellar BBS & Public Access System: ..\n",
      "darling@cellar.org \\\\\\ GEnie: T.DARLING \\\\ FactHQ \"Truth Thru Technology\"\n",
      "v~v~v~v~v~v~v~v~v~v~\\\\\\~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v~v\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((newsTrain[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__', '___', 'able', 'absolute', 'absolutes', 'ac', 'accept', 'access', 'according', 'acs', 'activities', 'acts', 'actually', 'ad', 'added', 'address', 'adirondack', 'aerospace', 'age', 'ago', 'agree', 'ahl', 'air', 'al', 'alaska', 'alchemy', 'alive', 'amateur', 'american', 'andrew', 'angeles', 'answer', 'anybody', 'applications', 'appropriate', 'apr', 'april', 'area', 'areas', 'aren', 'argument', 'ariane', 'ask', 'asked', 'assist', 'astronomy', 'atheist', 'atheists', 'athos', 'atmosphere', 'att', 'au', 'australia', 'authority', 'auto', 'automotive', 'autos', 'available', 'away', 'baalke', 'bad', 'baltimore', 'base', 'based', 'belief', 'beliefs', 'believe', 'believing', 'best', 'better', 'bible', 'big', 'biggest', 'billion', 'bit', 'bitnet', 'black', 'blue', 'bob', 'bobby', 'body', 'book', 'books', 'boston', 'box', 'boyle', 'brian', 'brown', 'budget', 'buffalo', 'built', 'buy', 'ca', 'calgary', 'called', 'callison', 'came', 'canada', 'car', 'cars', 'case', 'cb', 'cc', 'center', 'central', 'certainly', 'champs', 'chance', 'change', 'check', 'chicago', 'christ', 'christian', 'christianity', 'christians', 'church', 'city', 'claim', 'clarkson', 'clear', 'clh', 'club', 'cmu', 'coach', 'college', 'colorado', 'columbia', 'com', 'come', 'comes', 'commercial', 'committee', 'communications', 'community', 'computer', 'concept', 'conference', 'consider', 'contact', 'control', 'convertible', 'corporation', 'correct', 'cost', 'costs', 'couple', 'course', 'coverage', 'craig', 'created', 'cs', 'cso', 'cup', 'current', 'cwru', 'dallas', 'dan', 'data', 'dave', 'david', 'day', 'days', 'dc', 'deal', 'dealer', 'death', 'decided', 'defense', 'defensive', 'degrees', 'deleted', 'department', 'dept', 'described', 'design', 'details', 'detroit', 'development', 'did', 'didn', 'die', 'difference', 'different', 'digex', 'disciples', 'discuss', 'discussion', 'discussions', 'distribution', 'division', 'does', 'doesn', 'doing', 'don', 'doubt', 'doug', 'draft', 'drag', 'dseg', 'early', 'earth', 'ecn', 'effort', 'email', 'end', 'energy', 'eng', 'engine', 'engineering', 'entire', 'eos', 'eric', 'especially', 'eternal', 'european', 'event', 'evidence', 'evil', 'exactly', 'example', 'excellent', 'exist', 'existence', 'exists', 'experience', 'exploration', 'fact', 'faith', 'false', 'family', 'fans', 'faq', 'far', 'fast', 'father', 'fax', 'feel', 'fi', 'final', 'flight', 'flyers', 'fnal', 'follow', 'following', 'forget', 'forward', 'foundation', 'free', 'freenet', 'friend', 'fuel', 'funding', 'future', 'game', 'games', 'gary', 'gas', 'gave', 'gene', 'general', 'generally', 'geneva', 'georgia', 'gerald', 'gets', 'getting', 'gibbons', 'gilmour', 'given', 'giving', 'gm', 'goal', 'goalie', 'goals', 'god', 'going', 'good', 'got', 'gov', 'gravity', 'great', 'group', 'groups', 'guess', 'guy', 'half', 'hand', 'happened', 'hard', 'having', 'head', 'heard', 'heaven', 'hell', 'help', 'henry', 'hi', 'high', 'history', 'hit', 'hockey', 'holy', 'home', 'hope', 'host', 'human', 'ice', 'idea', 'ideas', 'images', 'important', 'include', 'including', 'indiana', 'inflatable', 'info', 'information', 'instead', 'institute', 'interested', 'international', 'internet', 'interpretation', 'involved', 'isn', 'issue', 'isu', 'james', 'jason', 'jeff', 'jesus', 'jets', 'jewish', 'jews', 'jim', 'joe', 'john', 'johnson', 'joseph', 'jpl', 'jr', 'jupiter', 'just', 'key', 'keywords', 'killing', 'kind', 'km', 'knew', 'know', 'knowledge', 'known', 'knows', 'laboratory', 'language', 'large', 'later', 'launch', 'law', 'leafs', 'league', 'leave', 'left', 'lemieux', 'let', 'life', 'light', 'like', 'likely', 'lindros', 'line', 'liquid', 'list', 'little', 'live', 'lived', 'living', 'll', 'local', 'long', 'look', 'looking', 'looks', 'lord', 'los', 'lot', 'louis', 'love', 'low', 'lunar', 'mail', 'mailing', 'major', 'make', 'makes', 'making', 'man', 'mark', 'mass', 'math', 'matter', 'matthew', 'maybe', 'mcgill', 'mean', 'means', 'member', 'members', 'men', 'mentioned', 'message', 'mi', 'michael', 'mid', 'mike', 'mind', 'minnesota', 'mission', 'missions', 'model', 'models', 'moncton', 'money', 'montreal', 'moon', 'mot', 'murray', 'nasa', 'national', 'nature', 'nd', 'near', 'necessarily', 'necessary', 'need', 'needs', 'net', 'network', 'new', 'news', 'newsgroup', 'nhl', 'night', 'nntp', 'non', 'north', 'note', 'nuclear', 'number', 'ny', 'oakland', 'offensive', 'office', 'oh', 'ohio', 'oil', 'ok', 'oklahoma', 'old', 'open', 'opinion', 'opinions', 'option', 'orbit', 'orbiter', 'order', 'org', 'original', 'outside', 'particular', 'parts', 'past', 'pat', 'paul', 'peace', 'people', 'performance', 'period', 'person', 'peter', 'philadelphia', 'phoenix', 'phone', 'physics', 'pittsburgh', 'place', 'planetary', 'play', 'played', 'player', 'players', 'playoffs', 'plus', 'po', 'point', 'points', 'position', 'possible', 'post', 'posting', 'power', 'pp', 'prb', 'president', 'pressure', 'previous', 'price', 'pro', 'probably', 'probe', 'problem', 'problems', 'process', 'profit', 'program', 'programs', 'project', 'prophecy', 'propulsion', 'provide', 'pts', 'public', 'puck', 'purpose', 'quebec', 'question', 'questions', 'quick', 'quite', 'radio', 'ramsey', 'rangers', 'ray', 'rd', 'read', 'reading', 'real', 'reality', 'really', 'reason', 'rec', 'record', 'red', 'redesign', 'reference', 'references', 'regular', 'religion', 'religions', 'religious', 'remember', 'remote', 'reply', 'request', 'research', 'reserve', 'result', 'rick', 'right', 'road', 'robert', 'rochester', 'rocket', 'rockets', 'romans', 'ron', 'running', 'russian', 'rutgers', 'said', 'salvation', 'san', 'satellite', 'satellites', 'saturn', 'save', 'saved', 'saw', 'say', 'saying', 'says', 'scale', 'school', 'sci', 'science', 'score', 'scoring', 'scott', 'scripture', 'season', 'second', 'seen', 'send', 'sender', 'sense', 'seriously', 'set', 'shall', 'short', 'shuttle', 'similar', 'simply', 'sin', 'single', 'sky', 'small', 'smith', 'society', 'software', 'solar', 'son', 'sort', 'sounds', 'source', 'south', 'space', 'spacecraft', 'speak', 'special', 'speed', 'spencer', 'spirit', 'spiritual', 'st', 'stage', 'standard', 'star', 'stars', 'start', 'state', 'statement', 'station', 'steve', 'stop', 'strong', 'student', 'studies', 'study', 'stupid', 'summary', 'sun', 'sunday', 'support', 'sure', 'surface', 'systems', 'taken', 'taking', 'talk', 'talking', 'talks', 'teachings', 'team', 'teams', 'technical', 'technology', 'tell', 'term', 'terms', 'testament', 'th', 'thanks', 'thing', 'things', 'think', 'thomas', 'thought', 'ti', 'tie', 'time', 'times', 'titan', 'tm', 'today', 'told', 'tom', 'tommy', 'took', 'toronto', 'total', 'trade', 'tried', 'trouble', 'true', 'truth', 'try', 'trying', 'turn', 'tv', 'tx', 'type', 'types', 'ucs', 'uiuc', 'uk', 'unc', 'understand', 'understanding', 'universe', 'university', 'unix', 'unless', 'uokmax', 'uoknor', 'upenn', 'usa', 'use', 'used', 'useful', 'using', 'usually', 'utoronto', 'uucp', 'uxa', 'various', 've', 'vehicle', 'version', 'view', 'vs', 'want', 'warning', 'washington', 'wasn', 'watch', 'way', 'week', 'went', 'western', 'white', 'wife', 'win', 'wings', 'winnipeg', 'won', 'word', 'words', 'work', 'works', 'world', 'worth', 'wouldn', 'wrong', 'wrote', 'yamauchi', 'year', 'years', 'yes', 'york', 'young']\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5,  1.5,  1.5,  1.5])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((4))*3 - np.ones((4))*1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ipykernel_py3]",
   "language": "python",
   "name": "conda-env-ipykernel_py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
