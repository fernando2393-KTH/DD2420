{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Clustering Using LDA\n",
    "\n",
    "The purpose of this tutorial is to give you a basic understanding of Latent Dirichlet Allocation (LDA) from http://ai.stanford.edu/~ang/papers/jair03-lda.pdf and use it to implement a simple downscaled topic clustering on the newsgroup dataset from scikit-learn.\n",
    "\n",
    "Section 1 will give some background to the mechanics and theory behind LDA. Section 2 will then tackle the task of implementing LDA to infer topics in documents based on their content. This section will provide you with skeleton code already written in Python 3 using the numpy, scipy, and scikit-learn libraries. \n",
    "\n",
    "If you do not have jupyter notebook installed then you probably aren't reading this, but see http://jupyter.readthedocs.io/en/latest/install.html\n",
    "\n",
    "If you do not have a python 3 kernel installed for jupyter notebook see https://ipython.readthedocs.io/en/latest/install/kernel_install.html or https://stackoverflow.com/questions/28831854/how-do-i-add-python3-kernel-to-jupyter-ipython\n",
    "\n",
    "If you do not have some of the libraries installed for your python 3 kernel, use the \"Kernel -> Conda packages\" dropdown menu in Jupyter if you used anaconda for your python 3 kernel, if not use the normal pip install commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Theory\n",
    "\n",
    "### Background and terminology\n",
    "\n",
    "Since we will be working in the setting of text corpora, we should clarify some of the terminology used in this setting:\n",
    "<ul>\n",
    "<li>A word is the basic unit of discrete data, defined to be an item from a vocabulary indexed by {1, . . . ,V}. \n",
    "<li> A document is a sequence of <i>N</i> words denoted by <b>w</b>=(<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>, . . . ,<i>w</i><sub>N</sub>), where <i>w</i><sub>n</sub> is the <i>n</i>th word in the sequence.</li>\n",
    "<li>A corpus is a collection of <i>M</i> documents denoted by $D$ ={<b>w</b><sub>1</sub>,<b>w</b><sub>2</sub>, . . . ,<b>w</b><sub>M</sub>}</li>\n",
    "</ul>\n",
    "\n",
    "It is important to note that LDA works in other domains besides text collections, but this is the setting in which we will use it.\n",
    "\n",
    "LDA is a generative probabilistic model that is used for modeling collections of discrete data. In our application we will be using it to model text corpora, or more specifically news group e-mails. The purpose of the model is to give us compact representations of the data in these collections, allowing us to process large collections while still retaining sufficient information to be able to perform for example classification and relevance measures. \n",
    "\n",
    "There have been several solutions for this type of information retrieval problem, such as the tf-idf (term frequency - inverse document frequency) scheme by Salton and McGill, 1983. This approach produces a term-by-document matrix X whose columns contain the tf-idf values for each of the documents in the corpus. This representation however did not provide significantly shortened representation of the corpora, or represent the inter- or intra- document statistics in a intuitive way. A step forward from this was given by LSI (latent semantic indexing) where singular value decomposition was used on the matrix X to offer a more compact representation. The authors of the method also argued that since the LSI features are linnear combinations of the basic tf-idf features, they incorporate some linguistical notions such as synonomy and polysemy.\n",
    "The first step to providing a generative model was the <i>probabilistic</i> LSI (pLSI), which uses mixture models to model each word in a document. The mixture components are the \"topics\" and represented as multinomial random variables, allowing different words in the document to be genereated by different topics. The compact representation for each document is then the list of numbers representing the mixing proportions for the fixed set of topics. The method however gives no generative probabilistic model for getting these numbers, causing the number of parameters in the model to grow linearly with the corpus size. Also, since there is no probabilistic model for the mixture components that represent a document, there is no clear way of assigning a probability to a document that is outside the training set.\n",
    "\n",
    "Both LSI and pLSI use the \"bag-of-words\" approach which assumes exchangeability within the words of the document as well as the documents themselves, meaning their order is of no importance. A theorem due to de Finetti (1990) states that any collection of exchangeable random variables has a representation as a mixture distribution—in general an infinite mixture. This means we must consider mixture models that capture the exchangeability of both documents and words if we wish to achieve exchangeable representations for them. It is this line of thinking that leads to LDA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Theory Behind LDA\n",
    "\n",
    "As mentioned earlier, LDA is a generative probabilistic model for a corpus. It can be seen as a hierarchical Bayesian model with three levels: each document in a corpus is modeled as a finite random mixture over a latent set of topics, and each of these topics are characterized by a distribution of words. A graphical model for LDA using plate notation can be seen below:\n",
    "![title](imgs/LDAPlateGM.png)\n",
    "From here we can see the three levels of the model. $\\alpha$ and $\\beta$ and corpus level parameters, $\\theta$ is a document level parameter for the M documents in the corpus, and $z$ and $w$ are word level parameters for the N words in a document.\n",
    "\n",
    "The generative process according to LDA for each document <b>w</b> is then:\n",
    "<ol>\n",
    "<li>Choose N ∼Poisson(ξ)</li>\n",
    "<li>Choose $\\theta$∼Dir($\\alpha$)</li>\n",
    "<li>For each of the N words w<sub>n</sub>:\n",
    "<ol type=\"a\">\n",
    "    <li>Choose a topic z<sub>n</sub> ∼Multinomial($\\theta$).</li>\n",
    "    <li>Choose a word w<sub>n</sub> from p(w<sub>n</sub> |z<sub>n</sub>,$\\beta$), a multinomial probability conditioned on the topic z<sub>n</sub>.</li>\n",
    "    </ol></li>\n",
    "</ol>\n",
    "\n",
    "There are however some simplifications to these steps that we will utilize. First, we assume that the dimensionality of the Dirichlet distribution, and therefore the dimensionality for the topic variable $z$ is known and fixed, meaning we assume a fixed known number of topics, $k$. Furthermore, the probabilities for words ($w$) are parameterized by a $k \\times V$ matrix $\\beta$ which defines $p(w^j = 1| z^i = 1) = \\beta_{i,j}$, that we will estimate later and keep fixed. We also note that $N$ is independant of the other data generating variables $\\theta$ and <b>z</b> so we will ignore the Poisson assumption and set it to a known fixed value (the length of the document).  \n",
    "\n",
    "#### Dirichlet Distribution in LDA\n",
    "\n",
    "The probability distribution for a $k$-dimensional Dirichlet random variable $\\theta$ is defined as follows: \n",
    "\n",
    "<b>Eq. 1:</b>\n",
    "![Eq 1](imgs/LDAEq1.png \"Eq 1\")\n",
    "\n",
    "\n",
    "where $\\alpha$ is $k$-dimensional with all elements larger than 0 and $\\Gamma(x)$ is the Gamma function. The Dirichlet distribution has some advantegous advantageous qualities; it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution. These properties help us in running variational inference for the parameters later.\n",
    "\n",
    "We can now express the joint distribution of a topic mixture $\\theta$, a set of $N$ topics <b>z</b>, and\n",
    "a set of $N$ words <b>w</b> given the corpus level parameters $\\alpha,\\beta$ as:\n",
    "\n",
    "<b>Eq. 2:</b>\n",
    "![Eq. 2](imgs/LDAEq2.png)\n",
    "where the probability $p(z_n |\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z^i_n=1$. We can then obtain the marginal distribution over a document by integrating over $\\theta$ and summing over $z$:\n",
    "\n",
    "<b>Eq. 3:</b>\n",
    "![Eq. 3](imgs/LDAEq3.png)\n",
    "\n",
    "\n",
    "#### Comparison to other Latent Variable Models\n",
    "In order to get feeling for how LDA works and what highlights its strengths, it can be helpful to relate it to other related models:\n",
    "\n",
    "  a) Unigram Model\n",
    "\n",
    "  b) Mixture of Unigrams Model\n",
    "\n",
    "  c) pLSI Model\n",
    "  \n",
    "\n",
    "\n",
    "We will begin by examing the absolute simplest model, the unigram model: \n",
    "\n",
    "![Eq. 3](imgs/UniGramMdl.png)\n",
    "\n",
    "This method has no latent variables and instead states that each word in a document is independantly drawn from a single multinomial distribution as seen here:\n",
    "\n",
    "![Eq. 3](imgs/UniGramEq.png)\n",
    "\n",
    "\n",
    "A slighly more complex model is the mixture of unigrams:\n",
    "\n",
    "![Eq. 3](imgs/MixUniGramMdl.png)\n",
    "\n",
    "This model incorporates a discrete latent topic variable, $z$. Here, each document <b>w</b> is generated by first sampling the topic variable $z$, and then generating all words from a conditional probability on that choice:\n",
    "\n",
    "![Eq. 3](imgs/MixUniGramEq.png)\n",
    "\n",
    "This effectively limits the modeling of words in a document to only being representative of one topic. The LDA model on the other hand allows for documents to exhibit multiple topics with different mixtures.\n",
    "\n",
    "Finally we have the pLSI model which we mentioned earlier. It was a relatively popular model around the time that LDA was proposed, and is the model with highest generative capabilities of these three mentioned. \n",
    "\n",
    "![Eq. 3](imgs/PISLMdl.png)\n",
    "\n",
    "pLSI proposes that each word is conditionally independant a \"document label\", $d$, given an unobserved topic $z$:\n",
    "\n",
    "![Eq. 3](imgs/PISLEq.png)\n",
    "\n",
    "This proposal aims to soften the constraint of having each document modeled as being generated from only one topic, as it is in the mixture of unigrams approach. It does so by incorporating the probability, $p(z | d)$ for a certain document $d$ as the mixture of topics that document. A true generative model cannot be created for this mixture however; as d is only a dummy index to the documents pISL was trained with, meaning it is a multinomial random variable with the same amount of possible values as training documents. This leads to the method only learning the topic mixtures, $p(z | d)$, for documents it has already seen, so there is no natural way to assign probability to an unseen document with it.\n",
    "Another problem is that to model $k$ topics with pLSI you need K multinomial distributions with vocabulary size $V$ and $M$ mixtures over the hidden topics $k$ for each training document, resulting in $kV + kM$ parameters. Not only does this not scale well but it is also prone to overfitting.\n",
    "\n",
    "LDA however treats the topic mixture weights as a $k$-parameter hidden variable, meaning the amount of parameters does not scale linnearly with the number training documents, and the generative model can still be used even with unseen documents.\n",
    "\n",
    "We can see these differences geometrically as well if we examine the distribution over words as a $V$-1 dimensional on a vocabulary of size $V$ with another $k$-1 dimensional simplex spanning $k$ topics. We can set $V$ and $k$ to 3 for simplicity (3 words gives a two-dimensional triangle):\n",
    "\n",
    "![title](imgs/UnigramSampling.png)\n",
    "\n",
    "How this distribution is spread out and how it uses the topics distribution differs among the methods. The mixture of unigrams method pics a random point on the word simplex that corresponds to one of the topic simplex vertices k, and draws all the words for a document from the distribution corresponding to that point. pLSI assumes that all words in training documents belong to a single randomly chosen topic. The topics are drawn from a document-specific distribution, meaning each document has a topic distribution that sits on the topic simplex. The training documents then give an empirical distribution (with the marked 'x's) over the topic simplex. LDA instead models that <b>each word</b> in a document is drawn from a randomly chosen topic that is sampled from a distribution governed by a random parameter. Since this parameter is sampled once per document, it gives a smooth probability distribution over the topic simplex (the circular topology markers). \n",
    "\n",
    "Now that we know how LDA compares with other methods, lets take a look at how to do inference in LDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and Parameter Estimation with LDA\n",
    "\n",
    "The main inference problem we will be interested in solving is the posterior distribution of the latent variables given a document, which would allow us to infer the topics associated with the document. This is given by the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "p( \\theta, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta) = \\frac{p( \\theta, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)}{p(\\mathbf{w} \\mid \\alpha, \\beta)} \n",
    "\\end{equation}\n",
    "\n",
    "However, to compute the normalizing denominator we would rewrite equation 3 using equation 1 and $p(z_n \\mid\\theta)=\\theta_i$ and then integrate, resulting in:\n",
    "\n",
    "\\begin{equation}\n",
    "p( \\mathbf{w} \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\sum_i\\alpha_i)}{\\prod_i\\Gamma(\\alpha_i)}\\int\\Bigg(\\prod_{i=1}^k\\theta_i^{\\alpha_i-1}\\Bigg)\\Bigg(\\prod_{n=1}^N\\sum_{i=1}^k\\prod_{j=1}^V(\\theta_i\\beta_{ij})^{w_n^j}\\Bigg)d\\theta\n",
    "\\end{equation}\n",
    "\n",
    "Unfortunately, this expression is intractable due to the coupling of $\\theta$ and $\\beta$ in the summation over topics. We can however solve this problem approximately using variational inference methods. \n",
    "\n",
    "#### Variational Inference for LDA\n",
    "\n",
    "It is possible to use several VI methods for LDA, including La Place approximation, variational approximation, and MCMC methods. In our case, we will be using the convexity-based variational approximation that was mentioned in  Olga's tutorial. From there we learned that in this VI we attempt to reformulate/simplify the original graphical model by removing some dependencies and introducing free variational parameters. This leads to a family of distributions dependant on these variational parameters which form a lower bound on the log likelyhood. We then aim to find the parameter values that give the tightest lower bound. \n",
    "\n",
    "In our case, the problematic dependancy is between $\\theta$ and $\\beta$ which is introduced by the edges between $\\theta, \\mathbf{z}$ and $\\mathbf{w}$ (remember w is a 'collision' node and is observed). If we simplify our model by removing these edges along with the <b>w</b> node, and introduce two variational parameters $\\gamma$ and $\\phi$ which give a family of distributions over the remaining latent variables, we are left with the graphical model shown on the right in the figure below:\n",
    "\n",
    "![LDA VI](imgs/LDAVIGM.png \"GM for the VI used for our LDA\")\n",
    "\n",
    "This results in the following distribution over the latent variables:\n",
    "\n",
    "\\begin{equation}\n",
    "p( \\theta, \\mathbf{z} \\mid \\gamma, \\phi) = q(\\theta\\mid\\gamma)\\prod_{n=1}^Nq(z_n\\mid\\phi_n)\n",
    "\\end{equation}\n",
    "\n",
    "where the new Dirichlet parameters $\\gamma$ and the multinomial parameters $\\phi$ are the free variational parameters. Now having simplified our graphical model, we need to find the optimal values for the variational parameters ($\\gamma^*, \\phi^*$). From Olga's tutorial we know that this is equivalent to finding the values which minimize the KL divergence between the simplified distribution and the true posterior distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "( \\gamma^*, \\phi^*) = \\arg\\!\\min_{(\\gamma,\\phi)}D\\big(q( \\theta, \\mathbf{z} \\mid \\gamma, \\phi) \\mid\\mid p( \\theta, \\mathbf{z} \\mid \\mathbf{w},\\alpha, \\beta\\big)\n",
    "\\end{equation}\n",
    "\n",
    "We do this by setting the derivatives of the KL divergence to zero w.r.t $\\gamma$ and $\\phi$ we get the following update equations for the parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_{ni} \\propto \\beta_{iw_n}\\mathrm{exp}\\big\\lbrace\\mathrm{E}_q\\big[log(\\theta_i)\\mid\\gamma\\big]\\big\\rbrace = \\beta_{iw_n}\\mathrm{exp}\\big\\lbrace\\Psi(\\gamma_i)\\big\\rbrace\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\gamma_i = \\alpha_i + \\sum_{n=1}^N\\phi_{ni}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Psi$ is the digamma function (first derivative of log $\\Gamma$). It is important to note that these update equations derived from the KL divergence are dependant on a certain choice of <b>w</b>. This means that the shown approximation for the variational parameters is only valid for one set of words, and must therefore be calculated for each document when we use them later on. \n",
    "\n",
    "We must also find a way of estimating the $\\beta$ matrix, as it is used in the approximations for the variational parameters. The log likelihood of the data given $\\beta$ and $\\alpha$ is intractable as we saw at the end of the previous section. However, it is possible to implement a variational EM procedure that gives us an approximation of the best value for $\\beta$ by first maximizing a lower bound for the optimal variational parameters $\\gamma^*,\\phi^*$, then maximizing the lower bound w.r.t $\\beta$ with the previously acquired variational parameters. Essentially we will iterate the following steps until a sufficient level of convergence:\n",
    "<ol>\n",
    "<li>(E-step) Find the optimizing values of the variational parameters {$\\gamma^∗\n",
    "_d,\\phi^∗_d : d\\in D$},for each document as described earlier.</li>\n",
    "<li>(M-step) Maximize the resulting lower bound on the log likelihood w.r.t $\\beta$ using:\n",
    "\\begin{equation}\n",
    "\\beta_{ij}\\propto\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\phi^*_{dni}w^j_{dn}\n",
    "\\end{equation}\n",
    "as well as maximize the resulting lower bound on the log likelihood w.r.t $\\alpha$ (this will be given to you).\n",
    "</ol>\n",
    "\n",
    "In laymans terms, what we are essentially doing in the E-step is finding out \"How prevalent are topics in the document across its words?\". In the M-step we then ask \"How prevalent are specific words across topics?\". By using the answer from one question as a starting point for the other, we iteratively gain the answer to both. \n",
    "\n",
    "<span style=\"color:red\">For proof for the update equations, see appendix of http://ai.stanford.edu/~ang/papers/jair03-lda.pdf</span>. This appendix also includes the derivation of the Newton-Raphson based method for updating $\\alpha$.  \n",
    "\n",
    "We have now seen the basic intuition behind LDA, and gone through methods for running inference based on the LDA model. In the next section we will put this knowledge in to practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Implementation\n",
    "\n",
    "The goal of this task is to use LDA to create topics newsgroup documents, and infer the topic that new documents would belong to. In this setting, our document corpus is the Newsgroup dataset from scikit-learn, and a document is a certain document/e-mail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "First we will load the dataset we will use for training and testing. We will simplify the example from the original paper to only do clustering for 4 topics, and only use 250 documents with a vocabulary of 750 words. While this does have an effect on the accuracy and performance of the algorithm, it's more important for you to be able to run the code in a managable amount of time. The documents I have chosen come from 4 different categories; \"Christian Religion\", \"Hockey\", \"Space\" and \"Cars\". This means that we have slightly unrealistic prior knowledge by assuming the exact correct number of topics, but don't worry there are bonus assignments in the end where you can play around with the number of topics. I have already compiled and done some preprocessing on the documents, as well as built the vocabulary dataset as pickle files. Run the code in the cell below and double check that you get the output \"found 200 training and 50 test documents, with a vocabulary of 750 words\". Do not worry if you get a warning regarding the version of CountVectorizer which is used for handling the vocabulary of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  200 training and  50  test documents, with a vocabulary of  750  words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.18.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle as pickle\n",
    "\n",
    "vectorizer = pickle.load(open(\"Data/vectorizerNews.p\", \"rb\"))\n",
    "trainDocs = pickle.load(open(\"Data/trainDocsNews.p\", \"rb\"))\n",
    "testDocs = pickle.load(open(\"Data/testDocsNews.p\", \"rb\"))\n",
    "\n",
    "#also load the original docs that aren't pre-processed for viewing later\n",
    "origTrainDocs = pickle.load(open(\"Data/trainDocsNewsOrig.p\", \"rb\"))\n",
    "origTestDocs = pickle.load(open(\"Data/testDocsNewsOrig.p\", \"rb\"))\n",
    "\n",
    "print(\"Found \", len(trainDocs), \"training and \", len(testDocs), \" test documents, with a vocabulary of \", len(vectorizer.get_feature_names()), \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation\n",
    "\n",
    "We must now find the optimal values for the variational parameters, as well as the values for $\\alpha$ and the $\\beta$ matrix that were introduced in the variational inference section. In order to follow the instructions given in the VI section we will need to do some setup first, so run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-dd04bd9fcc1b>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-dd04bd9fcc1b>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    print(\"Alpha dimension \" + str(alpha.shape))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import scipy.special as special\n",
    "import scipy.optimize \n",
    "import time\n",
    "\n",
    "#diGamma func from scipy, use this in your code!\n",
    "diGamma = special.digamma\n",
    "\n",
    "#Function definitions for maximizing the VI parameters. This will later be completed by you.\n",
    "def maxVIParam(phi, gamma, B, alpha, M, k, Wd, eta):\n",
    "    \n",
    "    for d in range(M):\n",
    "        N = len(Wd[d])\n",
    "        #Initialization of vars, as shown in E-step. \n",
    "        phi[d] = np.ones((N,k))*1.0/k\n",
    "        gamma[d] = np.ones(k)*(N/k) + alpha\n",
    "        converged = False\n",
    "        j = 0 #you can use this to print the update error to check your code in the beginning with something like:\n",
    "        '''if(j%10==0 and d==0):\n",
    "                    print(\"u e: \", updateError)'''\n",
    "        #YOUR CODE FOR THE E-STEP HERE\n",
    "        print(\"B dimension \" + str(B.shape))\n",
    "        print(\"Gamma dimension \" + str(len(gamma))\n",
    "        print(\"Alpha dimension \" + str(len(alpha))\n",
    "        prev_gamma = gamma[d]\n",
    "        while(not converged):\n",
    "            phi[d] = B @ np.exp(diGamma(alpha))\n",
    "            row_sums = phi[d].sum(axis=1)\n",
    "            phi[d] = phi[d] / row_sums[:, np.newaxis]\n",
    "            gamma[d] = alpha + phi[d].sum(axis=1)\n",
    "            if max(abs(prev_gamma - gamma[d])) < eta:\n",
    "                converged = True\n",
    "            else:\n",
    "                prev_gamma = gamma[d]            \n",
    "    \n",
    "    return gamma, phi\n",
    "\n",
    "#Function definitions for maximizing the B parameter. This will later be completed by you.\n",
    "def MaxB(B, phi, k, V, M, Wd):\n",
    "    \n",
    "    #YOUR CODE FOR THE M-STEP HERE\n",
    "    \n",
    "    return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Here are the functions needed for updating the alpha parameter, as shown in the start of appendix A.4.2.\n",
    "These are all provided for you as it is just plugging in the definition for the gradient and hessian into the \n",
    "Newton-Raphson based method to find a stationary point using SciPy. Feel free to take a look at the appendix to\n",
    "see where these values come from.'''\n",
    "\n",
    "#value of Likelihood(gamma,phi,alpha,beta) function w.r.t. alpha terms (see start of appendix A.4.2) \n",
    "def L_alpha_val(a):\n",
    "    val = 0\n",
    "    M = len(gamma)\n",
    "    k = len(a)\n",
    "    for d in range(M):\n",
    "        val += (np.log(scipy.special.gamma(np.sum(a))) - np.sum([np.log(scipy.special.gamma(a[i])) for i in range(k)]) + np.sum([((a[i] -1)*(diGamma(gamma[d][i]) - diGamma(np.sum(gamma[d])))) for i in range(k)]))\n",
    "\n",
    "    return -val\n",
    "\n",
    "#value of the derivative of above func w.r.t. alpha_i (2nd eq of appendix A.4.2) \n",
    "def L_alpha_der(a):\n",
    "    M = len(gamma)\n",
    "    k = len(a)\n",
    "    der = np.array(\n",
    "    [(M*(diGamma(np.sum(a)) - diGamma(a[i])) + np.sum([diGamma(gamma[d][i]) - diGamma(np.sum(gamma[d])) for d in range(M)])) for i in range(k)]\n",
    "    )\n",
    "    return -der\n",
    "\n",
    "def L_alpha_hess(a):\n",
    "    hess = np.zeros((len(a),len(a)))\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(a)):\n",
    "            k_delta = 1 if i == j else 0\n",
    "            hess[i,j] = k_delta*M*scipy.special.polygamma(1,a[i]) - scipy.special.polygamma(1,np.sum(a))\n",
    "    return -hess\n",
    "\n",
    "def MaxA(a):\n",
    "    res = scipy.optimize.minimize(L_alpha_val, a, method='Newton-CG',\n",
    "        jac=L_alpha_der, hess=L_alpha_hess,\n",
    "        options={'xtol': 1e-8, 'disp': False})\n",
    "    print(res.x)\n",
    "    \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, we can now initialize the required parameters and define the skeleton of our loop for the parameter estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on iter:  0\n",
      "B dimension (4, 750)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c81aaac46025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#Variational EM for gamma and phi (E-step from VI section)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxVIParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mBold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-dc6bea632a26>\u001b[0m in \u001b[0;36mmaxVIParam\u001b[0;34m(phi, gamma, B, alpha, M, k, Wd, eta)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#YOUR CODE FOR THE E-STEP HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B dimension \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gamma dimension \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Alpha dimension \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprev_gamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "eta = 10e-5 #threshold for convergence\n",
    "\n",
    "#hyperparamater init.\n",
    "V = len(vectorizer.get_feature_names()) #vocab. cardinality\n",
    "M = int(len(trainDocs)) #number of documents\n",
    "k = 4 #amount of emotions\n",
    "\n",
    "nIter=100 # number of iterations to run until parameter estimation is considered converged\n",
    "\n",
    "#initialize B matrix as random valid distr (most common according to https://profs.sci.univr.it/~bicego/papers/2015_SIMBAD.pdf)\n",
    "B = np.random.rand(k,V)\n",
    "\n",
    "#normalize B\n",
    "for i in range(k):\n",
    "    B[i,:] = B[i]/np.sum(B[i])\n",
    "    \n",
    "alpha = np.ones(k)\n",
    "#variational params (one for each doc)\n",
    "phi = [None]*M\n",
    "gamma = [None]*M\n",
    "\n",
    "#word lists for all docs\n",
    "Wd = [None]*M\n",
    "\n",
    "'''Since scikit gives a matrix of counts of all words, and we want a list of words,\n",
    "we do some quick transformations here. This gives us a representation of the documents \n",
    "as a list of numbers, where each number is the vocabulary index of a word. This way, to access\n",
    "B_ij where i is the ith topic and j is the nth word in the document d, you can simply write B[i][Wd[d][n]]. If you want\n",
    "replace this code a different representation for the words in a document, such as a one-hot vector for each word, you are\n",
    "of course free to do so but make sure to keep track of your indexes'''\n",
    "\n",
    "for d in range(M):\n",
    "    Wmat = vectorizer.transform([trainDocs[d]]).toarray()[0] #get vocabulary matrix for document\n",
    "    WVidxs = np.where(Wmat!=0)[0]\n",
    "    WVcounts = Wmat[WVidxs]\n",
    "    N = np.sum(WVcounts)\n",
    "    W = np.zeros((N)).astype(int)\n",
    "\n",
    "    i = 0\n",
    "    for WVidx, WV in enumerate(WVidxs):\n",
    "        for wordCount in range(WVcounts[WVidx]):\n",
    "            W[i] = WV\n",
    "            i+=1\n",
    "    Wd[d] = W #We save the list of words for the document for analysis later\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#start of parameter estimation loop\n",
    "for j in range(nIter):\n",
    "    print(\"on iter: \", j)\n",
    "    #Variational EM for gamma and phi (E-step from VI section)\n",
    "    start = time.time()\n",
    "    gamma, phi = maxVIParam(phi, gamma, B, alpha, M, k, Wd, eta)\n",
    "    end = time.time()\n",
    "    Bold = np.copy(B)\n",
    "    B = MaxB(B,phi,k,V,M,Wd) #first half of M-step from VI section \n",
    "    #renormalize B\n",
    "    for i in range(k):\n",
    "        B[i,:] = B[i]/np.sum(B[i])\n",
    "    \n",
    "    print(\"B max diff: \", np.amax(abs(B-Bold)))\n",
    "    end = time.time()\n",
    "    \n",
    "    alpha = MaxA(alpha) #second half of M-step from VI section \n",
    "    end = time.time()\n",
    "    print(\"iter took: \", end-start)\n",
    "    print(\"new alpha: \", alpha)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"took: \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI Parameter Estimation\n",
    "We can now begin with implementing the \"E step\" in the previous section which updates the variational parameters. The pseudo code for this is the following (remember these have to be calculated separately for each document):\n",
    "![VIPseudo](imgs/VIPseudo.png)\n",
    "\n",
    "Since we are working with four topics, k will be set to 4 and N will be the amount of words in the current document. Regarding the \"until convergence\" condition, it is sufficient to check if the largest difference between the previous and new gamma is less than $10^{-5}$. Now, use the pseudo code to fill in the missing code in the \"MaxVIParam\" function defined earlier and remember to use the provided diGamma function. To see that your implementation seems to be working, set nIter to 1 and add some printouts of the difference between updates for gamma, then check that they are converging to something smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beta Matrix Estimation\n",
    "After you have implemented the MaxVIParam function, it's time to update the Beta matrix. Recall that the update function for Beta was:\n",
    "\\begin{equation}\n",
    "\\beta_{ij}\\propto\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\phi^*_{dni}w^j_{dn}\n",
    "\\end{equation}\n",
    "Implement this in the definition for MaxB above. To verify your code, you may set nIter to something low such as 10 and uncomment the \"oldB\" and \"B max diff\" lines in the code. The diff might increase at first but should start decreasing at least before iteration 10. After you have verified this, set nIter to 100 (updates should be negligable by then) and let it run unattended as it might take a couple hours. You can use the code in the following cell to save/load the parameter values you calculated for later so you don't have to re-run everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(alpha, open(\"Data/myAlphaNews100.p\", \"wb\"))\n",
    "pickle.dump(B, open(\"Data/myBetaNews100.p\", \"wb\"))\n",
    "\n",
    "#alpha = pickle.load(open(\"myAlphaNews.p\", \"rb\"))\n",
    "#B = pickle.load(open(\"myBetaNews.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification\n",
    "Let's take a look at what we've done so far. We can get an idea of what our implementation has done up to this point by inspecting the B matrix. As you may remember, B$_{ij}$ holds the probability of a vocabulary word j being representative of a certain topic i. Using the code in the following cell we can see the most representative words for our 4 topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for topic  0 : \n",
      "['control' 'saturn' 'air' 'station' 'uk' 'live' 'national' 'good' 'cso'\n",
      " 'price' 'story' '25' 'lord' 'topic' 'report' 'columbia' 'nhl' 'rutgers'\n",
      " 'steve' 'discussion']\n",
      "top words for topic  1 : \n",
      "['anybody' 'ucs' 'teach' 'aerospace' 'ad' 'fly' 'world' 'quick' 'info'\n",
      " 'nuclear' 'wing' 'vehicle' 'body' 'hear' 'far' 'montreal' 'playoff'\n",
      " 'necessarily' 'death' 'orbiter']\n",
      "top words for topic  2 : \n",
      "['evidence' 'history' 'structure' 'fly' 'air' 'playoff' 'low' 'technology'\n",
      " 'thing' 'teaching' 'ramsey' 'past' 'head' 'want' 'explain' 'ac'\n",
      " 'american' 'provide' 'nasa' 'limit']\n",
      "top words for topic  3 : \n",
      "['expect' 'tie' 'oklahoma' 'near' 'story' 'rise' 'pass' 'term' '41'\n",
      " 'access' 'johnson' 'deal' 'shot' 'tell' 'worth' 'score' '02' 'cup' '10'\n",
      " 'jr']\n"
     ]
    }
   ],
   "source": [
    "#representation of top words for each topic:\n",
    "nTop = 20\n",
    "for i in range(k):\n",
    "    topVocabs = np.argsort(B[i])[-nTop:][::-1]\n",
    "    topWords = np.array(vectorizer.get_feature_names())[topVocabs]\n",
    "    print(\"top words for topic \",i,\": \")\n",
    "    print(topWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no guarantees regarding the order of the topics (LDA is unsupervised) or what your initial B matrix values were, it is difficult to say exactly what results you should be seeing. Hopefully, you can see the four original topics to some extent in your result. For example, one of my topics had top words like \"christ\", and \"god\", meaning it was most likely the topic for \"Christian Religion\" documents, while another literally had the words \"Hockey\" and \"NHL\" in it. Our vocabulary is as mentioned earlier quite limited, so it may be possible that one of your topics is a bit unclear or close to another. You can also load the $\\alpha$ and $\\beta$ values in the cell below which are pre-calculated for 200 iterations and compare your topic results to those available there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaTest = pickle.load(open(\"Data/CompareAlphaNews200.p\", \"rb\"))\n",
    "BTest = pickle.load(open(\"Data/CompareBetaNews200.p\", \"rb\"))\n",
    "vecTest = pickle.load(open(\"Data/vectorizerNews.p\", \"rb\"), encoding='latin1')\n",
    "nTop = 20\n",
    "for i in range(k):\n",
    "    topVocabs = np.argsort(BTest[i])[-nTop:][::-1]\n",
    "    topWords = np.array(vecTest.get_feature_names())[topVocabs]\n",
    "    print(\"top words for topic \",i,\": \")\n",
    "    print(topWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the topic of a test document\n",
    "\n",
    "In this section we will be using our estimated parameter values to infer the topic of some test documents. In order to do this, we will have to calculate the phi and gamma values for each new document we would like to do inference on. This is rather straight forward, and you should be able to reuse your code from the previous sections together with the test documents as a corpus instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "eta = 10e-5 #threshold for convergence\n",
    "\n",
    "#we are not re-initializing beta and alpha, we calculated them using the training docs.\n",
    "\n",
    "V = len(vectorizer.get_feature_names()) #vocab. cardinality\n",
    "M = int(len(testDocs)) #number of documents\n",
    "k = 4 #amount of emotions\n",
    "\n",
    "#variational params (one for each doc)\n",
    "phi = [None]*M\n",
    "gamma = [None]*M\n",
    "WdTest = [None]*M\n",
    "\n",
    "'''Same magic from before to get the word matrix correct, replace this if you redid this earlier.'''\n",
    "\n",
    "for d in range(M):\n",
    "    Wmat = vectorizer.transform([testDocs[d]]).toarray()[0] #get vocabulary matrix for document\n",
    "    WVidxs = np.where(Wmat!=0)[0]\n",
    "    WVcounts = Wmat[WVidxs]\n",
    "    N = np.sum(WVcounts)\n",
    "    W = np.zeros((N)).astype(int)\n",
    "\n",
    "    i = 0\n",
    "    for WVidx, WV in enumerate(WVidxs):\n",
    "        for wordCount in range(WVcounts[WVidx]):\n",
    "            W[i] = WV\n",
    "            i+=1\n",
    "    WdTest[d] = W #We save the list of words for the document for analysis later\n",
    "\n",
    "'''Now that you have your variables initialized for the test documents, you should be able to use your previous code for \n",
    "maximizing the VI parameters with those variables instead. Remember, we're just calculating the variational parameters\n",
    "gamma and phi for each test document so there is no iteration between maximizing Beta and maximizing gamma and phi.'''\n",
    "\n",
    "# YOUR CODE to Run the gamma/phi maximization here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now calculated the variational parameters for our test documents, so let us see what information we can infer from them. If you take a look at the pseudo code we used for the MaxVIParam method, you can see that the posterior gamma parameter $\\gamma_i $we are calculating is approximately the prior Dichlet parameter $\\alpha_i$ added to the expected number of words that were generated by that $i^{th}$ topic for a certain document. Looking at the values for the different $\\gamma_i$ over all words for a test document tells us what mixture of topics form such a document. Let us now take a look at the mixtures for some of our test documents by running the code in the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#take a look at some example test documents (14-24 has a nice mix of topics, with a couple difficult ones)\n",
    "dStart = 14\n",
    "dEnd = 24\n",
    "for d in range(dStart,dEnd):\n",
    "    print(\"Estimated mixture for document \", d,\" is: \")\n",
    "    print(\"_______________________\")\n",
    "    for i in range(len(gamma[d])):\n",
    "        print(\"topic \", i,\": \", gamma[d][i]/np.sum(gamma[d]))\n",
    "    print(\"_______________________\")\n",
    "    print(\"Which has the following text:\")\n",
    "    print(\" \")\n",
    "    print(origTestDocs[d])\n",
    "    print(\"__________________________________________\")\n",
    "    print(\"__________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisit the cell that presented the top words for your topics. Do the presented mixtures above make sense if you look at the document content? Recall which original categories (\"Religion\", \"Cars\", \"Hockey\", \"Space\") you (to your best ability) assigned to which numbers. Do the texts seem to be discussing those topics?\n",
    "\n",
    "<span style=\"color:blue\">If you're re-doing this test with the MoodyLyrics dataset from the bonus section, you may be noticing some weird results. LDA can experience some issues in this setting, as for example many words that would be present in a happy song could also be present in a sad song ('love', 'hold', 'forever') but in different order or with certain \"negating\" words between them. It is possible to alleviate this problem by using a vocabulary of n-grams, however this increases the total size of the vocabulary (and therefore the run time as well) substantially. </span>\n",
    "\n",
    "It is also possible to gain some more insight by examining the $\\phi$ values for the documents. Recall that the $\\phi$ values for the document approximate $p(z_n | \\mathbf{w})$, showing how the topics are mixed for the words in the document. The cell below provides a method for printing the phi values for each word in a document. Apart from just examining how the topics are mixed for specific words, take a look at the topic mixtures for the same word that appears in several different documents. As stated in the theory section, in LDA the distribution of topic mixtures that are assigned to each word is sampled differently for each document. This means that hopefully it should be apparent from your results how the topic mixture for the same word can be differ in different test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#14-24 gives a good mix, but try whatever you like\n",
    "dStart = 14 \n",
    "dEnd = 24 \n",
    "\n",
    "\n",
    "def getWordsFromMatrix(WdTest):\n",
    "    originalWords  = np.array(vectorizer.get_feature_names())[WdTest] \n",
    "    return originalWords\n",
    "\n",
    "for dk in range(dStart,dEnd):\n",
    "    \n",
    "    origWords = getWordsFromMatrix(WdTest[dk])\n",
    "    wordMixtures = [origWords[n] + \"\\t: \" + str(phi[dk][n]) for n in range(len(phi[dk]))]\n",
    "    for wm in set(wordMixtures):\n",
    "        print(wm)\n",
    "    print(\"________________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Objectives\n",
    "\n",
    "Well done! You have now implemented LDA, approximated the necessary variational parameters, and examined the results to infer information about topics in documents. If you feel like you would like to experiment some more, there are some variants that you could try:\n",
    "\n",
    "1. Load the provided dataset from the Associated Press docs dataset. This has random news articles from an undisclosed number of topics. Replace the dataset code in the beginning with what is provided in the next cell and redo your tests. What kind of topics does your result have? How many topics did you assume there were? (Some interesting cases I got were general topics like Crime and Economics and then one focusing solely on foreign affairs with President Bush)\n",
    "\n",
    "2. Run the tests using the MoodyLyrics dataset instead. This dataset includes the lyrics from songs in many different genres (I've included has slightly less than 200 / 50 documents and V=500). Run the tests again and see what kind of sense LDA tries to make out of these song lyrics. The dataset also provides an annotation as to what emotion (\"Angry\", \"Sad\", \"Happy\", \"Relaxed\") the song exhibits. Can you find a resemblence in your topics to these emotions? (<i>Disclaimer: The lyrics provided are not censored and some are not exactly \"PG-13\")</i>\n",
    "\n",
    "<b>It is possible to start to see results after ~50-60 iterations so if you would like to try out these bonus exercises you need not wait overnight</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the AP docs dataset instead:\n",
    "#(everything else should work like before)\n",
    "vectorizer = pickle.load(open(\"Data/vectorizerAP.p\", \"rb\"), encoding='latin1')\n",
    "trainDocs = pickle.load(open(\"Data/trainDocsAP.p\", \"rb\"), encoding='latin1')\n",
    "testDocs = pickle.load(open(\"Data/testDocsAP.p\", \"rb\"), encoding='latin1')\n",
    "\n",
    "\n",
    "#loading the moodyLyrics dataset instead:\n",
    "vectorizer = pickle.load(open(\"Data/vectorizerMoodyLyrics.p\", \"rb\"), encoding='latin1')\n",
    "trainLyricsFile = pickle.load(open(\"Data/trainDocsMoodyLyrics.p\", \"rb\"), encoding='latin1')\n",
    "testLyricsFile = pickle.load(open(\"Data/testDocsMoodyLyrics.p\", \"rb\"), encoding='latin1')\n",
    "\n",
    "trainDocs = trainLyricsFile['lyrics']\n",
    "testDocs = testLyricsFile['lyrics']\n",
    "#original moods can be seen with: trainGT = trainLyricsFile['groundTruth'] but the labeling is not perfect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
